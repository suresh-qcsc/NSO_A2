2025-08-06 12:33:44,170 p=1217120 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] **************************************************************************************************************
2025-08-06 12:33:44,177 p=1217120 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-06 12:33:56,105 p=1217120 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-06 12:33:56,815 p=1217120 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-06 12:33:56,838 p=1217120 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-06 12:33:56,861 p=1217120 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-06 12:34:10,049 p=1217120 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3)
2025-08-06 12:34:10,386 p=1217120 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3)
2025-08-06 12:34:10,832 p=1217120 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3)
2025-08-06 12:34:21,574 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC] => (item=python3-pip)
2025-08-06 12:34:22,278 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA] => (item=python3-pip)
2025-08-06 12:34:22,389 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB] => (item=python3-pip)
2025-08-06 12:34:34,804 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmpd)
2025-08-06 12:34:36,239 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmpd)
2025-08-06 12:34:36,572 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmpd)
2025-08-06 12:34:44,442 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp)
2025-08-06 12:34:46,115 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp)
2025-08-06 12:34:46,432 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp)
2025-08-06 12:34:59,342 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp-mibs-downloader)
2025-08-06 12:35:01,567 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp-mibs-downloader)
2025-08-06 12:35:01,912 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp-mibs-downloader)
2025-08-06 12:35:01,919 p=1217120 u=ubuntu n=ansible | TASK [Install Flask] **************************************************************************************************************************************************
2025-08-06 12:35:09,842 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:09,986 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:09,994 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:10,000 p=1217120 u=ubuntu n=ansible | TASK [Deploy the Flask application config for TCP Load Balancing] *****************************************************************************************************
2025-08-06 12:35:18,312 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:18,474 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:18,681 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:18,688 p=1217120 u=ubuntu n=ansible | TASK [Start Flask app in background on port 5000] *********************************************************************************************************************
2025-08-06 12:35:23,196 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:23,694 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:23,752 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:23,757 p=1217120 u=ubuntu n=ansible | TASK [Check Flask app HTTP response on private IP] ********************************************************************************************************************
2025-08-06 12:35:28,419 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:28,540 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:28,650 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:28,656 p=1217120 u=ubuntu n=ansible | TASK [Display Flask app HTTP response on private IP] ******************************************************************************************************************
2025-08-06 12:35:28,679 p=1217120 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "12:35:27 10.1.1.44:52012 -- 10.1.1.44 (rev1-deva) 96"
}
2025-08-06 12:35:28,684 p=1217120 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "12:35:27 10.1.1.48:43050 -- 10.1.1.48 (rev1-devb) 16"
}
2025-08-06 12:35:28,692 p=1217120 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "12:35:27 10.1.1.31:56622 -- 10.1.1.31 (rev1-devc) 98"
}
2025-08-06 12:35:28,697 p=1217120 u=ubuntu n=ansible | TASK [Remove the existing agent address lines] ************************************************************************************************************************
2025-08-06 12:35:33,079 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:33,724 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:33,757 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:33,764 p=1217120 u=ubuntu n=ansible | TASK [Configure agent address (0.0.0.0) for SNMPd to listen on all UDP interfaces] ************************************************************************************
2025-08-06 12:35:37,866 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:38,535 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:38,605 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:38,610 p=1217120 u=ubuntu n=ansible | TASK [Check snmpd config File] ****************************************************************************************************************************************
2025-08-06 12:35:43,246 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:43,428 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:43,572 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:43,577 p=1217120 u=ubuntu n=ansible | TASK [Display snmpd configuration file] *******************************************************************************************************************************
2025-08-06 12:35:43,604 p=1217120 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003812",
        "end": "2025-08-06 12:35:42.599108",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-06 12:35:42.595296",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-06 12:35:43,607 p=1217120 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.004184",
        "end": "2025-08-06 12:35:42.885921",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-06 12:35:42.881737",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-06 12:35:43,618 p=1217120 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003231",
        "end": "2025-08-06 12:35:42.757342",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-06 12:35:42.754111",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-06 12:35:43,623 p=1217120 u=ubuntu n=ansible | TASK [Restart SNMPD if agent address configuration is changed] ********************************************************************************************************
2025-08-06 12:35:48,886 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:49,007 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:49,367 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:49,373 p=1217120 u=ubuntu n=ansible | TASK [Test SNMPd with snmpget on 10.1.1.44] ***************************************************************************************************************************
2025-08-06 12:35:54,204 p=1217120 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-06 12:35:54,347 p=1217120 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-06 12:35:54,460 p=1217120 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-06 12:35:54,465 p=1217120 u=ubuntu n=ansible | TASK [Print SNMPd snmpget result] *************************************************************************************************************************************
2025-08-06 12:35:54,491 p=1217120 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-06 12:35:54,492 p=1217120 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-06 12:35:54,501 p=1217120 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-06 12:35:54,575 p=1217120 u=ubuntu n=ansible | PLAY [Set up HAProxy] *************************************************************************************************************************************************
2025-08-06 12:35:54,578 p=1217120 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-06 12:36:00,337 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-06 12:36:00,349 p=1217120 u=ubuntu n=ansible | TASK [Add HAProxy 2.9 PPA] ********************************************************************************************************************************************
2025-08-06 12:36:13,639 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:13,644 p=1217120 u=ubuntu n=ansible | TASK [Install HAProxy] ************************************************************************************************************************************************
2025-08-06 12:36:30,107 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:30,112 p=1217120 u=ubuntu n=ansible | TASK [Deploy stats web page password file] ****************************************************************************************************************************
2025-08-06 12:36:33,722 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:33,727 p=1217120 u=ubuntu n=ansible | TASK [Read stats page password from file] *****************************************************************************************************************************
2025-08-06 12:36:35,734 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-06 12:36:35,741 p=1217120 u=ubuntu n=ansible | TASK [Set up HAProxy stats secret variable] ***************************************************************************************************************************
2025-08-06 12:36:35,775 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-06 12:36:35,782 p=1217120 u=ubuntu n=ansible | TASK [Configure HAProxy] **********************************************************************************************************************************************
2025-08-06 12:36:39,401 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:39,406 p=1217120 u=ubuntu n=ansible | TASK [Deploy rsyslog 49-haproxy config file] **************************************************************************************************************************
2025-08-06 12:36:43,029 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:43,034 p=1217120 u=ubuntu n=ansible | TASK [Return 49_haproxy_conf to registered rsyslog_49_haproxy_conf] ***************************************************************************************************
2025-08-06 12:36:44,954 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:44,959 p=1217120 u=ubuntu n=ansible | TASK [debug] **********************************************************************************************************************************************************
2025-08-06 12:36:44,974 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "rsyslog_49_haproxy_conf.stdout_lines": [
        "# Create an additional socket in haproxy's chroot in order to allow logging via",
        "# /dev/log to chroot'ed HAProxy processes",
        "$AddUnixListenSocket /var/lib/haproxy/dev/log",
        "",
        "# Send HAProxy messages to a dedicated logfile",
        ":programname, startswith, \"haproxy\" {",
        "  /var/log/haproxy.log",
        "stop",
        "}"
    ]
}
2025-08-06 12:36:44,979 p=1217120 u=ubuntu n=ansible | TASK [Test HAProxy Configurations] ************************************************************************************************************************************
2025-08-06 12:36:46,917 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:46,922 p=1217120 u=ubuntu n=ansible | TASK [Display HAProxy config test result] *****************************************************************************************************************************
2025-08-06 12:36:46,938 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_configs_test_result": {
        "changed": true,
        "cmd": [
            "haproxy",
            "-f",
            "/etc/haproxy/haproxy.cfg",
            "-c"
        ],
        "delta": "0:00:00.027668",
        "end": "2025-08-06 12:36:46.609016",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-06 12:36:46.581348",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "",
        "stdout_lines": []
    }
}
2025-08-06 12:36:46,942 p=1217120 u=ubuntu n=ansible | TASK [Test HAProxy is running] ****************************************************************************************************************************************
2025-08-06 12:36:49,028 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-06 12:36:49,035 p=1217120 u=ubuntu n=ansible | TASK [Display the HAProxy service status] *****************************************************************************************************************************
2025-08-06 12:36:49,050 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_service_status.state": "started"
}
2025-08-06 12:36:49,055 p=1217120 u=ubuntu n=ansible | TASK [Check HAProxy server status] ************************************************************************************************************************************
2025-08-06 12:36:50,975 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:50,980 p=1217120 u=ubuntu n=ansible | TASK [Display HAProxy server status] **********************************************************************************************************************************
2025-08-06 12:36:50,995 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "systemctl_haproxy_service_status.stdout_lines": [
        "● haproxy.service - HAProxy Load Balancer",
        "     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Wed 2025-08-06 12:36:26 UTC; 24s ago",
        "       Docs: man:haproxy(1)",
        "             file:/usr/share/doc/haproxy/configuration.txt.gz",
        "   Main PID: 4471 (haproxy)",
        "     Status: \"Ready.\"",
        "      Tasks: 2 (limit: 4588)",
        "     Memory: 39.8M",
        "     CGroup: /system.slice/haproxy.service",
        "             ├─4471 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "             └─4491 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "",
        "Aug 06 12:36:26 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 06 12:36:26 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : New worker (4491) forked",
        "Aug 06 12:36:26 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 06 12:36:26 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : Loading success."
    ]
}
2025-08-06 12:36:51,000 p=1217120 u=ubuntu n=ansible | TASK [Check HAProxy config errors via journalctl] *********************************************************************************************************************
2025-08-06 12:36:52,906 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:52,912 p=1217120 u=ubuntu n=ansible | TASK [Display HAProxy config errors] **********************************************************************************************************************************
2025-08-06 12:36:52,928 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_journalctl_logs.stdout_lines": [
        "-- Logs begin at Wed 2025-08-06 12:31:59 UTC, end at Wed 2025-08-06 12:36:52 UTC. --",
        "Aug 06 12:36:26 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 06 12:36:26 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : New worker (4491) forked",
        "Aug 06 12:36:26 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 06 12:36:26 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : Loading success."
    ]
}
2025-08-06 12:36:52,933 p=1217120 u=ubuntu n=ansible | TASK [Check the HAProxy configuration file] ***************************************************************************************************************************
2025-08-06 12:36:54,831 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:54,836 p=1217120 u=ubuntu n=ansible | TASK [Display HAProxy configuration file] *****************************************************************************************************************************
2025-08-06 12:36:54,852 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_config_file.stdout_lines": [
        "global",
        "    profiling.tasks on #Enable HAProxy profiling (CPU time spent on processing a http request inside HAProxy)",
        "    nbthread 1 # 1 thread, IDs from 1 to 2, nbthread <number of CPU cores>",
        "    thread-groups 1",
        "    # declare threads",
        "    thread-group 1 1-1",
        "    # bind threads to cpu cores",
        "    cpu-map 1/all 0-0 # bind all threads to CPU 0 #syntax:cpu-map 1/1-<Number Of CPU Cores> 0-<Number of CPU Cores - 1>",
        "    # define logging",
        "    log /dev/log local0 info",
        "    #log /dev/log local0 emerg",
        "    #log /dev/log local1 alert",
        "    #log /dev/log local2 crit",
        "    #log /dev/log local3 err",
        "    #log /dev/log local4 warning",
        "    #log /dev/log local5 notice",
        "    #log /dev/log local6 info",
        "    #log /dev/log local7 debug",
        "    #Security Considerations",
        "    chroot /var/lib/haproxy #chroot statement pointing to a /var/lib/haproxy location",
        "    user haproxy # uid/user statement",
        "    group haproxy # gid/group statement",
        "    stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin",
        "    stats maxconn 20",
        "    stats timeout 30000",
        "    daemon",
        "    maxconn 512",
        "        ",
        "defaults",
        "    mode http",
        "    timeout connect 5000ms",
        "    timeout client 5000ms",
        "    timeout server 5000ms",
        "    errorfile 400 /etc/haproxy/errors/400.http",
        "    errorfile 403 /etc/haproxy/errors/403.http",
        "    errorfile 408 /etc/haproxy/errors/408.http",
        "    errorfile 500 /etc/haproxy/errors/500.http",
        "    errorfile 502 /etc/haproxy/errors/502.http",
        "    errorfile 503 /etc/haproxy/errors/503.http",
        "    errorfile 504 /etc/haproxy/errors/504.http",
        "",
        "frontend web_stats",
        "    mode http",
        "    bind *:80 ",
        "    http-request use-service prometheus-exporter if { path /metrics }",
        "    stats enable # enable stats page",
        "    stats uri /stats # stats uri",
        "    stats hide-version",
        "    stats refresh 1s",
        "    stats auth admin:uipassword",
        "",
        "frontend haproxy_frontend",
        "    log global",
        "    bind *:80  thread 1/all shards by-thread  #bind this proxy to threads 1 to 1 or all",
        "    mode http",
        "    option httplog",
        "    #option dontlog-normal",
        "    #option logasap",
        "    #define custom log-format",
        "    log-format \"%ci:%cp [%tr] %ft %b/%s %TR/%Tw/%Tc/%Tr/%Ta %ST %B %CC %CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r %[http_first_req] cpu_calls:%[cpu_calls] cpu_ns_tot:%[cpu_ns_tot] cpu_ns_avg:%[cpu_ns_avg] lat_ns_tot:%[lat_ns_tot] lat_ns_avg:%[lat_ns_avg]\"",
        "    default_backend haproxy_backend",
        "    ",
        "backend haproxy_backend",
        "    retry-on all-retryable-errors # This works when conn-failure, empty-response, junk-response, response-timeout, rtt-rejected, 500, 502, 503, and 504",
        "    retries 3",
        "             server rev1_devA 10.1.1.44:5000 check maxconn 64",
        "             server rev1_devB 10.1.1.48:5000 check maxconn 64",
        "             server rev1_devC 10.1.1.31:5000 check maxconn 64",
        "    "
    ]
}
2025-08-06 12:36:54,864 p=1217120 u=ubuntu n=ansible | RUNNING HANDLER [Restart HAProxy service] *****************************************************************************************************************************
2025-08-06 12:36:57,344 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:57,349 p=1217120 u=ubuntu n=ansible | RUNNING HANDLER [Restart rsyslog service] *****************************************************************************************************************************
2025-08-06 12:36:59,493 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:36:59,504 p=1217120 u=ubuntu n=ansible | PLAY [Install the Grafana Alloy Agent on HAproxy] *********************************************************************************************************************
2025-08-06 12:36:59,509 p=1217120 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-06 12:37:02,185 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-06 12:37:02,196 p=1217120 u=ubuntu n=ansible | TASK [Install the Grafana Alloy Agent] ********************************************************************************************************************************
2025-08-06 12:37:12,528 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:37:12,532 p=1217120 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-06 12:37:14,505 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:37:14,510 p=1217120 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-06 12:37:14,525 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Wed 2025-08-06 12:37:11 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 11672 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 36.8M",
        "     CGroup: /system.slice/alloy.service",
        "             └─11672 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576265845Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=livedebugging duration=12.063µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576277487Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=ui duration=1.812µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576295933Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a duration=255.436219ms",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576777516Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.578118196Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.578367808Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.579002576Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.592650935Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=011c916fbe4f9a01c209d3c05b6a745f",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.592681589Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=011c916fbe4f9a01c209d3c05b6a745f duration=151.118µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.603825321Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-06 12:37:14,530 p=1217120 u=ubuntu n=ansible | TASK [DeployAlloy config file] ****************************************************************************************************************************************
2025-08-06 12:37:18,068 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:37:18,073 p=1217120 u=ubuntu n=ansible | TASK [Restart the Grafana alloy service] ******************************************************************************************************************************
2025-08-06 12:37:20,224 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:37:20,229 p=1217120 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-06 12:37:22,140 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:37:22,147 p=1217120 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-06 12:37:22,162 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Wed 2025-08-06 12:37:19 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 13261 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 39.1M",
        "     CGroup: /system.slice/alloy.service",
        "             └─13261 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232084911Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=logging duration=15.646572ms",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232137171Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=labelstore duration=33.307µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232159323Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 duration=60.535183ms",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.233443862Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.234301231Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.23631355Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.240799677Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.242439644Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=aba169a0479105003a4e37f521868627",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.242669614Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=aba169a0479105003a4e37f521868627 duration=304.052µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.254832891Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-06 12:37:22,168 p=1217120 u=ubuntu n=ansible | TASK [Check the Grafana alloy logs] ***********************************************************************************************************************************
2025-08-06 12:37:24,110 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:37:24,115 p=1217120 u=ubuntu n=ansible | TASK [Display the Grafana alloy logs] *********************************************************************************************************************************
2025-08-06 12:37:24,133 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_logs_response.stdout_lines": [
        "-- Logs begin at Wed 2025-08-06 12:31:59 UTC, end at Wed 2025-08-06 12:37:23 UTC. --",
        "Aug 06 12:37:11 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.337180506Z level=info \"boringcrypto enabled\"=false",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.315795271Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.339125968Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.33928364Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.339449295Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.339583784Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=0",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.339771136Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a05cde",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.339886156Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.339987752Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=prometheus.remote_write.metrics_service duration=16.121273ms",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.340102992Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=logging duration=2.933065ms",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.340226761Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=otel duration=4.956µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.340361802Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=labelstore duration=12.206µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.345144018Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=loki.write.grafana_cloud_loki duration=4.670235ms",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.345327245Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=tracing duration=9.426µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.347809432Z level=info msg=\"running usage stats reporter\"",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.349965019Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a05cde",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576115821Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=remotecfg duration=230.655895ms",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576200155Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576219518Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=http duration=42.044µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576244677Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=cluster duration=13.234µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576265845Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=livedebugging duration=12.063µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576277487Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a node_id=ui duration=1.812µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576295933Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=aeca578b043ba1d27fdbf37c587e5e4a duration=255.436219ms",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.576777516Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.578118196Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.578367808Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.579002576Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.592650935Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=011c916fbe4f9a01c209d3c05b6a745f",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.592681589Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=011c916fbe4f9a01c209d3c05b6a745f duration=151.118µs",
        "Aug 06 12:37:12 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:12.603825321Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: interrupt received",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.725387144Z level=info msg=\"node exited without error\" node=livedebugging",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.7255434Z level=error msg=\"failed to start reporter\" err=\"context canceled\"",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.725602817Z level=info msg=\"node exited without error\" node=ui",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.725679751Z level=info msg=\"node exited without error\" node=loki.write.grafana_cloud_loki",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.725703334Z level=info msg=\"node exited without error\" node=otel",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.725728895Z level=info msg=\"node exited without error\" node=labelstore",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.725754392Z level=info msg=\"node exited without error\" node=remotecfg",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.727215202Z level=info msg=\"Stopping remote storage...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 06 12:37:19 rev1-haproxy systemd[1]: Stopping Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines...",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.728338352Z level=info msg=\"http server closed\" service=http addr=127.0.0.1:12345 err=\"http: Server closed\"",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.732849843Z level=info msg=\"WAL watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a05cde",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.733014018Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.733180884Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.733754415Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.733897528Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.734030771Z level=info msg=\"http server closed\" service=http addr=memory err=\"http: Server closed\"",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.734200933Z level=info msg=\"node exited without error\" node=http",
        "Aug 06 12:37:19 rev1-haproxy alloy[11672]: ts=2025-08-06T12:37:19.734356238Z level=info msg=\"node exited without error\" node=cluster",
        "Aug 06 12:37:19 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 06 12:37:19 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 06 12:37:19 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.216442685Z level=info \"boringcrypto enabled\"=false",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.170757142Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.218488615Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.218661632Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.230538786Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.230692024Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=1",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.230833925Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=1",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.23098007Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a05cde",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.231108054Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.231229284Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=prometheus.remote_write.metrics_service duration=8.277254ms",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.231362815Z level=info msg=\"running usage stats reporter\"",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.231375137Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a05cde url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a05cde",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.231384865Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=remotecfg duration=31.213389ms",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.23139476Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.231400257Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=http duration=30.284µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.231407814Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=cluster duration=1.378µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.23141443Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=otel duration=3.726µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.23200348Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=livedebugging duration=5.105µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232026647Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=ui duration=1.048µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232034795Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=150.506µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232043533Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=4.086238ms",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232051407Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=loki.write.grafana_cloud_loki duration=777.373µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232058835Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=tracing duration=8.286µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232084911Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=logging duration=15.646572ms",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232137171Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 node_id=labelstore duration=33.307µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.232159323Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=69ef5e166b347c49f43bf6e52796d961 duration=60.535183ms",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.233443862Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.234301231Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.23631355Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.240799677Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.242439644Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=aba169a0479105003a4e37f521868627",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.242669614Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=aba169a0479105003a4e37f521868627 duration=304.052µs",
        "Aug 06 12:37:20 rev1-haproxy alloy[13261]: ts=2025-08-06T12:37:20.254832891Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-06 12:37:24,138 p=1217120 u=ubuntu n=ansible | TASK [Check the Grafana alloy configuration file] *********************************************************************************************************************
2025-08-06 12:37:26,085 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:37:26,090 p=1217120 u=ubuntu n=ansible | TASK [Display the Grafana alloy config] *******************************************************************************************************************************
2025-08-06 12:37:26,106 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_config_response.stdout_lines": [
        "remotecfg {",
        "  url            = \"https://fleet-management-prod-016.grafana.net\"",
        "  id             = \"rev1-haproxy\"",
        "  poll_frequency = \"60s\"",
        "",
        "  basic_auth {",
        "    username = \"1303247\"",
        "    password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "  }",
        "}",
        "",
        "prometheus.remote_write \"metrics_service\" {",
        "  endpoint {",
        "    url = \"https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push\"",
        "    basic_auth {",
        "      username = \"2530729\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "loki.write \"grafana_cloud_loki\" {",
        "  endpoint {",
        "    url = \"https://logs-prod-025.grafana.net/loki/api/v1/push\"",
        "    basic_auth {",
        "      username = \"1261041\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "discovery.relabel \"metrics_integrations_integrations_haproxy\" {",
        "  targets = [{",
        "    __address__ = \"127.0.0.1:80\",",
        "  }]",
        "",
        "  rule {",
        "    target_label = \"instance\"",
        "    replacement  = constants.hostname",
        "  }",
        "}",
        "",
        "prometheus.scrape \"metrics_integrations_integrations_haproxy\" {",
        "  targets    = discovery.relabel.metrics_integrations_integrations_haproxy.output",
        "  forward_to = [prometheus.remote_write.metrics_service.receiver]",
        "  job_name   = \"integrations/haproxy\"",
        "}"
    ]
}
2025-08-06 12:37:26,122 p=1217120 u=ubuntu n=ansible | [WARNING]: Found variable using reserved name: timeout

2025-08-06 12:37:26,122 p=1217120 u=ubuntu n=ansible | PLAY [Install snmp, snmpd, NGINX UDP load balancer config for SNMP] ***************************************************************************************************
2025-08-06 12:37:26,124 p=1217120 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-06 12:37:31,068 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-06 12:37:31,081 p=1217120 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-06 12:37:48,422 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=nginx)
2025-08-06 12:37:57,845 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmpd)
2025-08-06 12:38:04,660 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp)
2025-08-06 12:38:17,007 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp-mibs-downloader)
2025-08-06 12:38:17,015 p=1217120 u=ubuntu n=ansible | TASK [Deploy NGINX stream config for SNMP UDP load balancing] *********************************************************************************************************
2025-08-06 12:38:20,284 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-06 12:38:20,289 p=1217120 u=ubuntu n=ansible | TASK [Check nginx is running] *****************************************************************************************************************************************
2025-08-06 12:38:22,159 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-06 12:38:22,164 p=1217120 u=ubuntu n=ansible | TASK [display nginx status] *******************************************************************************************************************************************
2025-08-06 12:38:22,188 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX] => {
    "nginx_running_status": {
        "changed": false,
        "failed": false,
        "name": "nginx",
        "state": "started",
        "status": {
            "ActiveEnterTimestamp": "Wed 2025-08-06 12:37:45 UTC",
            "ActiveEnterTimestampMonotonic": "345930759",
            "ActiveExitTimestampMonotonic": "0",
            "ActiveState": "active",
            "After": "network.target systemd-journald.socket basic.target sysinit.target system.slice",
            "AllowIsolate": "no",
            "AllowedCPUs": "",
            "AllowedMemoryNodes": "",
            "AmbientCapabilities": "",
            "AssertResult": "yes",
            "AssertTimestamp": "Wed 2025-08-06 12:37:45 UTC",
            "AssertTimestampMonotonic": "345876435",
            "Before": "multi-user.target shutdown.target",
            "BlockIOAccounting": "no",
            "BlockIOWeight": "[not set]",
            "CPUAccounting": "no",
            "CPUAffinity": "",
            "CPUAffinityFromNUMA": "no",
            "CPUQuotaPerSecUSec": "infinity",
            "CPUQuotaPeriodUSec": "infinity",
            "CPUSchedulingPolicy": "0",
            "CPUSchedulingPriority": "0",
            "CPUSchedulingResetOnFork": "no",
            "CPUShares": "[not set]",
            "CPUUsageNSec": "[not set]",
            "CPUWeight": "[not set]",
            "CacheDirectoryMode": "0755",
            "CanIsolate": "no",
            "CanReload": "yes",
            "CanStart": "yes",
            "CanStop": "yes",
            "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read",
            "CleanResult": "success",
            "CollectMode": "inactive",
            "ConditionResult": "yes",
            "ConditionTimestamp": "Wed 2025-08-06 12:37:45 UTC",
            "ConditionTimestampMonotonic": "345876435",
            "ConfigurationDirectoryMode": "0755",
            "Conflicts": "shutdown.target",
            "ControlGroup": "/system.slice/nginx.service",
            "ControlPID": "0",
            "DefaultDependencies": "yes",
            "DefaultMemoryLow": "0",
            "DefaultMemoryMin": "0",
            "Delegate": "no",
            "Description": "A high performance web server and a reverse proxy server",
            "DevicePolicy": "auto",
            "Documentation": "man:nginx(8)",
            "DynamicUser": "no",
            "EffectiveCPUs": "",
            "EffectiveMemoryNodes": "",
            "ExecMainCode": "0",
            "ExecMainExitTimestampMonotonic": "0",
            "ExecMainPID": "3434",
            "ExecMainStartTimestamp": "Wed 2025-08-06 12:37:45 UTC",
            "ExecMainStartTimestampMonotonic": "345930741",
            "ExecMainStatus": "0",
            "ExecReload": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecReloadEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStart": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPre": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPreEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStop": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStopEx": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "FailureAction": "none",
            "FileDescriptorStoreMax": "0",
            "FinalKillSignal": "9",
            "FragmentPath": "/lib/systemd/system/nginx.service",
            "GID": "[not set]",
            "GuessMainPID": "yes",
            "IOAccounting": "no",
            "IOReadBytes": "18446744073709551615",
            "IOReadOperations": "18446744073709551615",
            "IOSchedulingClass": "0",
            "IOSchedulingPriority": "0",
            "IOWeight": "[not set]",
            "IOWriteBytes": "18446744073709551615",
            "IOWriteOperations": "18446744073709551615",
            "IPAccounting": "no",
            "IPEgressBytes": "[no data]",
            "IPEgressPackets": "[no data]",
            "IPIngressBytes": "[no data]",
            "IPIngressPackets": "[no data]",
            "Id": "nginx.service",
            "IgnoreOnIsolate": "no",
            "IgnoreSIGPIPE": "yes",
            "InactiveEnterTimestampMonotonic": "0",
            "InactiveExitTimestamp": "Wed 2025-08-06 12:37:45 UTC",
            "InactiveExitTimestampMonotonic": "345877709",
            "InvocationID": "b0a7f3fea6784401a0950771eb00f4db",
            "JobRunningTimeoutUSec": "infinity",
            "JobTimeoutAction": "none",
            "JobTimeoutUSec": "infinity",
            "KeyringMode": "private",
            "KillMode": "mixed",
            "KillSignal": "15",
            "LimitAS": "infinity",
            "LimitASSoft": "infinity",
            "LimitCORE": "infinity",
            "LimitCORESoft": "0",
            "LimitCPU": "infinity",
            "LimitCPUSoft": "infinity",
            "LimitDATA": "infinity",
            "LimitDATASoft": "infinity",
            "LimitFSIZE": "infinity",
            "LimitFSIZESoft": "infinity",
            "LimitLOCKS": "infinity",
            "LimitLOCKSSoft": "infinity",
            "LimitMEMLOCK": "65536",
            "LimitMEMLOCKSoft": "65536",
            "LimitMSGQUEUE": "819200",
            "LimitMSGQUEUESoft": "819200",
            "LimitNICE": "0",
            "LimitNICESoft": "0",
            "LimitNOFILE": "524288",
            "LimitNOFILESoft": "1024",
            "LimitNPROC": "15295",
            "LimitNPROCSoft": "15295",
            "LimitRSS": "infinity",
            "LimitRSSSoft": "infinity",
            "LimitRTPRIO": "0",
            "LimitRTPRIOSoft": "0",
            "LimitRTTIME": "infinity",
            "LimitRTTIMESoft": "infinity",
            "LimitSIGPENDING": "15295",
            "LimitSIGPENDINGSoft": "15295",
            "LimitSTACK": "infinity",
            "LimitSTACKSoft": "8388608",
            "LoadState": "loaded",
            "LockPersonality": "no",
            "LogLevelMax": "-1",
            "LogRateLimitBurst": "0",
            "LogRateLimitIntervalUSec": "0",
            "LogsDirectoryMode": "0755",
            "MainPID": "3434",
            "MemoryAccounting": "yes",
            "MemoryCurrent": "5496832",
            "MemoryDenyWriteExecute": "no",
            "MemoryHigh": "infinity",
            "MemoryLimit": "infinity",
            "MemoryLow": "0",
            "MemoryMax": "infinity",
            "MemoryMin": "0",
            "MemorySwapMax": "infinity",
            "MountAPIVFS": "no",
            "MountFlags": "",
            "NFileDescriptorStore": "0",
            "NRestarts": "0",
            "NUMAMask": "",
            "NUMAPolicy": "n/a",
            "Names": "nginx.service",
            "NeedDaemonReload": "no",
            "Nice": "0",
            "NoNewPrivileges": "no",
            "NonBlocking": "no",
            "NotifyAccess": "none",
            "OOMPolicy": "stop",
            "OOMScoreAdjust": "0",
            "OnFailureJobMode": "replace",
            "PIDFile": "/run/nginx.pid",
            "Perpetual": "no",
            "PrivateDevices": "no",
            "PrivateMounts": "no",
            "PrivateNetwork": "no",
            "PrivateTmp": "no",
            "PrivateUsers": "no",
            "ProtectControlGroups": "no",
            "ProtectHome": "no",
            "ProtectHostname": "no",
            "ProtectKernelLogs": "no",
            "ProtectKernelModules": "no",
            "ProtectKernelTunables": "no",
            "ProtectSystem": "no",
            "RefuseManualStart": "no",
            "RefuseManualStop": "no",
            "ReloadResult": "success",
            "RemainAfterExit": "no",
            "RemoveIPC": "no",
            "Requires": "sysinit.target system.slice",
            "Restart": "no",
            "RestartKillSignal": "15",
            "RestartUSec": "100ms",
            "RestrictNamespaces": "no",
            "RestrictRealtime": "no",
            "RestrictSUIDSGID": "no",
            "Result": "success",
            "RootDirectoryStartOnly": "no",
            "RuntimeDirectoryMode": "0755",
            "RuntimeDirectoryPreserve": "no",
            "RuntimeMaxUSec": "infinity",
            "SameProcessGroup": "no",
            "SecureBits": "0",
            "SendSIGHUP": "no",
            "SendSIGKILL": "yes",
            "Slice": "system.slice",
            "StandardError": "inherit",
            "StandardInput": "null",
            "StandardInputData": "",
            "StandardOutput": "journal",
            "StartLimitAction": "none",
            "StartLimitBurst": "5",
            "StartLimitIntervalUSec": "10s",
            "StartupBlockIOWeight": "[not set]",
            "StartupCPUShares": "[not set]",
            "StartupCPUWeight": "[not set]",
            "StartupIOWeight": "[not set]",
            "StateChangeTimestamp": "Wed 2025-08-06 12:37:45 UTC",
            "StateChangeTimestampMonotonic": "345930759",
            "StateDirectoryMode": "0755",
            "StatusErrno": "0",
            "StopWhenUnneeded": "no",
            "SubState": "running",
            "SuccessAction": "none",
            "SyslogFacility": "3",
            "SyslogLevel": "6",
            "SyslogLevelPrefix": "yes",
            "SyslogPriority": "30",
            "SystemCallErrorNumber": "0",
            "TTYReset": "no",
            "TTYVHangup": "no",
            "TTYVTDisallocate": "no",
            "TasksAccounting": "yes",
            "TasksCurrent": "2",
            "TasksMax": "4588",
            "TimeoutAbortUSec": "5s",
            "TimeoutCleanUSec": "infinity",
            "TimeoutStartUSec": "1min 30s",
            "TimeoutStopUSec": "5s",
            "TimerSlackNSec": "50000",
            "Transient": "no",
            "Type": "forking",
            "UID": "[not set]",
            "UMask": "0022",
            "UnitFilePreset": "enabled",
            "UnitFileState": "enabled",
            "UtmpMode": "init",
            "WantedBy": "multi-user.target",
            "WatchdogSignal": "6",
            "WatchdogTimestampMonotonic": "0",
            "WatchdogUSec": "0"
        }
    }
}
2025-08-06 12:38:22,199 p=1217120 u=ubuntu n=ansible | RUNNING HANDLER [Reload NGINX] ****************************************************************************************************************************************
2025-08-06 12:38:24,116 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-06 12:38:24,126 p=1217120 u=ubuntu n=ansible | PLAY [Test HAProxy (http), HAProxy Web stats (STATS)+ Metrics (PROMEX) and HAProxy logs] ******************************************************************************
2025-08-06 12:38:24,133 p=1217120 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-06 12:38:27,402 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-06 12:38:27,413 p=1217120 u=ubuntu n=ansible | TASK [Gather HAProxy server public IP address] ************************************************************************************************************************
2025-08-06 12:38:29,704 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-06 12:38:29,709 p=1217120 u=ubuntu n=ansible | TASK [Send HTTP request to HAProxy and collect response] **************************************************************************************************************
2025-08-06 12:38:36,005 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0)
2025-08-06 12:38:38,020 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1)
2025-08-06 12:38:48,094 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2)
2025-08-06 12:38:48,100 p=1217120 u=ubuntu n=ansible | TASK [Display the HAProxy response] ***********************************************************************************************************************************
2025-08-06 12:38:48,117 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "12:38:35 10.1.1.43:46888 -- 10.1.1.48 (rev1-devb) 81\n",
    "item": 0
}
2025-08-06 12:38:48,121 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "12:38:37 10.1.1.43:47196 -- 10.1.1.31 (rev1-devc) 20\n",
    "item": 1
}
2025-08-06 12:38:48,124 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "12:38:47 10.1.1.43:41518 -- 10.1.1.44 (rev1-deva) 94\n",
    "item": 2
}
2025-08-06 12:38:48,130 p=1217120 u=ubuntu n=ansible | TASK [Send HTTP requests to HAProxy stats page and collect responses] *************************************************************************************************
2025-08-06 12:39:02,356 p=1217120 u=ubuntu n=ansible | fatal: [rev1_HAproxy]: FAILED! => {"attempts": 3, "changed": false, "connection": "close", "content": "<!doctype html>\n<html lang=en>\n<title>404 Not Found</title>\n<h1>Not Found</h1>\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n", "content_length": "207", "content_type": "text/html; charset=utf-8", "date": "Wed, 06 Aug 2025 12:39:02 GMT", "elapsed": 0, "msg": "Status code was 404 and not [200]: HTTP Error 404: NOT FOUND", "redirected": false, "server": "Werkzeug/3.0.6 Python/3.8.10", "status": 404, "url": "http://188.240.223.88/stats;csv"}
2025-08-06 12:39:02,356 p=1217120 u=ubuntu n=ansible | ...ignoring
2025-08-06 12:39:02,361 p=1217120 u=ubuntu n=ansible | TASK [Display the stats response content] *****************************************************************************************************************************
2025-08-06 12:39:02,377 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_stats_response.content": "<!doctype html>\n<html lang=en>\n<title>404 Not Found</title>\n<h1>Not Found</h1>\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\n"
}
2025-08-06 12:39:02,382 p=1217120 u=ubuntu n=ansible | TASK [Test the HAProxy metrics (promex) path] *************************************************************************************************************************
2025-08-06 12:39:04,388 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:39:04,393 p=1217120 u=ubuntu n=ansible | TASK [Display the HAProxy metrics (promex) response content] **********************************************************************************************************
2025-08-06 12:39:04,408 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_metrics_path_result.stdout_lines": [
        "<!doctype html>",
        "<html lang=en>",
        "<title>404 Not Found</title>",
        "<h1>Not Found</h1>",
        "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>"
    ]
}
2025-08-06 12:39:04,413 p=1217120 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-06 12:39:06,365 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:39:06,372 p=1217120 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-06 12:39:06,388 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : path to executable is /usr/sbin/haproxy",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [WARNING]  (4471) : Exiting Master process...",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [ALERT]    (4471) : Current worker (4491) exited with code 143 (Terminated)",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [WARNING]  (4471) : All workers exited. Exiting... (0)",
        "Aug  6 12:36:56 rev1-haproxy haproxy[10244]: [NOTICE]   (10244) : New worker (10246) forked",
        "Aug  6 12:36:56 rev1-haproxy haproxy[10244]: [NOTICE]   (10244) : Loading success.",
        "Aug  6 12:38:11 rev1-haproxy haproxy[10246]: 127.0.0.1:34910 [06/Aug/2025:12:38:11.737] haproxy_frontend haproxy_backend/rev1_devA 0/0/1/8/9 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:22024 cpu_ns_avg:5506 lat_ns_tot:5748 lat_ns_avg:1437",
        "Aug  6 12:38:35 rev1-haproxy haproxy[10246]: 188.240.223.88:20483 [06/Aug/2025:12:38:35.665] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/5/5 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14041 cpu_ns_avg:3510 lat_ns_tot:5430 lat_ns_avg:1357",
        "Aug  6 12:38:37 rev1-haproxy haproxy[10246]: 188.240.223.88:9287 [06/Aug/2025:12:38:37.695] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14925 cpu_ns_avg:3731 lat_ns_tot:5338 lat_ns_avg:1334",
        "Aug  6 12:38:47 rev1-haproxy haproxy[10246]: 188.240.223.88:38652 [06/Aug/2025:12:38:47.776] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/3/4 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12646 cpu_ns_avg:3161 lat_ns_tot:5601 lat_ns_avg:1400",
        "Aug  6 12:38:49 rev1-haproxy haproxy[10246]: 188.240.223.88:47665 [06/Aug/2025:12:38:49.880] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/3/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13905 cpu_ns_avg:3476 lat_ns_tot:6074 lat_ns_avg:1518",
        "Aug  6 12:38:53 rev1-haproxy haproxy[10246]: 188.240.223.88:43381 [06/Aug/2025:12:38:53.901] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15847 cpu_ns_avg:3961 lat_ns_tot:5262 lat_ns_avg:1315",
        "Aug  6 12:38:57 rev1-haproxy haproxy[10246]: 188.240.223.88:12097 [06/Aug/2025:12:38:57.987] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/5/5 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12362 cpu_ns_avg:3090 lat_ns_tot:5485 lat_ns_avg:1371",
        "Aug  6 12:39:02 rev1-haproxy haproxy[10246]: 188.240.223.88:65321 [06/Aug/2025:12:39:02.030] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/4/4 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15588 cpu_ns_avg:3897 lat_ns_tot:5727 lat_ns_avg:1431",
        "Aug  6 12:39:04 rev1-haproxy haproxy[10246]: 188.240.223.88:47854 [06/Aug/2025:12:39:04.067] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10459 cpu_ns_avg:2614 lat_ns_tot:5532 lat_ns_avg:1383"
    ]
}
2025-08-06 12:39:06,393 p=1217120 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-06 12:39:08,301 p=1217120 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-06 12:39:08,307 p=1217120 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-06 12:39:08,324 p=1217120 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [NOTICE]   (4471) : path to executable is /usr/sbin/haproxy",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [WARNING]  (4471) : Exiting Master process...",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [ALERT]    (4471) : Current worker (4491) exited with code 143 (Terminated)",
        "Aug  6 12:36:56 rev1-haproxy haproxy[4471]: [WARNING]  (4471) : All workers exited. Exiting... (0)",
        "Aug  6 12:36:56 rev1-haproxy haproxy[10244]: [NOTICE]   (10244) : New worker (10246) forked",
        "Aug  6 12:36:56 rev1-haproxy haproxy[10244]: [NOTICE]   (10244) : Loading success.",
        "Aug  6 12:38:11 rev1-haproxy haproxy[10246]: 127.0.0.1:34910 [06/Aug/2025:12:38:11.737] haproxy_frontend haproxy_backend/rev1_devA 0/0/1/8/9 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:22024 cpu_ns_avg:5506 lat_ns_tot:5748 lat_ns_avg:1437",
        "Aug  6 12:38:35 rev1-haproxy haproxy[10246]: 188.240.223.88:20483 [06/Aug/2025:12:38:35.665] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/5/5 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14041 cpu_ns_avg:3510 lat_ns_tot:5430 lat_ns_avg:1357",
        "Aug  6 12:38:37 rev1-haproxy haproxy[10246]: 188.240.223.88:9287 [06/Aug/2025:12:38:37.695] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14925 cpu_ns_avg:3731 lat_ns_tot:5338 lat_ns_avg:1334",
        "Aug  6 12:38:47 rev1-haproxy haproxy[10246]: 188.240.223.88:38652 [06/Aug/2025:12:38:47.776] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/3/4 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12646 cpu_ns_avg:3161 lat_ns_tot:5601 lat_ns_avg:1400",
        "Aug  6 12:38:49 rev1-haproxy haproxy[10246]: 188.240.223.88:47665 [06/Aug/2025:12:38:49.880] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/3/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13905 cpu_ns_avg:3476 lat_ns_tot:6074 lat_ns_avg:1518",
        "Aug  6 12:38:53 rev1-haproxy haproxy[10246]: 188.240.223.88:43381 [06/Aug/2025:12:38:53.901] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15847 cpu_ns_avg:3961 lat_ns_tot:5262 lat_ns_avg:1315",
        "Aug  6 12:38:57 rev1-haproxy haproxy[10246]: 188.240.223.88:12097 [06/Aug/2025:12:38:57.987] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/5/5 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12362 cpu_ns_avg:3090 lat_ns_tot:5485 lat_ns_avg:1371",
        "Aug  6 12:39:02 rev1-haproxy haproxy[10246]: 188.240.223.88:65321 [06/Aug/2025:12:39:02.030] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/4/4 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15588 cpu_ns_avg:3897 lat_ns_tot:5727 lat_ns_avg:1431",
        "Aug  6 12:39:04 rev1-haproxy haproxy[10246]: 188.240.223.88:47854 [06/Aug/2025:12:39:04.067] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10459 cpu_ns_avg:2614 lat_ns_tot:5532 lat_ns_avg:1383"
    ]
}
2025-08-06 12:39:08,340 p=1217120 u=ubuntu n=ansible | PLAY [Test NGINX (snmp) proxy] ****************************************************************************************************************************************
2025-08-06 12:39:08,345 p=1217120 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-06 12:39:11,103 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-06 12:39:11,114 p=1217120 u=ubuntu n=ansible | TASK [Gather NGINX public IP address] *********************************************************************************************************************************
2025-08-06 12:39:13,058 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-06 12:39:13,065 p=1217120 u=ubuntu n=ansible | TASK [Send SNMP request to NGINX server and collect responses] ********************************************************************************************************
2025-08-06 12:39:14,847 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=0)
2025-08-06 12:39:16,588 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=1)
2025-08-06 12:39:18,346 p=1217120 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=2)
2025-08-06 12:39:18,352 p=1217120 u=ubuntu n=ansible | TASK [Display the NGINX response content] *****************************************************************************************************************************
2025-08-06 12:39:18,369 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=0) => {
    "ansible_loop_var": "item",
    "item": 0,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-06 12:39:18,372 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=1) => {
    "ansible_loop_var": "item",
    "item": 1,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-06 12:39:18,375 p=1217120 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=2) => {
    "ansible_loop_var": "item",
    "item": 2,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-06 12:39:18,391 p=1217120 u=ubuntu n=ansible | PLAY RECAP ************************************************************************************************************************************************************
2025-08-06 12:39:18,391 p=1217120 u=ubuntu n=ansible | rev1_HAproxy               : ok=46   changed=22   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1   
2025-08-06 12:39:18,392 p=1217120 u=ubuntu n=ansible | rev1_NGINX                 : ok=10   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-06 12:39:18,392 p=1217120 u=ubuntu n=ansible | rev1_devA                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-06 12:39:18,392 p=1217120 u=ubuntu n=ansible | rev1_devB                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-06 12:39:18,392 p=1217120 u=ubuntu n=ansible | rev1_devC                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 17:12:10,257 p=1266872 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] **************************************************************************************************************
2025-08-07 17:12:10,266 p=1266872 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 17:12:22,321 p=1266872 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-07 17:12:22,539 p=1266872 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-07 17:12:22,575 p=1266872 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-07 17:12:22,599 p=1266872 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-07 17:12:35,417 p=1266872 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3)
2025-08-07 17:12:36,267 p=1266872 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3)
2025-08-07 17:12:36,519 p=1266872 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3)
2025-08-07 17:12:46,531 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA] => (item=python3-pip)
2025-08-07 17:12:47,198 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB] => (item=python3-pip)
2025-08-07 17:12:47,539 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC] => (item=python3-pip)
2025-08-07 17:12:58,975 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmpd)
2025-08-07 17:12:59,144 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmpd)
2025-08-07 17:13:00,561 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmpd)
2025-08-07 17:13:08,259 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp)
2025-08-07 17:13:08,430 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp)
2025-08-07 17:13:10,436 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp)
2025-08-07 17:13:22,520 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp-mibs-downloader)
2025-08-07 17:13:23,144 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp-mibs-downloader)
2025-08-07 17:13:25,768 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp-mibs-downloader)
2025-08-07 17:13:25,775 p=1266872 u=ubuntu n=ansible | TASK [Install Flask] **************************************************************************************************************************************************
2025-08-07 17:13:32,593 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:13:33,215 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:13:33,366 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:13:33,372 p=1266872 u=ubuntu n=ansible | TASK [Deploy the Flask application config for TCP Load Balancing] *****************************************************************************************************
2025-08-07 17:13:41,607 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:13:41,655 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:13:42,221 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:13:42,228 p=1266872 u=ubuntu n=ansible | TASK [Start Flask app in background on port 5000] *********************************************************************************************************************
2025-08-07 17:13:46,948 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:13:47,154 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:13:47,194 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:13:47,199 p=1266872 u=ubuntu n=ansible | TASK [Check Flask app HTTP response on private IP] ********************************************************************************************************************
2025-08-07 17:13:51,136 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:13:51,758 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:13:51,815 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:13:51,821 p=1266872 u=ubuntu n=ansible | TASK [Display Flask app HTTP response on private IP] ******************************************************************************************************************
2025-08-07 17:13:51,847 p=1266872 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "17:13:50 10.1.1.36:51132 -- 10.1.1.36 (rev1-deva) 39"
}
2025-08-07 17:13:51,849 p=1266872 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "17:13:51 10.1.1.12:54482 -- 10.1.1.12 (rev1-devb) 36"
}
2025-08-07 17:13:51,859 p=1266872 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "17:13:51 10.1.1.42:43860 -- 10.1.1.42 (rev1-devc) 46"
}
2025-08-07 17:13:51,864 p=1266872 u=ubuntu n=ansible | TASK [Remove the existing agent address lines] ************************************************************************************************************************
2025-08-07 17:13:56,575 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:13:56,849 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:13:56,860 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:13:56,868 p=1266872 u=ubuntu n=ansible | TASK [Configure agent address (0.0.0.0) for SNMPd to listen on all UDP interfaces] ************************************************************************************
2025-08-07 17:14:00,779 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:14:01,268 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:14:01,340 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:14:01,346 p=1266872 u=ubuntu n=ansible | TASK [Check snmpd config File] ****************************************************************************************************************************************
2025-08-07 17:14:05,281 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:14:05,886 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:14:05,924 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:14:05,929 p=1266872 u=ubuntu n=ansible | TASK [Display snmpd configuration file] *******************************************************************************************************************************
2025-08-07 17:14:05,956 p=1266872 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003135",
        "end": "2025-08-07 17:14:04.687768",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 17:14:04.684633",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 17:14:05,962 p=1266872 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003105",
        "end": "2025-08-07 17:14:05.226357",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 17:14:05.223252",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 17:14:05,971 p=1266872 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003103",
        "end": "2025-08-07 17:14:05.288873",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 17:14:05.285770",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 17:14:05,976 p=1266872 u=ubuntu n=ansible | TASK [Restart SNMPD if agent address configuration is changed] ********************************************************************************************************
2025-08-07 17:14:11,030 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:14:11,279 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:14:11,305 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:14:11,311 p=1266872 u=ubuntu n=ansible | TASK [Test SNMPd with snmpget on 10.1.1.36] ***************************************************************************************************************************
2025-08-07 17:14:15,261 p=1266872 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 17:14:15,902 p=1266872 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 17:14:15,962 p=1266872 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 17:14:15,967 p=1266872 u=ubuntu n=ansible | TASK [Print SNMPd snmpget result] *************************************************************************************************************************************
2025-08-07 17:14:15,991 p=1266872 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-07 17:14:15,993 p=1266872 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-07 17:14:16,002 p=1266872 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-07 17:14:16,077 p=1266872 u=ubuntu n=ansible | PLAY [Set up HAProxy] *************************************************************************************************************************************************
2025-08-07 17:14:16,080 p=1266872 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 17:14:21,580 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:14:21,592 p=1266872 u=ubuntu n=ansible | TASK [Add HAProxy 2.9 PPA] ********************************************************************************************************************************************
2025-08-07 17:14:35,489 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:14:35,494 p=1266872 u=ubuntu n=ansible | TASK [Install HAProxy] ************************************************************************************************************************************************
2025-08-07 17:14:48,485 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:14:48,489 p=1266872 u=ubuntu n=ansible | TASK [Deploy stats web page password file] ****************************************************************************************************************************
2025-08-07 17:14:52,229 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:14:52,234 p=1266872 u=ubuntu n=ansible | TASK [Read stats page password from file] *****************************************************************************************************************************
2025-08-07 17:14:54,398 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:14:54,404 p=1266872 u=ubuntu n=ansible | TASK [Set up HAProxy stats secret variable] ***************************************************************************************************************************
2025-08-07 17:14:54,457 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:14:54,463 p=1266872 u=ubuntu n=ansible | TASK [Configure HAProxy] **********************************************************************************************************************************************
2025-08-07 17:14:58,174 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:14:58,179 p=1266872 u=ubuntu n=ansible | TASK [Deploy rsyslog 49-haproxy config file] **************************************************************************************************************************
2025-08-07 17:15:01,935 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:01,943 p=1266872 u=ubuntu n=ansible | TASK [Return 49_haproxy_conf to registered rsyslog_49_haproxy_conf] ***************************************************************************************************
2025-08-07 17:15:03,989 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:03,994 p=1266872 u=ubuntu n=ansible | TASK [debug] **********************************************************************************************************************************************************
2025-08-07 17:15:04,010 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "rsyslog_49_haproxy_conf.stdout_lines": [
        "# Create an additional socket in haproxy's chroot in order to allow logging via",
        "# /dev/log to chroot'ed HAProxy processes",
        "$AddUnixListenSocket /var/lib/haproxy/dev/log",
        "",
        "# Send HAProxy messages to a dedicated logfile",
        ":programname, startswith, \"haproxy\" {",
        "  /var/log/haproxy.log",
        "stop",
        "}"
    ]
}
2025-08-07 17:15:04,015 p=1266872 u=ubuntu n=ansible | TASK [Test HAProxy Configurations] ************************************************************************************************************************************
2025-08-07 17:15:06,057 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:06,062 p=1266872 u=ubuntu n=ansible | TASK [Display HAProxy config test result] *****************************************************************************************************************************
2025-08-07 17:15:06,078 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_configs_test_result": {
        "changed": true,
        "cmd": [
            "haproxy",
            "-f",
            "/etc/haproxy/haproxy.cfg",
            "-c"
        ],
        "delta": "0:00:00.028070",
        "end": "2025-08-07 17:15:05.727872",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 17:15:05.699802",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "",
        "stdout_lines": []
    }
}
2025-08-07 17:15:06,083 p=1266872 u=ubuntu n=ansible | TASK [Test HAProxy is running] ****************************************************************************************************************************************
2025-08-07 17:15:08,204 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:15:08,211 p=1266872 u=ubuntu n=ansible | TASK [Display the HAProxy service status] *****************************************************************************************************************************
2025-08-07 17:15:08,227 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_service_status.state": "started"
}
2025-08-07 17:15:08,232 p=1266872 u=ubuntu n=ansible | TASK [Check HAProxy server status] ************************************************************************************************************************************
2025-08-07 17:15:10,219 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:10,225 p=1266872 u=ubuntu n=ansible | TASK [Display HAProxy server status] **********************************************************************************************************************************
2025-08-07 17:15:10,240 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "systemctl_haproxy_service_status.stdout_lines": [
        "● haproxy.service - HAProxy Load Balancer",
        "     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Thu 2025-08-07 17:14:44 UTC; 25s ago",
        "       Docs: man:haproxy(1)",
        "             file:/usr/share/doc/haproxy/configuration.txt.gz",
        "   Main PID: 4487 (haproxy)",
        "     Status: \"Ready.\"",
        "      Tasks: 2 (limit: 4588)",
        "     Memory: 39.8M",
        "     CGroup: /system.slice/haproxy.service",
        "             ├─4487 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "             └─4507 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "",
        "Aug 07 17:14:44 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 07 17:14:44 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : New worker (4507) forked",
        "Aug 07 17:14:44 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 07 17:14:44 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : Loading success."
    ]
}
2025-08-07 17:15:10,245 p=1266872 u=ubuntu n=ansible | TASK [Check HAProxy config errors via journalctl] *********************************************************************************************************************
2025-08-07 17:15:12,194 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:12,199 p=1266872 u=ubuntu n=ansible | TASK [Display HAProxy config errors] **********************************************************************************************************************************
2025-08-07 17:15:12,214 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_journalctl_logs.stdout_lines": [
        "-- Logs begin at Thu 2025-08-07 17:10:23 UTC, end at Thu 2025-08-07 17:15:11 UTC. --",
        "Aug 07 17:14:44 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 07 17:14:44 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : New worker (4507) forked",
        "Aug 07 17:14:44 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 07 17:14:44 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : Loading success."
    ]
}
2025-08-07 17:15:12,219 p=1266872 u=ubuntu n=ansible | TASK [Check the HAProxy configuration file] ***************************************************************************************************************************
2025-08-07 17:15:14,201 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:14,206 p=1266872 u=ubuntu n=ansible | TASK [Display HAProxy configuration file] *****************************************************************************************************************************
2025-08-07 17:15:14,222 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_config_file.stdout_lines": [
        "global",
        "    profiling.tasks on #Enable HAProxy profiling (CPU time spent on processing a http request inside HAProxy)",
        "    nbthread 1 # 1 thread, IDs from 1 to 2, nbthread <number of CPU cores>",
        "    thread-groups 1",
        "    # declare threads",
        "    thread-group 1 1-1",
        "    # bind threads to cpu cores",
        "    cpu-map 1/all 0-0 # bind all threads to CPU 0 #syntax:cpu-map 1/1-<Number Of CPU Cores> 0-<Number of CPU Cores - 1>",
        "    # define logging",
        "    log /dev/log local0 info",
        "    #log /dev/log local0 emerg",
        "    #log /dev/log local1 alert",
        "    #log /dev/log local2 crit",
        "    #log /dev/log local3 err",
        "    #log /dev/log local4 warning",
        "    #log /dev/log local5 notice",
        "    #log /dev/log local6 info",
        "    #log /dev/log local7 debug",
        "    #Security Considerations",
        "    chroot /var/lib/haproxy #chroot statement pointing to a /var/lib/haproxy location",
        "    user haproxy # uid/user statement",
        "    group haproxy # gid/group statement",
        "    stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin",
        "    stats maxconn 20",
        "    stats timeout 30000",
        "    daemon",
        "    #maxconn 512",
        "        ",
        "defaults",
        "    mode http",
        "    timeout connect 5000ms",
        "    timeout client 5000ms",
        "    timeout server 5000ms",
        "    errorfile 400 /etc/haproxy/errors/400.http",
        "    errorfile 403 /etc/haproxy/errors/403.http",
        "    errorfile 408 /etc/haproxy/errors/408.http",
        "    errorfile 500 /etc/haproxy/errors/500.http",
        "    errorfile 502 /etc/haproxy/errors/502.http",
        "    errorfile 503 /etc/haproxy/errors/503.http",
        "    errorfile 504 /etc/haproxy/errors/504.http",
        "",
        "frontend web_stats",
        "    mode http",
        "    bind *:80 ",
        "    http-request use-service prometheus-exporter if { path /metrics }",
        "    stats enable # enable stats page",
        "    stats uri /stats # stats uri",
        "    stats hide-version",
        "    stats refresh 1s",
        "    stats auth admin:uipassword",
        "",
        "frontend haproxy_frontend",
        "    log global",
        "    bind *:80  thread 1/all shards by-thread  #bind this proxy to threads 1 to 1 or all",
        "    mode http",
        "    option httplog",
        "    #option dontlog-normal",
        "    #option logasap",
        "    #define custom log-format",
        "    log-format \"%ci:%cp [%tr] %ft %b/%s %TR/%Tw/%Tc/%Tr/%Ta %ST %B %CC %CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r %[http_first_req] cpu_calls:%[cpu_calls] cpu_ns_tot:%[cpu_ns_tot] cpu_ns_avg:%[cpu_ns_avg] lat_ns_tot:%[lat_ns_tot] lat_ns_avg:%[lat_ns_avg]\"",
        "    default_backend haproxy_backend",
        "    ",
        "backend haproxy_backend",
        "    retry-on all-retryable-errors # This works when conn-failure, empty-response, junk-response, response-timeout, rtt-rejected, 500, 502, 503, and 504",
        "    retries 3",
        "             server rev1_devA 10.1.1.36:5000 check #maxconn 64",
        "             server rev1_devB 10.1.1.12:5000 check #maxconn 64",
        "             server rev1_devC 10.1.1.42:5000 check #maxconn 64",
        "    "
    ]
}
2025-08-07 17:15:14,234 p=1266872 u=ubuntu n=ansible | RUNNING HANDLER [Restart HAProxy service] *****************************************************************************************************************************
2025-08-07 17:15:16,490 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:16,495 p=1266872 u=ubuntu n=ansible | RUNNING HANDLER [Restart rsyslog service] *****************************************************************************************************************************
2025-08-07 17:15:18,695 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:18,707 p=1266872 u=ubuntu n=ansible | PLAY [Install the Grafana Alloy Agent on HAproxy] *********************************************************************************************************************
2025-08-07 17:15:18,712 p=1266872 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 17:15:21,466 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:15:21,478 p=1266872 u=ubuntu n=ansible | TASK [Install the Grafana Alloy Agent] ********************************************************************************************************************************
2025-08-07 17:15:32,738 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:32,743 p=1266872 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-07 17:15:34,850 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:34,855 p=1266872 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-07 17:15:34,872 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Thu 2025-08-07 17:15:32 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 11788 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 40.1M",
        "     CGroup: /system.slice/alloy.service",
        "             └─11788 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.77430523Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=livedebugging duration=19.516µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.774320175Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=ui duration=2.916µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.774377342Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 duration=263.108321ms",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.775387458Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.776744288Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.777001971Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.77781814Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.79882148Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=6c896804114d7ead376a3ac96771a109",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.798871207Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=6c896804114d7ead376a3ac96771a109 duration=127.451µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.811873474Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 17:15:34,877 p=1266872 u=ubuntu n=ansible | TASK [DeployAlloy config file] ****************************************************************************************************************************************
2025-08-07 17:15:38,684 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:38,689 p=1266872 u=ubuntu n=ansible | TASK [Restart the Grafana alloy service] ******************************************************************************************************************************
2025-08-07 17:15:41,008 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:41,013 p=1266872 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-07 17:15:43,106 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:43,114 p=1266872 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-07 17:15:43,132 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Thu 2025-08-07 17:15:40 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 13537 (alloy)",
        "      Tasks: 7 (limit: 4588)",
        "     Memory: 39.2M",
        "     CGroup: /system.slice/alloy.service",
        "             └─13537 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.01156297Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=cluster duration=5.544µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011723346Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=labelstore duration=12.749µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011858942Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d duration=59.399793ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.018637571Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.022113182Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024044618Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024529154Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.02847717Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.028512703Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0 duration=96.551µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.042645073Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 17:15:43,138 p=1266872 u=ubuntu n=ansible | TASK [Check the Grafana alloy logs] ***********************************************************************************************************************************
2025-08-07 17:15:45,184 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:45,189 p=1266872 u=ubuntu n=ansible | TASK [Display the Grafana alloy logs] *********************************************************************************************************************************
2025-08-07 17:15:45,207 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_logs_response.stdout_lines": [
        "-- Logs begin at Thu 2025-08-07 17:10:23 UTC, end at Thu 2025-08-07 17:15:44 UTC. --",
        "Aug 07 17:15:32 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.548888653Z level=info \"boringcrypto enabled\"=false",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.510146822Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.556553319Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.556723801Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.556883116Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=otel duration=3.468µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.557022693Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=labelstore duration=22.717µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.557140473Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=loki.write.grafana_cloud_loki duration=5.035056ms",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.557265828Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.557509026Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=0",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.55762003Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.557734706Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.557833412Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=prometheus.remote_write.metrics_service duration=32.325104ms",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.557936647Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=tracing duration=10.021µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.55805634Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=logging duration=9.182044ms",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.562867471Z level=info msg=\"running usage stats reporter\"",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.564765792Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.774075181Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=remotecfg duration=215.880731ms",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.774227448Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.77423606Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=http duration=52.868µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.774257185Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=cluster duration=8.67µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.77430523Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=livedebugging duration=19.516µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.774320175Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 node_id=ui duration=2.916µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.774377342Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=515d4d5b66ecaf9bcd0ae952557731f0 duration=263.108321ms",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.775387458Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.776744288Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.777001971Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.77781814Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.79882148Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=6c896804114d7ead376a3ac96771a109",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.798871207Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=6c896804114d7ead376a3ac96771a109 duration=127.451µs",
        "Aug 07 17:15:32 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:32.811873474Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: interrupt received",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.513563459Z level=info msg=\"node exited without error\" node=otel",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.513715619Z level=error msg=\"failed to start reporter\" err=\"context canceled\"",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.51392242Z level=info msg=\"http server closed\" service=http addr=127.0.0.1:12345 err=\"http: Server closed\"",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.514586206Z level=info msg=\"node exited without error\" node=loki.write.grafana_cloud_loki",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.514623024Z level=info msg=\"node exited without error\" node=remotecfg",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.514698617Z level=info msg=\"node exited without error\" node=livedebugging",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.514724272Z level=info msg=\"node exited without error\" node=labelstore",
        "Aug 07 17:15:40 rev1-haproxy systemd[1]: Stopping Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines...",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.520606392Z level=info msg=\"Stopping remote storage...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.520845086Z level=info msg=\"node exited without error\" node=ui",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521022116Z level=info msg=\"http server closed\" service=http addr=memory err=\"http: Server closed\"",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521205641Z level=info msg=\"node exited without error\" node=http",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521374434Z level=info msg=\"node exited without error\" node=cluster",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521613893Z level=info msg=\"WAL watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521786104Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521978388Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.522493625Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.522644903Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 07 17:15:40 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 07 17:15:40 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 17:15:40 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.973351064Z level=info \"boringcrypto enabled\"=false",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.951441113Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.974954876Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975124531Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975249136Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=tracing duration=15.905µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975360816Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=otel duration=1.047µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975487017Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=livedebugging duration=23.822µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.97559649Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=205.4µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975706114Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=loki.write.grafana_cloud_loki duration=899.583µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975813507Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.97591342Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=1",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976021776Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=1",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976170482Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976322018Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976428706Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=prometheus.remote_write.metrics_service duration=14.191313ms",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976552115Z level=info msg=\"running usage stats reporter\"",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976771608Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976886751Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=5.272845ms",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.977021679Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=logging duration=3.683543ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.010827187Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=remotecfg duration=33.68779ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011067917Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.01122324Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=http duration=184.124µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011412805Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=ui duration=41.472µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.01156297Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=cluster duration=5.544µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011723346Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=labelstore duration=12.749µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011858942Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d duration=59.399793ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.018637571Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.022113182Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024044618Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024529154Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.02847717Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.028512703Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0 duration=96.551µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.042645073Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 17:15:45,212 p=1266872 u=ubuntu n=ansible | TASK [Check the Grafana alloy configuration file] *********************************************************************************************************************
2025-08-07 17:15:47,218 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:15:47,223 p=1266872 u=ubuntu n=ansible | TASK [Display the Grafana alloy config] *******************************************************************************************************************************
2025-08-07 17:15:47,241 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_config_response.stdout_lines": [
        "remotecfg {",
        "  url            = \"https://fleet-management-prod-016.grafana.net\"",
        "  id             = \"rev1-haproxy\"",
        "  poll_frequency = \"60s\"",
        "",
        "  basic_auth {",
        "    username = \"1303247\"",
        "    password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "  }",
        "}",
        "",
        "prometheus.remote_write \"metrics_service\" {",
        "  endpoint {",
        "    url = \"https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push\"",
        "    basic_auth {",
        "      username = \"2530729\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "loki.write \"grafana_cloud_loki\" {",
        "  endpoint {",
        "    url = \"https://logs-prod-025.grafana.net/loki/api/v1/push\"",
        "    basic_auth {",
        "      username = \"1261041\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "discovery.relabel \"metrics_integrations_integrations_haproxy\" {",
        "  targets = [{",
        "    __address__ = \"127.0.0.1:80\",",
        "  }]",
        "",
        "  rule {",
        "    target_label = \"instance\"",
        "    replacement  = constants.hostname",
        "  }",
        "}",
        "",
        "prometheus.scrape \"metrics_integrations_integrations_haproxy\" {",
        "  targets    = discovery.relabel.metrics_integrations_integrations_haproxy.output",
        "  forward_to = [prometheus.remote_write.metrics_service.receiver]",
        "  job_name   = \"integrations/haproxy\"",
        "}"
    ]
}
2025-08-07 17:15:47,257 p=1266872 u=ubuntu n=ansible | [WARNING]: Found variable using reserved name: timeout

2025-08-07 17:15:47,258 p=1266872 u=ubuntu n=ansible | PLAY [Install snmp, snmpd, NGINX UDP load balancer config for SNMP] ***************************************************************************************************
2025-08-07 17:15:47,260 p=1266872 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 17:15:52,054 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 17:15:52,066 p=1266872 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-07 17:16:08,867 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=nginx)
2025-08-07 17:16:18,309 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmpd)
2025-08-07 17:16:24,915 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp)
2025-08-07 17:16:37,223 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp-mibs-downloader)
2025-08-07 17:16:37,230 p=1266872 u=ubuntu n=ansible | TASK [Deploy NGINX stream config for SNMP UDP load balancing] *********************************************************************************************************
2025-08-07 17:16:40,587 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-07 17:16:40,592 p=1266872 u=ubuntu n=ansible | TASK [Check nginx is running] *****************************************************************************************************************************************
2025-08-07 17:16:42,503 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 17:16:42,508 p=1266872 u=ubuntu n=ansible | TASK [display nginx status] *******************************************************************************************************************************************
2025-08-07 17:16:42,531 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX] => {
    "nginx_running_status": {
        "changed": false,
        "failed": false,
        "name": "nginx",
        "state": "started",
        "status": {
            "ActiveEnterTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "ActiveEnterTimestampMonotonic": "341679164",
            "ActiveExitTimestampMonotonic": "0",
            "ActiveState": "active",
            "After": "systemd-journald.socket network.target basic.target sysinit.target system.slice",
            "AllowIsolate": "no",
            "AllowedCPUs": "",
            "AllowedMemoryNodes": "",
            "AmbientCapabilities": "",
            "AssertResult": "yes",
            "AssertTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "AssertTimestampMonotonic": "341619751",
            "Before": "shutdown.target multi-user.target",
            "BlockIOAccounting": "no",
            "BlockIOWeight": "[not set]",
            "CPUAccounting": "no",
            "CPUAffinity": "",
            "CPUAffinityFromNUMA": "no",
            "CPUQuotaPerSecUSec": "infinity",
            "CPUQuotaPeriodUSec": "infinity",
            "CPUSchedulingPolicy": "0",
            "CPUSchedulingPriority": "0",
            "CPUSchedulingResetOnFork": "no",
            "CPUShares": "[not set]",
            "CPUUsageNSec": "[not set]",
            "CPUWeight": "[not set]",
            "CacheDirectoryMode": "0755",
            "CanIsolate": "no",
            "CanReload": "yes",
            "CanStart": "yes",
            "CanStop": "yes",
            "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read",
            "CleanResult": "success",
            "CollectMode": "inactive",
            "ConditionResult": "yes",
            "ConditionTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "ConditionTimestampMonotonic": "341619751",
            "ConfigurationDirectoryMode": "0755",
            "Conflicts": "shutdown.target",
            "ControlGroup": "/system.slice/nginx.service",
            "ControlPID": "0",
            "DefaultDependencies": "yes",
            "DefaultMemoryLow": "0",
            "DefaultMemoryMin": "0",
            "Delegate": "no",
            "Description": "A high performance web server and a reverse proxy server",
            "DevicePolicy": "auto",
            "Documentation": "man:nginx(8)",
            "DynamicUser": "no",
            "EffectiveCPUs": "",
            "EffectiveMemoryNodes": "",
            "ExecMainCode": "0",
            "ExecMainExitTimestampMonotonic": "0",
            "ExecMainPID": "3443",
            "ExecMainStartTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "ExecMainStartTimestampMonotonic": "341679146",
            "ExecMainStatus": "0",
            "ExecReload": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecReloadEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStart": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPre": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPreEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStop": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStopEx": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "FailureAction": "none",
            "FileDescriptorStoreMax": "0",
            "FinalKillSignal": "9",
            "FragmentPath": "/lib/systemd/system/nginx.service",
            "GID": "[not set]",
            "GuessMainPID": "yes",
            "IOAccounting": "no",
            "IOReadBytes": "18446744073709551615",
            "IOReadOperations": "18446744073709551615",
            "IOSchedulingClass": "0",
            "IOSchedulingPriority": "0",
            "IOWeight": "[not set]",
            "IOWriteBytes": "18446744073709551615",
            "IOWriteOperations": "18446744073709551615",
            "IPAccounting": "no",
            "IPEgressBytes": "[no data]",
            "IPEgressPackets": "[no data]",
            "IPIngressBytes": "[no data]",
            "IPIngressPackets": "[no data]",
            "Id": "nginx.service",
            "IgnoreOnIsolate": "no",
            "IgnoreSIGPIPE": "yes",
            "InactiveEnterTimestampMonotonic": "0",
            "InactiveExitTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "InactiveExitTimestampMonotonic": "341621095",
            "InvocationID": "13ef5a9202394522995778f33bf8b6bf",
            "JobRunningTimeoutUSec": "infinity",
            "JobTimeoutAction": "none",
            "JobTimeoutUSec": "infinity",
            "KeyringMode": "private",
            "KillMode": "mixed",
            "KillSignal": "15",
            "LimitAS": "infinity",
            "LimitASSoft": "infinity",
            "LimitCORE": "infinity",
            "LimitCORESoft": "0",
            "LimitCPU": "infinity",
            "LimitCPUSoft": "infinity",
            "LimitDATA": "infinity",
            "LimitDATASoft": "infinity",
            "LimitFSIZE": "infinity",
            "LimitFSIZESoft": "infinity",
            "LimitLOCKS": "infinity",
            "LimitLOCKSSoft": "infinity",
            "LimitMEMLOCK": "65536",
            "LimitMEMLOCKSoft": "65536",
            "LimitMSGQUEUE": "819200",
            "LimitMSGQUEUESoft": "819200",
            "LimitNICE": "0",
            "LimitNICESoft": "0",
            "LimitNOFILE": "524288",
            "LimitNOFILESoft": "1024",
            "LimitNPROC": "15295",
            "LimitNPROCSoft": "15295",
            "LimitRSS": "infinity",
            "LimitRSSSoft": "infinity",
            "LimitRTPRIO": "0",
            "LimitRTPRIOSoft": "0",
            "LimitRTTIME": "infinity",
            "LimitRTTIMESoft": "infinity",
            "LimitSIGPENDING": "15295",
            "LimitSIGPENDINGSoft": "15295",
            "LimitSTACK": "infinity",
            "LimitSTACKSoft": "8388608",
            "LoadState": "loaded",
            "LockPersonality": "no",
            "LogLevelMax": "-1",
            "LogRateLimitBurst": "0",
            "LogRateLimitIntervalUSec": "0",
            "LogsDirectoryMode": "0755",
            "MainPID": "3443",
            "MemoryAccounting": "yes",
            "MemoryCurrent": "5443584",
            "MemoryDenyWriteExecute": "no",
            "MemoryHigh": "infinity",
            "MemoryLimit": "infinity",
            "MemoryLow": "0",
            "MemoryMax": "infinity",
            "MemoryMin": "0",
            "MemorySwapMax": "infinity",
            "MountAPIVFS": "no",
            "MountFlags": "",
            "NFileDescriptorStore": "0",
            "NRestarts": "0",
            "NUMAMask": "",
            "NUMAPolicy": "n/a",
            "Names": "nginx.service",
            "NeedDaemonReload": "no",
            "Nice": "0",
            "NoNewPrivileges": "no",
            "NonBlocking": "no",
            "NotifyAccess": "none",
            "OOMPolicy": "stop",
            "OOMScoreAdjust": "0",
            "OnFailureJobMode": "replace",
            "PIDFile": "/run/nginx.pid",
            "Perpetual": "no",
            "PrivateDevices": "no",
            "PrivateMounts": "no",
            "PrivateNetwork": "no",
            "PrivateTmp": "no",
            "PrivateUsers": "no",
            "ProtectControlGroups": "no",
            "ProtectHome": "no",
            "ProtectHostname": "no",
            "ProtectKernelLogs": "no",
            "ProtectKernelModules": "no",
            "ProtectKernelTunables": "no",
            "ProtectSystem": "no",
            "RefuseManualStart": "no",
            "RefuseManualStop": "no",
            "ReloadResult": "success",
            "RemainAfterExit": "no",
            "RemoveIPC": "no",
            "Requires": "sysinit.target system.slice",
            "Restart": "no",
            "RestartKillSignal": "15",
            "RestartUSec": "100ms",
            "RestrictNamespaces": "no",
            "RestrictRealtime": "no",
            "RestrictSUIDSGID": "no",
            "Result": "success",
            "RootDirectoryStartOnly": "no",
            "RuntimeDirectoryMode": "0755",
            "RuntimeDirectoryPreserve": "no",
            "RuntimeMaxUSec": "infinity",
            "SameProcessGroup": "no",
            "SecureBits": "0",
            "SendSIGHUP": "no",
            "SendSIGKILL": "yes",
            "Slice": "system.slice",
            "StandardError": "inherit",
            "StandardInput": "null",
            "StandardInputData": "",
            "StandardOutput": "journal",
            "StartLimitAction": "none",
            "StartLimitBurst": "5",
            "StartLimitIntervalUSec": "10s",
            "StartupBlockIOWeight": "[not set]",
            "StartupCPUShares": "[not set]",
            "StartupCPUWeight": "[not set]",
            "StartupIOWeight": "[not set]",
            "StateChangeTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "StateChangeTimestampMonotonic": "341679164",
            "StateDirectoryMode": "0755",
            "StatusErrno": "0",
            "StopWhenUnneeded": "no",
            "SubState": "running",
            "SuccessAction": "none",
            "SyslogFacility": "3",
            "SyslogLevel": "6",
            "SyslogLevelPrefix": "yes",
            "SyslogPriority": "30",
            "SystemCallErrorNumber": "0",
            "TTYReset": "no",
            "TTYVHangup": "no",
            "TTYVTDisallocate": "no",
            "TasksAccounting": "yes",
            "TasksCurrent": "2",
            "TasksMax": "4588",
            "TimeoutAbortUSec": "5s",
            "TimeoutCleanUSec": "infinity",
            "TimeoutStartUSec": "1min 30s",
            "TimeoutStopUSec": "5s",
            "TimerSlackNSec": "50000",
            "Transient": "no",
            "Type": "forking",
            "UID": "[not set]",
            "UMask": "0022",
            "UnitFilePreset": "enabled",
            "UnitFileState": "enabled",
            "UtmpMode": "init",
            "WantedBy": "multi-user.target",
            "WatchdogSignal": "6",
            "WatchdogTimestampMonotonic": "0",
            "WatchdogUSec": "0"
        }
    }
}
2025-08-07 17:16:42,543 p=1266872 u=ubuntu n=ansible | RUNNING HANDLER [Reload NGINX] ****************************************************************************************************************************************
2025-08-07 17:16:44,454 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-07 17:16:44,464 p=1266872 u=ubuntu n=ansible | PLAY [Test HAProxy (http), HAProxy Web stats (STATS)+ Metrics (PROMEX) and HAProxy logs] ******************************************************************************
2025-08-07 17:16:44,471 p=1266872 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 17:16:47,572 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:16:47,583 p=1266872 u=ubuntu n=ansible | TASK [Gather HAProxy server public IP address] ************************************************************************************************************************
2025-08-07 17:16:49,889 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:16:49,896 p=1266872 u=ubuntu n=ansible | TASK [Send HTTP request to HAProxy and collect response] **************************************************************************************************************
2025-08-07 17:16:52,156 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0)
2025-08-07 17:16:58,299 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1)
2025-08-07 17:17:08,533 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2)
2025-08-07 17:17:08,539 p=1266872 u=ubuntu n=ansible | TASK [Display the HAProxy response] ***********************************************************************************************************************************
2025-08-07 17:17:08,557 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "17:16:51 10.1.1.21:58756 -- 10.1.1.12 (rev1-devb) 32\n",
    "item": 0
}
2025-08-07 17:17:08,560 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "17:16:57 10.1.1.21:53686 -- 10.1.1.42 (rev1-devc) 70\n",
    "item": 1
}
2025-08-07 17:17:08,564 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "17:17:08 10.1.1.21:35844 -- 10.1.1.36 (rev1-deva) 17\n",
    "item": 2
}
2025-08-07 17:17:08,569 p=1266872 u=ubuntu n=ansible | TASK [Send HTTP requests to HAProxy stats page and collect responses] *************************************************************************************************
2025-08-07 17:17:22,827 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 17:17:22,832 p=1266872 u=ubuntu n=ansible | TASK [Display the stats response content] *****************************************************************************************************************************
2025-08-07 17:17:22,847 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_stats_response.content": "# pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,agent_status,agent_code,agent_duration,check_desc,agent_desc,check_rise,check_fall,check_health,agent_rise,agent_fall,agent_health,addr,cookie,mode,algo,conn_rate,conn_rate_max,conn_tot,intercepted,dcon,dses,wrew,connect,reuse,cache_lookups,cache_hits,srv_icur,src_ilim,qtime_max,ctime_max,rtime_max,ttime_max,eint,idle_conn_cur,safe_conn_cur,used_conn_cur,need_conn_est,uweight,agg_server_status,agg_server_check_status,agg_check_status,srid,sess_other,h1sess,h2sess,h3sess,req_other,h1req,h2req,h3req,proto,-,ssl_sess,ssl_reused_sess,ssl_failed_handshake,quic_rxbuf_full,quic_dropped_pkt,quic_dropped_pkt_bufoverrun,quic_dropped_parsing_pkt,quic_socket_full,quic_sendto_err,quic_sendto_err_unknwn,quic_sent_pkt,quic_lost_pkt,quic_too_short_dgram,quic_retry_sent,quic_retry_validated,quic_retry_error,quic_half_open_conn,quic_hdshk_fail,quic_stless_rst_sent,quic_conn_migration_done,quic_transp_err_no_error,quic_transp_err_internal_error,quic_transp_err_connection_refused,quic_transp_err_flow_control_error,quic_transp_err_stream_limit_error,quic_transp_err_stream_state_error,quic_transp_err_final_size_error,quic_transp_err_frame_encoding_error,quic_transp_err_transport_parameter_error,quic_transp_err_connection_id_limit,quic_transp_err_protocol_violation_error,quic_transp_err_invalid_token,quic_transp_err_application_error,quic_transp_err_crypto_buffer_exceeded,quic_transp_err_key_update_error,quic_transp_err_aead_limit_reached,quic_transp_err_no_viable_path,quic_transp_err_crypto_error,quic_transp_err_unknown_error,quic_data_blocked,quic_stream_data_blocked,quic_streams_blocked_bidi,quic_streams_blocked_uni,h3_data,h3_headers,h3_cancel_push,h3_push_promise,h3_max_push_id,h3_goaway,h3_settings,h3_no_error,h3_general_protocol_error,h3_internal_error,h3_stream_creation_error,h3_closed_critical_stream,h3_frame_unexpected,h3_frame_error,h3_excessive_load,h3_id_error,h3_settings_error,h3_missing_settings,h3_request_rejected,h3_request_cancelled,h3_request_incomplete,h3_message_error,h3_connect_error,h3_version_fallback,pack_decompression_failed,qpack_encoder_stream_error,qpack_decoder_stream_error,h2_headers_rcvd,h2_data_rcvd,h2_settings_rcvd,h2_rst_stream_rcvd,h2_goaway_rcvd,h2_detected_conn_protocol_errors,h2_detected_strm_protocol_errors,h2_rst_stream_resp,h2_goaway_resp,h2_open_connections,h2_backend_open_streams,h2_total_connections,h2_backend_total_streams,h1_open_connections,h1_open_streams,h1_total_connections,h1_total_streams,h1_bytes_in,h1_bytes_out,h1_spliced_bytes_in,h1_spliced_bytes_out,\nweb_stats,FRONTEND,,,1,1,262113,5,586,62710,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,1,0,1,,,,0,1,0,0,3,0,,1,1,5,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,1,1,5,2,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,5,0,0,0,5,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,5,5,809,62678,0,0,\nhaproxy_frontend,FRONTEND,,,0,1,262113,7,1036,2090,0,0,0,,,,,OPEN,,,,,,,,,1,3,0,,,,0,0,0,1,,,,0,3,0,4,0,0,,0,1,7,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,7,0,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,7,0,0,0,7,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,7,1149,3198,0,0,\nhaproxy_backend,rev1_devA,0,0,0,1,,3,540,942,,0,,0,0,0,0,UP,1,1,0,0,0,126,0,,1,4,1,,3,,2,0,,1,L4OK,,0,0,1,0,2,0,0,,,,3,0,0,,,,,4,,,0,1,3,3,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,3,0,,,0,,0,1,3,3,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devB,0,0,0,1,,2,248,574,,0,,0,0,0,0,UP,1,1,0,0,0,126,0,,1,4,2,,2,,2,0,,1,L4OK,,0,0,1,0,1,0,0,,,,2,0,0,,,,,12,,,0,0,2,2,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,2,0,,,0,,0,0,2,2,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devC,0,0,0,1,,2,248,574,,0,,0,0,0,0,UP,1,1,0,0,0,126,0,,1,4,3,,2,,2,0,,1,L4OK,,0,0,1,0,1,0,0,,,,2,0,0,,,,,8,,,0,0,2,2,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,2,0,,,0,,0,0,2,2,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,BACKEND,0,0,0,1,26212,7,1036,2090,0,0,,0,0,0,0,UP,3,3,0,,0,126,0,,1,4,0,,7,,1,0,,1,,,,0,3,0,4,0,0,,,,7,0,0,0,0,0,0,4,,,0,1,2,3,,,,,,,,,,,,,,http,,,,,,,,0,7,0,0,0,,,0,1,3,3,0,,,,,3,0,0,0,,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,7,3217,1035,0,0,\n"
}
2025-08-07 17:17:22,852 p=1266872 u=ubuntu n=ansible | TASK [Test the HAProxy metrics (promex) path] *************************************************************************************************************************
2025-08-07 17:17:24,915 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:17:24,920 p=1266872 u=ubuntu n=ansible | TASK [Display the HAProxy metrics (promex) response content] **********************************************************************************************************
2025-08-07 17:17:24,937 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_metrics_path_result.stdout_lines": [
        "<!doctype html>",
        "<html lang=en>",
        "<title>404 Not Found</title>",
        "<h1>Not Found</h1>",
        "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>"
    ]
}
2025-08-07 17:17:24,942 p=1266872 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-07 17:17:27,005 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:17:27,012 p=1266872 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-07 17:17:27,028 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : path to executable is /usr/sbin/haproxy",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [WARNING]  (4487) : Exiting Master process...",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [ALERT]    (4487) : Current worker (4507) exited with code 143 (Terminated)",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [WARNING]  (4487) : All workers exited. Exiting... (0)",
        "Aug  7 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : New worker (10251) forked",
        "Aug  7 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : Loading success.",
        "Aug  7 17:16:11 rev1-haproxy haproxy[10251]: 127.0.0.1:50254 [07/Aug/2025:17:16:11.737] haproxy_frontend haproxy_backend/rev1_devA 0/0/1/2/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:18459 cpu_ns_avg:4614 lat_ns_tot:5929 lat_ns_avg:1482",
        "Aug  7 17:16:51 rev1-haproxy haproxy[10251]: 185.62.207.61:13292 [07/Aug/2025:17:16:51.812] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/1/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15882 cpu_ns_avg:3970 lat_ns_tot:5464 lat_ns_avg:1366",
        "Aug  7 17:16:57 rev1-haproxy haproxy[10251]: 185.62.207.61:53393 [07/Aug/2025:17:16:57.975] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:26909 cpu_ns_avg:6727 lat_ns_tot:4054 lat_ns_avg:1013",
        "Aug  7 17:17:08 rev1-haproxy haproxy[10251]: 185.62.207.61:49162 [07/Aug/2025:17:17:08.213] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12175 cpu_ns_avg:3043 lat_ns_tot:5075 lat_ns_avg:1268",
        "Aug  7 17:17:10 rev1-haproxy haproxy[10251]: 185.62.207.61:22398 [07/Aug/2025:17:17:10.310] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15196 cpu_ns_avg:3799 lat_ns_tot:5361 lat_ns_avg:1340",
        "Aug  7 17:17:14 rev1-haproxy haproxy[10251]: 185.62.207.61:47110 [07/Aug/2025:17:17:14.398] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 2/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12097 cpu_ns_avg:3024 lat_ns_tot:5680 lat_ns_avg:1420",
        "Aug  7 17:17:18 rev1-haproxy haproxy[10251]: 185.62.207.61:8563 [07/Aug/2025:17:17:18.443] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/3/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10375 cpu_ns_avg:2593 lat_ns_tot:3305 lat_ns_avg:826",
        "Aug  7 17:17:24 rev1-haproxy haproxy[10251]: 185.62.207.61:27817 [07/Aug/2025:17:17:24.591] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:9195 cpu_ns_avg:2298 lat_ns_tot:5384 lat_ns_avg:1346"
    ]
}
2025-08-07 17:17:27,033 p=1266872 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-07 17:17:29,010 p=1266872 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 17:17:29,015 p=1266872 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-07 17:17:29,031 p=1266872 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : path to executable is /usr/sbin/haproxy",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [WARNING]  (4487) : Exiting Master process...",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [ALERT]    (4487) : Current worker (4507) exited with code 143 (Terminated)",
        "Aug  7 17:15:16 rev1-haproxy haproxy[4487]: [WARNING]  (4487) : All workers exited. Exiting... (0)",
        "Aug  7 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : New worker (10251) forked",
        "Aug  7 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : Loading success.",
        "Aug  7 17:16:11 rev1-haproxy haproxy[10251]: 127.0.0.1:50254 [07/Aug/2025:17:16:11.737] haproxy_frontend haproxy_backend/rev1_devA 0/0/1/2/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:18459 cpu_ns_avg:4614 lat_ns_tot:5929 lat_ns_avg:1482",
        "Aug  7 17:16:51 rev1-haproxy haproxy[10251]: 185.62.207.61:13292 [07/Aug/2025:17:16:51.812] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/1/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15882 cpu_ns_avg:3970 lat_ns_tot:5464 lat_ns_avg:1366",
        "Aug  7 17:16:57 rev1-haproxy haproxy[10251]: 185.62.207.61:53393 [07/Aug/2025:17:16:57.975] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:26909 cpu_ns_avg:6727 lat_ns_tot:4054 lat_ns_avg:1013",
        "Aug  7 17:17:08 rev1-haproxy haproxy[10251]: 185.62.207.61:49162 [07/Aug/2025:17:17:08.213] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12175 cpu_ns_avg:3043 lat_ns_tot:5075 lat_ns_avg:1268",
        "Aug  7 17:17:10 rev1-haproxy haproxy[10251]: 185.62.207.61:22398 [07/Aug/2025:17:17:10.310] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:15196 cpu_ns_avg:3799 lat_ns_tot:5361 lat_ns_avg:1340",
        "Aug  7 17:17:14 rev1-haproxy haproxy[10251]: 185.62.207.61:47110 [07/Aug/2025:17:17:14.398] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 2/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12097 cpu_ns_avg:3024 lat_ns_tot:5680 lat_ns_avg:1420",
        "Aug  7 17:17:18 rev1-haproxy haproxy[10251]: 185.62.207.61:8563 [07/Aug/2025:17:17:18.443] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/3/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10375 cpu_ns_avg:2593 lat_ns_tot:3305 lat_ns_avg:826",
        "Aug  7 17:17:24 rev1-haproxy haproxy[10251]: 185.62.207.61:27817 [07/Aug/2025:17:17:24.591] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:9195 cpu_ns_avg:2298 lat_ns_tot:5384 lat_ns_avg:1346"
    ]
}
2025-08-07 17:17:29,047 p=1266872 u=ubuntu n=ansible | PLAY [Test NGINX (snmp) proxy] ****************************************************************************************************************************************
2025-08-07 17:17:29,052 p=1266872 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 17:17:31,624 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 17:17:31,636 p=1266872 u=ubuntu n=ansible | TASK [Gather NGINX public IP address] *********************************************************************************************************************************
2025-08-07 17:17:33,638 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 17:17:33,645 p=1266872 u=ubuntu n=ansible | TASK [Send SNMP request to NGINX server and collect responses] ********************************************************************************************************
2025-08-07 17:17:35,428 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=0)
2025-08-07 17:17:37,174 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=1)
2025-08-07 17:17:38,937 p=1266872 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=2)
2025-08-07 17:17:38,944 p=1266872 u=ubuntu n=ansible | TASK [Display the NGINX response content] *****************************************************************************************************************************
2025-08-07 17:17:38,962 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=0) => {
    "ansible_loop_var": "item",
    "item": 0,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-07 17:17:38,966 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=1) => {
    "ansible_loop_var": "item",
    "item": 1,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-07 17:17:38,970 p=1266872 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=2) => {
    "ansible_loop_var": "item",
    "item": 2,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-07 17:17:38,986 p=1266872 u=ubuntu n=ansible | PLAY RECAP ************************************************************************************************************************************************************
2025-08-07 17:17:38,986 p=1266872 u=ubuntu n=ansible | rev1_HAproxy               : ok=46   changed=22   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 17:17:38,987 p=1266872 u=ubuntu n=ansible | rev1_NGINX                 : ok=10   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 17:17:38,987 p=1266872 u=ubuntu n=ansible | rev1_devA                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 17:17:38,987 p=1266872 u=ubuntu n=ansible | rev1_devB                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 17:17:38,987 p=1266872 u=ubuntu n=ansible | rev1_devC                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 18:18:41,598 p=1274933 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] **************************************************************************************************************
2025-08-07 18:18:41,615 p=1274933 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 18:18:51,793 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-07 18:18:51,795 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-07 18:18:52,607 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev69]
2025-08-07 18:18:53,089 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev85]
2025-08-07 18:18:53,793 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev48]
2025-08-07 18:18:57,375 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-07 18:18:57,420 p=1274933 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-07 18:19:04,571 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3)
2025-08-07 18:19:06,079 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3)
2025-08-07 18:19:10,142 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3-pip)
2025-08-07 18:19:11,808 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3-pip)
2025-08-07 18:19:12,530 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev48] => (item=python3)
2025-08-07 18:19:12,580 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev85] => (item=python3)
2025-08-07 18:19:12,611 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev69] => (item=python3)
2025-08-07 18:19:16,760 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => (item=snmpd)
2025-08-07 18:19:18,784 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => (item=snmpd)
2025-08-07 18:19:22,845 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => (item=snmp)
2025-08-07 18:19:24,649 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48] => (item=python3-pip)
2025-08-07 18:19:24,804 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => (item=snmp)
2025-08-07 18:19:25,081 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69] => (item=python3-pip)
2025-08-07 18:19:26,199 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85] => (item=python3-pip)
2025-08-07 18:19:29,302 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => (item=snmp-mibs-downloader)
2025-08-07 18:19:31,624 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => (item=snmp-mibs-downloader)
2025-08-07 18:19:36,399 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3)
2025-08-07 18:19:38,712 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69] => (item=snmpd)
2025-08-07 18:19:39,125 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48] => (item=snmpd)
2025-08-07 18:19:41,335 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85] => (item=snmpd)
2025-08-07 18:19:42,361 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3-pip)
2025-08-07 18:19:48,220 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => (item=snmpd)
2025-08-07 18:19:48,570 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69] => (item=snmp)
2025-08-07 18:19:49,096 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48] => (item=snmp)
2025-08-07 18:19:52,448 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85] => (item=snmp)
2025-08-07 18:19:54,279 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => (item=snmp)
2025-08-07 18:19:59,927 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => (item=snmp-mibs-downloader)
2025-08-07 18:20:04,120 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69] => (item=snmp-mibs-downloader)
2025-08-07 18:20:04,997 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48] => (item=snmp-mibs-downloader)
2025-08-07 18:20:08,813 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85] => (item=snmp-mibs-downloader)
2025-08-07 18:20:08,820 p=1274933 u=ubuntu n=ansible | TASK [Install Flask] **************************************************************************************************************************************************
2025-08-07 18:20:15,720 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-07 18:20:15,774 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-07 18:20:17,199 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:20:17,477 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:20:18,142 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:20:20,699 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-07 18:20:20,705 p=1274933 u=ubuntu n=ansible | TASK [Deploy the Flask application config for TCP Load Balancing] *****************************************************************************************************
2025-08-07 18:20:29,068 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-07 18:20:29,157 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-07 18:20:30,922 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:20:31,127 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:20:31,629 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:20:35,359 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-07 18:20:35,365 p=1274933 u=ubuntu n=ansible | TASK [Start Flask app in background on port 5000] *********************************************************************************************************************
2025-08-07 18:20:40,752 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:20:41,272 p=1274933 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 18:20:41,344 p=1274933 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 18:20:41,406 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:20:42,079 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:20:44,392 p=1274933 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 18:20:44,397 p=1274933 u=ubuntu n=ansible | TASK [Check Flask app HTTP response on private IP] ********************************************************************************************************************
2025-08-07 18:20:50,082 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:20:50,296 p=1274933 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 18:20:50,331 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:20:50,359 p=1274933 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 18:20:51,054 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:20:53,567 p=1274933 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 18:20:53,573 p=1274933 u=ubuntu n=ansible | TASK [Display Flask app HTTP response on private IP] ******************************************************************************************************************
2025-08-07 18:20:53,601 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev48] => {
    "msg": "18:20:49 10.1.1.28:34248 -- 10.1.1.28 (rev1-dev48) 13"
}
2025-08-07 18:20:53,646 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev85] => {
    "msg": "18:20:49 10.1.1.20:46874 -- 10.1.1.20 (rev1-dev85) 40"
}
2025-08-07 18:20:53,659 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev69] => {
    "msg": "18:20:50 10.1.1.40:50338 -- 10.1.1.40 (rev1-dev69) 23"
}
2025-08-07 18:20:53,671 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "18:20:49 10.1.1.42:52012 -- 10.1.1.42 (rev1-devc) 82"
}
2025-08-07 18:20:53,672 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "18:20:49 10.1.1.12:44890 -- 10.1.1.12 (rev1-devb) 80"
}
2025-08-07 18:20:53,684 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "18:20:52 10.1.1.36:43452 -- 10.1.1.36 (rev1-deva) 87"
}
2025-08-07 18:20:53,689 p=1274933 u=ubuntu n=ansible | TASK [Remove the existing agent address lines] ************************************************************************************************************************
2025-08-07 18:20:59,336 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:20:59,949 p=1274933 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 18:21:00,228 p=1274933 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 18:21:00,322 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:21:01,077 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:21:03,537 p=1274933 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 18:21:03,542 p=1274933 u=ubuntu n=ansible | TASK [Configure agent address (0.0.0.0) for SNMPd to listen on all UDP interfaces] ************************************************************************************
2025-08-07 18:21:09,274 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:21:09,326 p=1274933 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 18:21:09,348 p=1274933 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 18:21:09,379 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:21:09,922 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:21:12,878 p=1274933 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 18:21:12,885 p=1274933 u=ubuntu n=ansible | TASK [Check snmpd config File] ****************************************************************************************************************************************
2025-08-07 18:21:17,953 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:21:18,050 p=1274933 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 18:21:18,349 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:21:18,588 p=1274933 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 18:21:18,808 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:21:21,767 p=1274933 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 18:21:21,772 p=1274933 u=ubuntu n=ansible | TASK [Display snmpd configuration file] *******************************************************************************************************************************
2025-08-07 18:21:21,805 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev48] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003858",
        "end": "2025-08-07 18:21:17.260681",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 18:21:17.256823",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 18:21:21,819 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev85] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.004424",
        "end": "2025-08-07 18:21:17.902938",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 18:21:17.898514",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 18:21:21,836 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev69] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003801",
        "end": "2025-08-07 18:21:17.654263",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 18:21:17.650462",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 18:21:21,850 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003208",
        "end": "2025-08-07 18:21:17.825003",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 18:21:17.821795",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 18:21:21,852 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003074",
        "end": "2025-08-07 18:21:17.464378",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 18:21:17.461304",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 18:21:21,860 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003252",
        "end": "2025-08-07 18:21:21.208798",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 18:21:21.205546",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 18:21:21,866 p=1274933 u=ubuntu n=ansible | TASK [Restart SNMPD if agent address configuration is changed] ********************************************************************************************************
2025-08-07 18:21:27,436 p=1274933 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 18:21:27,650 p=1274933 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 18:21:27,713 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:21:27,865 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:21:28,300 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:21:31,338 p=1274933 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 18:21:31,345 p=1274933 u=ubuntu n=ansible | TASK [Test SNMPd with snmpget on 10.1.1.28] ***************************************************************************************************************************
2025-08-07 18:21:36,984 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev48]
2025-08-07 18:21:37,108 p=1274933 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 18:21:37,204 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev85]
2025-08-07 18:21:37,232 p=1274933 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 18:21:37,469 p=1274933 u=ubuntu n=ansible | changed: [rev1_dev69]
2025-08-07 18:21:40,524 p=1274933 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 18:21:40,529 p=1274933 u=ubuntu n=ansible | TASK [Print SNMPd snmpget result] *************************************************************************************************************************************
2025-08-07 18:21:40,555 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev48] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-dev48"
}
2025-08-07 18:21:40,567 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev85] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-dev85"
}
2025-08-07 18:21:40,580 p=1274933 u=ubuntu n=ansible | ok: [rev1_dev69] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-dev69"
}
2025-08-07 18:21:40,592 p=1274933 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-07 18:21:40,593 p=1274933 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-07 18:21:40,602 p=1274933 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-07 18:21:40,685 p=1274933 u=ubuntu n=ansible | PLAY [Set up HAProxy] *************************************************************************************************************************************************
2025-08-07 18:21:40,688 p=1274933 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 18:21:44,492 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:21:44,503 p=1274933 u=ubuntu n=ansible | TASK [Add HAProxy 2.9 PPA] ********************************************************************************************************************************************
2025-08-07 18:21:46,770 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:21:46,776 p=1274933 u=ubuntu n=ansible | TASK [Install HAProxy] ************************************************************************************************************************************************
2025-08-07 18:21:52,709 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:21:52,715 p=1274933 u=ubuntu n=ansible | TASK [Deploy stats web page password file] ****************************************************************************************************************************
2025-08-07 18:21:55,808 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:21:55,813 p=1274933 u=ubuntu n=ansible | TASK [Read stats page password from file] *****************************************************************************************************************************
2025-08-07 18:21:57,889 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:21:57,896 p=1274933 u=ubuntu n=ansible | TASK [Set up HAProxy stats secret variable] ***************************************************************************************************************************
2025-08-07 18:21:57,955 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:21:57,964 p=1274933 u=ubuntu n=ansible | TASK [Configure HAProxy] **********************************************************************************************************************************************
2025-08-07 18:22:01,827 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:01,833 p=1274933 u=ubuntu n=ansible | TASK [Deploy rsyslog 49-haproxy config file] **************************************************************************************************************************
2025-08-07 18:22:04,971 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:22:04,977 p=1274933 u=ubuntu n=ansible | TASK [Return 49_haproxy_conf to registered rsyslog_49_haproxy_conf] ***************************************************************************************************
2025-08-07 18:22:06,952 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:06,957 p=1274933 u=ubuntu n=ansible | TASK [debug] **********************************************************************************************************************************************************
2025-08-07 18:22:06,973 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "rsyslog_49_haproxy_conf.stdout_lines": [
        "# Create an additional socket in haproxy's chroot in order to allow logging via",
        "# /dev/log to chroot'ed HAProxy processes",
        "$AddUnixListenSocket /var/lib/haproxy/dev/log",
        "",
        "# Send HAProxy messages to a dedicated logfile",
        ":programname, startswith, \"haproxy\" {",
        "  /var/log/haproxy.log",
        "stop",
        "}"
    ]
}
2025-08-07 18:22:06,978 p=1274933 u=ubuntu n=ansible | TASK [Test HAProxy Configurations] ************************************************************************************************************************************
2025-08-07 18:22:08,948 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:08,953 p=1274933 u=ubuntu n=ansible | TASK [Display HAProxy config test result] *****************************************************************************************************************************
2025-08-07 18:22:08,969 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_configs_test_result": {
        "changed": true,
        "cmd": [
            "haproxy",
            "-f",
            "/etc/haproxy/haproxy.cfg",
            "-c"
        ],
        "delta": "0:00:00.024088",
        "end": "2025-08-07 18:22:08.619725",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 18:22:08.595637",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "",
        "stdout_lines": []
    }
}
2025-08-07 18:22:08,974 p=1274933 u=ubuntu n=ansible | TASK [Test HAProxy is running] ****************************************************************************************************************************************
2025-08-07 18:22:11,111 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:22:11,117 p=1274933 u=ubuntu n=ansible | TASK [Display the HAProxy service status] *****************************************************************************************************************************
2025-08-07 18:22:11,133 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_service_status.state": "started"
}
2025-08-07 18:22:11,138 p=1274933 u=ubuntu n=ansible | TASK [Check HAProxy server status] ************************************************************************************************************************************
2025-08-07 18:22:13,123 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:13,128 p=1274933 u=ubuntu n=ansible | TASK [Display HAProxy server status] **********************************************************************************************************************************
2025-08-07 18:22:13,145 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "systemctl_haproxy_service_status.stdout_lines": [
        "● haproxy.service - HAProxy Load Balancer",
        "     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Thu 2025-08-07 17:15:16 UTC; 1h 6min ago",
        "       Docs: man:haproxy(1)",
        "             file:/usr/share/doc/haproxy/configuration.txt.gz",
        "   Main PID: 10249 (haproxy)",
        "     Status: \"Ready.\"",
        "      Tasks: 2 (limit: 4588)",
        "     Memory: 40.3M",
        "     CGroup: /system.slice/haproxy.service",
        "             ├─10249 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "             └─10251 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: haproxy.service: Succeeded.",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: Stopped HAProxy Load Balancer.",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 07 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : New worker (10251) forked",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 07 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : Loading success."
    ]
}
2025-08-07 18:22:13,157 p=1274933 u=ubuntu n=ansible | TASK [Check HAProxy config errors via journalctl] *********************************************************************************************************************
2025-08-07 18:22:15,149 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:15,154 p=1274933 u=ubuntu n=ansible | TASK [Display HAProxy config errors] **********************************************************************************************************************************
2025-08-07 18:22:15,170 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_journalctl_logs.stdout_lines": [
        "-- Logs begin at Thu 2025-08-07 17:10:23 UTC, end at Thu 2025-08-07 18:22:14 UTC. --",
        "Aug 07 17:14:44 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 07 17:14:44 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : New worker (4507) forked",
        "Aug 07 17:14:44 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 07 17:14:44 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : Loading success.",
        "Aug 07 17:15:16 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug 07 17:15:16 rev1-haproxy haproxy[4487]: [NOTICE]   (4487) : path to executable is /usr/sbin/haproxy",
        "Aug 07 17:15:16 rev1-haproxy haproxy[4487]: [WARNING]  (4487) : Exiting Master process...",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: Stopping HAProxy Load Balancer...",
        "Aug 07 17:15:16 rev1-haproxy haproxy[4487]: [ALERT]    (4487) : Current worker (4507) exited with code 143 (Terminated)",
        "Aug 07 17:15:16 rev1-haproxy haproxy[4487]: [WARNING]  (4487) : All workers exited. Exiting... (0)",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: haproxy.service: Succeeded.",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: Stopped HAProxy Load Balancer.",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 07 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : New worker (10251) forked",
        "Aug 07 17:15:16 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 07 17:15:16 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : Loading success."
    ]
}
2025-08-07 18:22:15,178 p=1274933 u=ubuntu n=ansible | TASK [Check the HAProxy configuration file] ***************************************************************************************************************************
2025-08-07 18:22:17,195 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:17,201 p=1274933 u=ubuntu n=ansible | TASK [Display HAProxy configuration file] *****************************************************************************************************************************
2025-08-07 18:22:17,219 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_config_file.stdout_lines": [
        "global",
        "    profiling.tasks on #Enable HAProxy profiling (CPU time spent on processing a http request inside HAProxy)",
        "    nbthread 1 # 1 thread, IDs from 1 to 2, nbthread <number of CPU cores>",
        "    thread-groups 1",
        "    # declare threads",
        "    thread-group 1 1-1",
        "    # bind threads to cpu cores",
        "    cpu-map 1/all 0-0 # bind all threads to CPU 0 #syntax:cpu-map 1/1-<Number Of CPU Cores> 0-<Number of CPU Cores - 1>",
        "    # define logging",
        "    log /dev/log local0 info",
        "    #log /dev/log local0 emerg",
        "    #log /dev/log local1 alert",
        "    #log /dev/log local2 crit",
        "    #log /dev/log local3 err",
        "    #log /dev/log local4 warning",
        "    #log /dev/log local5 notice",
        "    #log /dev/log local6 info",
        "    #log /dev/log local7 debug",
        "    #Security Considerations",
        "    chroot /var/lib/haproxy #chroot statement pointing to a /var/lib/haproxy location",
        "    user haproxy # uid/user statement",
        "    group haproxy # gid/group statement",
        "    stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin",
        "    stats maxconn 20",
        "    stats timeout 30000",
        "    daemon",
        "    #maxconn 512",
        "        ",
        "defaults",
        "    mode http",
        "    timeout connect 5000ms",
        "    timeout client 5000ms",
        "    timeout server 5000ms",
        "    errorfile 400 /etc/haproxy/errors/400.http",
        "    errorfile 403 /etc/haproxy/errors/403.http",
        "    errorfile 408 /etc/haproxy/errors/408.http",
        "    errorfile 500 /etc/haproxy/errors/500.http",
        "    errorfile 502 /etc/haproxy/errors/502.http",
        "    errorfile 503 /etc/haproxy/errors/503.http",
        "    errorfile 504 /etc/haproxy/errors/504.http",
        "",
        "frontend web_stats",
        "    mode http",
        "    bind *:80 ",
        "    http-request use-service prometheus-exporter if { path /metrics }",
        "    stats enable # enable stats page",
        "    stats uri /stats # stats uri",
        "    stats hide-version",
        "    stats refresh 1s",
        "    stats auth admin:uipassword",
        "",
        "frontend haproxy_frontend",
        "    log global",
        "    bind *:80  thread 1/all shards by-thread  #bind this proxy to threads 1 to 1 or all",
        "    mode http",
        "    option httplog",
        "    #option dontlog-normal",
        "    #option logasap",
        "    #define custom log-format",
        "    log-format \"%ci:%cp [%tr] %ft %b/%s %TR/%Tw/%Tc/%Tr/%Ta %ST %B %CC %CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r %[http_first_req] cpu_calls:%[cpu_calls] cpu_ns_tot:%[cpu_ns_tot] cpu_ns_avg:%[cpu_ns_avg] lat_ns_tot:%[lat_ns_tot] lat_ns_avg:%[lat_ns_avg]\"",
        "    default_backend haproxy_backend",
        "    ",
        "backend haproxy_backend",
        "    retry-on all-retryable-errors # This works when conn-failure, empty-response, junk-response, response-timeout, rtt-rejected, 500, 502, 503, and 504",
        "    retries 3",
        "             server rev1_dev48 10.1.1.28:5000 check #maxconn 64",
        "             server rev1_dev85 10.1.1.20:5000 check #maxconn 64",
        "             server rev1_dev69 10.1.1.40:5000 check #maxconn 64",
        "             server rev1_devC 10.1.1.42:5000 check #maxconn 64",
        "             server rev1_devB 10.1.1.12:5000 check #maxconn 64",
        "             server rev1_devA 10.1.1.36:5000 check #maxconn 64",
        "    "
    ]
}
2025-08-07 18:22:17,231 p=1274933 u=ubuntu n=ansible | RUNNING HANDLER [Restart HAProxy service] *****************************************************************************************************************************
2025-08-07 18:22:19,523 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:19,533 p=1274933 u=ubuntu n=ansible | PLAY [Install the Grafana Alloy Agent on HAproxy] *********************************************************************************************************************
2025-08-07 18:22:19,538 p=1274933 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 18:22:22,243 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:22:22,255 p=1274933 u=ubuntu n=ansible | TASK [Install the Grafana Alloy Agent] ********************************************************************************************************************************
2025-08-07 18:22:33,016 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:33,024 p=1274933 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-07 18:22:34,989 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:34,994 p=1274933 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-07 18:22:35,013 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Thu 2025-08-07 17:15:40 UTC; 1h 6min ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 13537 (alloy)",
        "      Tasks: 7 (limit: 4588)",
        "     Memory: 41.1M",
        "     CGroup: /system.slice/alloy.service",
        "             └─13537 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011723346Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=labelstore duration=12.749µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011858942Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d duration=59.399793ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.018637571Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.022113182Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024044618Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024529154Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.02847717Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.028512703Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0 duration=96.551µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.042645073Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:16:11 rev1-haproxy alloy[13537]: ts=2025-08-07T17:16:11.742555349Z level=info msg=\"Done replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push duration=30.772817901s"
    ]
}
2025-08-07 18:22:35,019 p=1274933 u=ubuntu n=ansible | TASK [DeployAlloy config file] ****************************************************************************************************************************************
2025-08-07 18:22:38,756 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:38,762 p=1274933 u=ubuntu n=ansible | TASK [Restart the Grafana alloy service] ******************************************************************************************************************************
2025-08-07 18:22:40,984 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:40,989 p=1274933 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-07 18:22:43,002 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:43,009 p=1274933 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-07 18:22:43,026 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Thu 2025-08-07 18:22:40 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 31158 (alloy)",
        "      Tasks: 7 (limit: 4588)",
        "     Memory: 40.1M",
        "     CGroup: /system.slice/alloy.service",
        "             └─31158 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024993247Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=logging duration=6.623792ms",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.025354351Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=otel duration=339.031µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.02538184Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 duration=106.781447ms",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.026922547Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.027229231Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.029535369Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.03048755Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.036922136Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=59c2f3535b432fe6775dd5f062e9445a",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.036943965Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=59c2f3535b432fe6775dd5f062e9445a duration=95.003µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.050696418Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 18:22:43,031 p=1274933 u=ubuntu n=ansible | TASK [Check the Grafana alloy logs] ***********************************************************************************************************************************
2025-08-07 18:22:45,032 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:45,037 p=1274933 u=ubuntu n=ansible | TASK [Display the Grafana alloy logs] *********************************************************************************************************************************
2025-08-07 18:22:45,055 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_logs_response.stdout_lines": [
        "-- Logs begin at Thu 2025-08-07 17:10:23 UTC, end at Thu 2025-08-07 18:22:44 UTC. --",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521786104Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.521978388Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.522493625Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[11788]: ts=2025-08-07T17:15:40.522644903Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 07 17:15:40 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 07 17:15:40 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 17:15:40 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.973351064Z level=info \"boringcrypto enabled\"=false",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.951441113Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.974954876Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975124531Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975249136Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=tracing duration=15.905µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975360816Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=otel duration=1.047µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975487017Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=livedebugging duration=23.822µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.97559649Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=205.4µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975706114Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=loki.write.grafana_cloud_loki duration=899.583µs",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.975813507Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.97591342Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=1",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976021776Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=1",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976170482Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976322018Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976428706Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=prometheus.remote_write.metrics_service duration=14.191313ms",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976552115Z level=info msg=\"running usage stats reporter\"",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976771608Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.976886751Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=5.272845ms",
        "Aug 07 17:15:40 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:40.977021679Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=logging duration=3.683543ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.010827187Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=remotecfg duration=33.68779ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011067917Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.01122324Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=http duration=184.124µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011412805Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=ui duration=41.472µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.01156297Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=cluster duration=5.544µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011723346Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d node_id=labelstore duration=12.749µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.011858942Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=950546f195303e767ab95649b6ebeb3d duration=59.399793ms",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.018637571Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.022113182Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024044618Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.024529154Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.02847717Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.028512703Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=81ffd47d46ab36118a406f65319797f0 duration=96.551µs",
        "Aug 07 17:15:41 rev1-haproxy alloy[13537]: ts=2025-08-07T17:15:41.042645073Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 17:16:11 rev1-haproxy alloy[13537]: ts=2025-08-07T17:16:11.742555349Z level=info msg=\"Done replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push duration=30.772817901s",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: interrupt received",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.430594644Z level=error msg=\"failed to start reporter\" err=\"context canceled\"",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.431042222Z level=info msg=\"http server closed\" service=http addr=127.0.0.1:12345 err=\"http: Server closed\"",
        "Aug 07 18:22:40 rev1-haproxy systemd[1]: Stopping Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines...",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.433449126Z level=info msg=\"node exited without error\" node=discovery.relabel.metrics_integrations_integrations_haproxy",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.433521139Z level=info msg=\"node exited without error\" node=otel",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.433811455Z level=info msg=\"node exited without error\" node=livedebugging",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.433872091Z level=info msg=\"node exited without error\" node=ui",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.433911796Z level=info msg=\"node exited without error\" node=labelstore",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.437343469Z level=info msg=\"node exited without error\" node=loki.write.grafana_cloud_loki",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.43977688Z level=info msg=\"node exited without error\" node=remotecfg",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.439983514Z level=info msg=\"http server closed\" service=http addr=memory err=\"http: Server closed\"",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.440169538Z level=info msg=\"node exited without error\" node=http",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.44037998Z level=info msg=\"node exited without error\" node=cluster",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.441616684Z level=info msg=\"node exited without error\" node=prometheus.scrape.metrics_integrations_integrations_haproxy",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.441794989Z level=info msg=\"scrape manager stopped\" component_path=/ component_id=prometheus.scrape.metrics_integrations_integrations_haproxy",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.444892517Z level=info msg=\"Stopping remote storage...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.445209771Z level=info msg=\"WAL watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.445374658Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.445523964Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.446572004Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 18:22:40 rev1-haproxy alloy[13537]: ts=2025-08-07T18:22:40.44673991Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 07 18:22:40 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 07 18:22:40 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 18:22:40 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.018394794Z level=info \"boringcrypto enabled\"=false",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:40.917790007Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.019151004Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.019277178Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.019418033Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.019553729Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=2",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.019689157Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=2",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.019805497Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=2 maxSegment=2",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.019935434Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.02005786Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.020178368Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=prometheus.remote_write.metrics_service duration=9.335871ms",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.020448008Z level=info msg=\"running usage stats reporter\"",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.020572671Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=a9de97 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=a9de97",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.020692635Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=remotecfg duration=87.065022ms",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.020828794Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.020965063Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=http duration=18.622µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024275519Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=cluster duration=3.697µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024449883Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=livedebugging duration=5.739µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024579611Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=ui duration=1.351µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024697103Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=labelstore duration=14.022µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024839111Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=168.721µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024958894Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=2.165979ms",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024973826Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=loki.write.grafana_cloud_loki duration=657.035µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.02498122Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=tracing duration=20.488µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.024993247Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=logging duration=6.623792ms",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.025354351Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 node_id=otel duration=339.031µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.02538184Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=2e62b9862a25c2751306d53c976b3d44 duration=106.781447ms",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.026922547Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.027229231Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.029535369Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.03048755Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.036922136Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=59c2f3535b432fe6775dd5f062e9445a",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.036943965Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=59c2f3535b432fe6775dd5f062e9445a duration=95.003µs",
        "Aug 07 18:22:41 rev1-haproxy alloy[31158]: ts=2025-08-07T18:22:41.050696418Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 18:22:45,060 p=1274933 u=ubuntu n=ansible | TASK [Check the Grafana alloy configuration file] *********************************************************************************************************************
2025-08-07 18:22:47,072 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:22:47,077 p=1274933 u=ubuntu n=ansible | TASK [Display the Grafana alloy config] *******************************************************************************************************************************
2025-08-07 18:22:47,093 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_config_response.stdout_lines": [
        "remotecfg {",
        "  url            = \"https://fleet-management-prod-016.grafana.net\"",
        "  id             = \"rev1-haproxy\"",
        "  poll_frequency = \"60s\"",
        "",
        "  basic_auth {",
        "    username = \"1303247\"",
        "    password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "  }",
        "}",
        "",
        "prometheus.remote_write \"metrics_service\" {",
        "  endpoint {",
        "    url = \"https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push\"",
        "    basic_auth {",
        "      username = \"2530729\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "loki.write \"grafana_cloud_loki\" {",
        "  endpoint {",
        "    url = \"https://logs-prod-025.grafana.net/loki/api/v1/push\"",
        "    basic_auth {",
        "      username = \"1261041\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "discovery.relabel \"metrics_integrations_integrations_haproxy\" {",
        "  targets = [{",
        "    __address__ = \"127.0.0.1:80\",",
        "  }]",
        "",
        "  rule {",
        "    target_label = \"instance\"",
        "    replacement  = constants.hostname",
        "  }",
        "}",
        "",
        "prometheus.scrape \"metrics_integrations_integrations_haproxy\" {",
        "  targets    = discovery.relabel.metrics_integrations_integrations_haproxy.output",
        "  forward_to = [prometheus.remote_write.metrics_service.receiver]",
        "  job_name   = \"integrations/haproxy\"",
        "}"
    ]
}
2025-08-07 18:22:47,112 p=1274933 u=ubuntu n=ansible | [WARNING]: Found variable using reserved name: timeout

2025-08-07 18:22:47,112 p=1274933 u=ubuntu n=ansible | PLAY [Install snmp, snmpd, NGINX UDP load balancer config for SNMP] ***************************************************************************************************
2025-08-07 18:22:47,115 p=1274933 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 18:22:50,391 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 18:22:50,402 p=1274933 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-07 18:22:54,208 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=nginx)
2025-08-07 18:22:58,040 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=snmpd)
2025-08-07 18:23:01,745 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=snmp)
2025-08-07 18:23:05,429 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=snmp-mibs-downloader)
2025-08-07 18:23:05,434 p=1274933 u=ubuntu n=ansible | TASK [Deploy NGINX stream config for SNMP UDP load balancing] *********************************************************************************************************
2025-08-07 18:23:09,023 p=1274933 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-07 18:23:09,028 p=1274933 u=ubuntu n=ansible | TASK [Check nginx is running] *****************************************************************************************************************************************
2025-08-07 18:23:10,976 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 18:23:10,981 p=1274933 u=ubuntu n=ansible | TASK [display nginx status] *******************************************************************************************************************************************
2025-08-07 18:23:11,004 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => {
    "nginx_running_status": {
        "changed": false,
        "failed": false,
        "name": "nginx",
        "state": "started",
        "status": {
            "ActiveEnterTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "ActiveEnterTimestampMonotonic": "341679164",
            "ActiveExitTimestampMonotonic": "0",
            "ActiveState": "active",
            "After": "systemd-journald.socket network.target basic.target sysinit.target system.slice",
            "AllowIsolate": "no",
            "AllowedCPUs": "",
            "AllowedMemoryNodes": "",
            "AmbientCapabilities": "",
            "AssertResult": "yes",
            "AssertTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "AssertTimestampMonotonic": "341619751",
            "Before": "shutdown.target multi-user.target",
            "BlockIOAccounting": "no",
            "BlockIOWeight": "[not set]",
            "CPUAccounting": "no",
            "CPUAffinity": "",
            "CPUAffinityFromNUMA": "no",
            "CPUQuotaPerSecUSec": "infinity",
            "CPUQuotaPeriodUSec": "infinity",
            "CPUSchedulingPolicy": "0",
            "CPUSchedulingPriority": "0",
            "CPUSchedulingResetOnFork": "no",
            "CPUShares": "[not set]",
            "CPUUsageNSec": "[not set]",
            "CPUWeight": "[not set]",
            "CacheDirectoryMode": "0755",
            "CanIsolate": "no",
            "CanReload": "yes",
            "CanStart": "yes",
            "CanStop": "yes",
            "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read",
            "CleanResult": "success",
            "CollectMode": "inactive",
            "ConditionResult": "yes",
            "ConditionTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "ConditionTimestampMonotonic": "341619751",
            "ConfigurationDirectoryMode": "0755",
            "Conflicts": "shutdown.target",
            "ControlGroup": "/system.slice/nginx.service",
            "ControlPID": "0",
            "DefaultDependencies": "yes",
            "DefaultMemoryLow": "0",
            "DefaultMemoryMin": "0",
            "Delegate": "no",
            "Description": "A high performance web server and a reverse proxy server",
            "DevicePolicy": "auto",
            "Documentation": "man:nginx(8)",
            "DynamicUser": "no",
            "EffectiveCPUs": "",
            "EffectiveMemoryNodes": "",
            "ExecMainCode": "0",
            "ExecMainExitTimestampMonotonic": "0",
            "ExecMainPID": "3443",
            "ExecMainStartTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "ExecMainStartTimestampMonotonic": "341679146",
            "ExecMainStatus": "0",
            "ExecReload": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; ignore_errors=no ; start_time=[Thu 2025-08-07 17:16:44 UTC] ; stop_time=[Thu 2025-08-07 17:16:44 UTC] ; pid=10975 ; code=exited ; status=0 }",
            "ExecReloadEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; flags= ; start_time=[Thu 2025-08-07 17:16:44 UTC] ; stop_time=[Thu 2025-08-07 17:16:44 UTC] ; pid=10975 ; code=exited ; status=0 }",
            "ExecStart": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPre": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPreEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStop": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStopEx": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "FailureAction": "none",
            "FileDescriptorStoreMax": "0",
            "FinalKillSignal": "9",
            "FragmentPath": "/lib/systemd/system/nginx.service",
            "GID": "[not set]",
            "GuessMainPID": "yes",
            "IOAccounting": "no",
            "IOReadBytes": "18446744073709551615",
            "IOReadOperations": "18446744073709551615",
            "IOSchedulingClass": "0",
            "IOSchedulingPriority": "0",
            "IOWeight": "[not set]",
            "IOWriteBytes": "18446744073709551615",
            "IOWriteOperations": "18446744073709551615",
            "IPAccounting": "no",
            "IPEgressBytes": "[no data]",
            "IPEgressPackets": "[no data]",
            "IPIngressBytes": "[no data]",
            "IPIngressPackets": "[no data]",
            "Id": "nginx.service",
            "IgnoreOnIsolate": "no",
            "IgnoreSIGPIPE": "yes",
            "InactiveEnterTimestampMonotonic": "0",
            "InactiveExitTimestamp": "Thu 2025-08-07 17:16:06 UTC",
            "InactiveExitTimestampMonotonic": "341621095",
            "InvocationID": "13ef5a9202394522995778f33bf8b6bf",
            "JobRunningTimeoutUSec": "infinity",
            "JobTimeoutAction": "none",
            "JobTimeoutUSec": "infinity",
            "KeyringMode": "private",
            "KillMode": "mixed",
            "KillSignal": "15",
            "LimitAS": "infinity",
            "LimitASSoft": "infinity",
            "LimitCORE": "infinity",
            "LimitCORESoft": "0",
            "LimitCPU": "infinity",
            "LimitCPUSoft": "infinity",
            "LimitDATA": "infinity",
            "LimitDATASoft": "infinity",
            "LimitFSIZE": "infinity",
            "LimitFSIZESoft": "infinity",
            "LimitLOCKS": "infinity",
            "LimitLOCKSSoft": "infinity",
            "LimitMEMLOCK": "65536",
            "LimitMEMLOCKSoft": "65536",
            "LimitMSGQUEUE": "819200",
            "LimitMSGQUEUESoft": "819200",
            "LimitNICE": "0",
            "LimitNICESoft": "0",
            "LimitNOFILE": "524288",
            "LimitNOFILESoft": "1024",
            "LimitNPROC": "15295",
            "LimitNPROCSoft": "15295",
            "LimitRSS": "infinity",
            "LimitRSSSoft": "infinity",
            "LimitRTPRIO": "0",
            "LimitRTPRIOSoft": "0",
            "LimitRTTIME": "infinity",
            "LimitRTTIMESoft": "infinity",
            "LimitSIGPENDING": "15295",
            "LimitSIGPENDINGSoft": "15295",
            "LimitSTACK": "infinity",
            "LimitSTACKSoft": "8388608",
            "LoadState": "loaded",
            "LockPersonality": "no",
            "LogLevelMax": "-1",
            "LogRateLimitBurst": "0",
            "LogRateLimitIntervalUSec": "0",
            "LogsDirectoryMode": "0755",
            "MainPID": "3443",
            "MemoryAccounting": "yes",
            "MemoryCurrent": "5804032",
            "MemoryDenyWriteExecute": "no",
            "MemoryHigh": "infinity",
            "MemoryLimit": "infinity",
            "MemoryLow": "0",
            "MemoryMax": "infinity",
            "MemoryMin": "0",
            "MemorySwapMax": "infinity",
            "MountAPIVFS": "no",
            "MountFlags": "",
            "NFileDescriptorStore": "0",
            "NRestarts": "0",
            "NUMAMask": "",
            "NUMAPolicy": "n/a",
            "Names": "nginx.service",
            "NeedDaemonReload": "no",
            "Nice": "0",
            "NoNewPrivileges": "no",
            "NonBlocking": "no",
            "NotifyAccess": "none",
            "OOMPolicy": "stop",
            "OOMScoreAdjust": "0",
            "OnFailureJobMode": "replace",
            "PIDFile": "/run/nginx.pid",
            "Perpetual": "no",
            "PrivateDevices": "no",
            "PrivateMounts": "no",
            "PrivateNetwork": "no",
            "PrivateTmp": "no",
            "PrivateUsers": "no",
            "ProtectControlGroups": "no",
            "ProtectHome": "no",
            "ProtectHostname": "no",
            "ProtectKernelLogs": "no",
            "ProtectKernelModules": "no",
            "ProtectKernelTunables": "no",
            "ProtectSystem": "no",
            "RefuseManualStart": "no",
            "RefuseManualStop": "no",
            "ReloadResult": "success",
            "RemainAfterExit": "no",
            "RemoveIPC": "no",
            "Requires": "sysinit.target system.slice",
            "Restart": "no",
            "RestartKillSignal": "15",
            "RestartUSec": "100ms",
            "RestrictNamespaces": "no",
            "RestrictRealtime": "no",
            "RestrictSUIDSGID": "no",
            "Result": "success",
            "RootDirectoryStartOnly": "no",
            "RuntimeDirectoryMode": "0755",
            "RuntimeDirectoryPreserve": "no",
            "RuntimeMaxUSec": "infinity",
            "SameProcessGroup": "no",
            "SecureBits": "0",
            "SendSIGHUP": "no",
            "SendSIGKILL": "yes",
            "Slice": "system.slice",
            "StandardError": "inherit",
            "StandardInput": "null",
            "StandardInputData": "",
            "StandardOutput": "journal",
            "StartLimitAction": "none",
            "StartLimitBurst": "5",
            "StartLimitIntervalUSec": "10s",
            "StartupBlockIOWeight": "[not set]",
            "StartupCPUShares": "[not set]",
            "StartupCPUWeight": "[not set]",
            "StartupIOWeight": "[not set]",
            "StateChangeTimestamp": "Thu 2025-08-07 17:16:44 UTC",
            "StateChangeTimestampMonotonic": "379505681",
            "StateDirectoryMode": "0755",
            "StatusErrno": "0",
            "StopWhenUnneeded": "no",
            "SubState": "running",
            "SuccessAction": "none",
            "SyslogFacility": "3",
            "SyslogLevel": "6",
            "SyslogLevelPrefix": "yes",
            "SyslogPriority": "30",
            "SystemCallErrorNumber": "0",
            "TTYReset": "no",
            "TTYVHangup": "no",
            "TTYVTDisallocate": "no",
            "TasksAccounting": "yes",
            "TasksCurrent": "2",
            "TasksMax": "4588",
            "TimeoutAbortUSec": "5s",
            "TimeoutCleanUSec": "infinity",
            "TimeoutStartUSec": "1min 30s",
            "TimeoutStopUSec": "5s",
            "TimerSlackNSec": "50000",
            "Transient": "no",
            "Type": "forking",
            "UID": "[not set]",
            "UMask": "0022",
            "UnitFilePreset": "enabled",
            "UnitFileState": "enabled",
            "UtmpMode": "init",
            "WantedBy": "multi-user.target",
            "WatchdogSignal": "6",
            "WatchdogTimestampMonotonic": "0",
            "WatchdogUSec": "0"
        }
    }
}
2025-08-07 18:23:11,055 p=1274933 u=ubuntu n=ansible | RUNNING HANDLER [Reload NGINX] ****************************************************************************************************************************************
2025-08-07 18:23:12,960 p=1274933 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-07 18:23:12,969 p=1274933 u=ubuntu n=ansible | PLAY [Test HAProxy (http), HAProxy Web stats (STATS)+ Metrics (PROMEX) and HAProxy logs] ******************************************************************************
2025-08-07 18:23:12,975 p=1274933 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 18:23:15,883 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:23:15,896 p=1274933 u=ubuntu n=ansible | TASK [Gather HAProxy server public IP address] ************************************************************************************************************************
2025-08-07 18:23:18,278 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:23:18,284 p=1274933 u=ubuntu n=ansible | TASK [Send HTTP request to HAProxy and collect response] **************************************************************************************************************
2025-08-07 18:23:20,555 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0)
2025-08-07 18:23:22,639 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1)
2025-08-07 18:23:37,054 p=1274933 u=ubuntu n=ansible | failed: [rev1_HAproxy] (item=2) => {"ansible_loop_var": "item", "attempts": 3, "cache_control": "no-cache", "changed": false, "content": "<html><body><h1>503 Service Unavailable</h1>\nNo server is available to handle this request.\n</body></html>\n\n", "content_type": "text/html", "elapsed": 0, "item": 2, "msg": "Status code was 503 and not [200]: HTTP Error 503: Service Unavailable", "redirected": false, "status": 503, "url": "http://185.62.207.61"}
2025-08-07 18:23:37,055 p=1274933 u=ubuntu n=ansible | ...ignoring
2025-08-07 18:23:37,060 p=1274933 u=ubuntu n=ansible | TASK [Display the HAProxy response] ***********************************************************************************************************************************
2025-08-07 18:23:37,078 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "18:23:20 10.1.1.21:48946 -- 10.1.1.20 (rev1-dev85) 80\n",
    "item": 0
}
2025-08-07 18:23:37,084 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "18:23:22 10.1.1.21:54110 -- 10.1.1.40 (rev1-dev69) 30\n",
    "item": 1
}
2025-08-07 18:23:37,088 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "<html><body><h1>503 Service Unavailable</h1>\nNo server is available to handle this request.\n</body></html>\n\n",
    "item": 2
}
2025-08-07 18:23:37,095 p=1274933 u=ubuntu n=ansible | TASK [Send HTTP requests to HAProxy stats page and collect responses] *************************************************************************************************
2025-08-07 18:23:51,764 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 18:23:51,770 p=1274933 u=ubuntu n=ansible | TASK [Display the stats response content] *****************************************************************************************************************************
2025-08-07 18:23:51,785 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_stats_response.content": "# pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,agent_status,agent_code,agent_duration,check_desc,agent_desc,check_rise,check_fall,check_health,agent_rise,agent_fall,agent_health,addr,cookie,mode,algo,conn_rate,conn_rate_max,conn_tot,intercepted,dcon,dses,wrew,connect,reuse,cache_lookups,cache_hits,srv_icur,src_ilim,qtime_max,ctime_max,rtime_max,ttime_max,eint,idle_conn_cur,safe_conn_cur,used_conn_cur,need_conn_est,uweight,agg_server_status,agg_server_check_status,agg_check_status,srid,sess_other,h1sess,h2sess,h3sess,req_other,h1req,h2req,h3req,proto,-,ssl_sess,ssl_reused_sess,ssl_failed_handshake,quic_rxbuf_full,quic_dropped_pkt,quic_dropped_pkt_bufoverrun,quic_dropped_parsing_pkt,quic_socket_full,quic_sendto_err,quic_sendto_err_unknwn,quic_sent_pkt,quic_lost_pkt,quic_too_short_dgram,quic_retry_sent,quic_retry_validated,quic_retry_error,quic_half_open_conn,quic_hdshk_fail,quic_stless_rst_sent,quic_conn_migration_done,quic_transp_err_no_error,quic_transp_err_internal_error,quic_transp_err_connection_refused,quic_transp_err_flow_control_error,quic_transp_err_stream_limit_error,quic_transp_err_stream_state_error,quic_transp_err_final_size_error,quic_transp_err_frame_encoding_error,quic_transp_err_transport_parameter_error,quic_transp_err_connection_id_limit,quic_transp_err_protocol_violation_error,quic_transp_err_invalid_token,quic_transp_err_application_error,quic_transp_err_crypto_buffer_exceeded,quic_transp_err_key_update_error,quic_transp_err_aead_limit_reached,quic_transp_err_no_viable_path,quic_transp_err_crypto_error,quic_transp_err_unknown_error,quic_data_blocked,quic_stream_data_blocked,quic_streams_blocked_bidi,quic_streams_blocked_uni,h3_data,h3_headers,h3_cancel_push,h3_push_promise,h3_max_push_id,h3_goaway,h3_settings,h3_no_error,h3_general_protocol_error,h3_internal_error,h3_stream_creation_error,h3_closed_critical_stream,h3_frame_unexpected,h3_frame_error,h3_excessive_load,h3_id_error,h3_settings_error,h3_missing_settings,h3_request_rejected,h3_request_cancelled,h3_request_incomplete,h3_message_error,h3_connect_error,h3_version_fallback,pack_decompression_failed,qpack_encoder_stream_error,qpack_decoder_stream_error,h2_headers_rcvd,h2_data_rcvd,h2_settings_rcvd,h2_rst_stream_rcvd,h2_goaway_rcvd,h2_detected_conn_protocol_errors,h2_detected_strm_protocol_errors,h2_rst_stream_resp,h2_goaway_resp,h2_open_connections,h2_backend_open_streams,h2_total_connections,h2_backend_total_streams,h1_open_connections,h1_open_streams,h1_total_connections,h1_total_streams,h1_bytes_in,h1_bytes_out,h1_spliced_bytes_in,h1_spliced_bytes_out,\nweb_stats,FRONTEND,,,1,1,262111,5,392,864,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,1,0,1,,,,0,0,0,0,4,0,,1,1,5,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,1,1,5,1,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,5,0,0,0,5,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,5,5,626,776,0,0,\nhaproxy_frontend,FRONTEND,,,0,1,262111,6,938,1886,0,0,0,,,,,OPEN,,,,,,,,,1,3,0,,,,0,0,0,1,,,,0,2,0,4,0,0,,0,1,6,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,6,0,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,6,0,0,0,6,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,1035,2923,0,0,\nhaproxy_backend,rev1_dev48,0,0,0,1,,1,292,368,,0,,0,0,0,0,UP,1,1,0,0,0,92,0,,1,4,1,,1,,2,0,,1,L4OK,,0,0,0,0,1,0,0,,,,1,0,0,,,,,40,,,0,1,9,11,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,1,9,11,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_dev85,0,0,0,1,,1,98,207,,0,,0,0,0,0,UP,1,1,0,0,0,92,0,,1,4,2,,1,,2,0,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,31,,,0,0,5,6,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,5,6,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_dev69,0,0,0,1,,1,98,207,,0,,0,0,0,0,UP,1,1,0,0,0,92,0,,1,4,3,,1,,2,0,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,29,,,0,0,11,12,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,11,12,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devC,0,0,0,1,,1,150,368,,0,,0,0,0,0,UP,1,1,0,0,0,92,0,,1,4,4,,1,,2,0,,1,L4OK,,0,0,0,0,1,0,0,,,,1,0,0,,,,,13,,,0,0,3,3,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,3,3,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devB,0,0,0,1,,1,150,368,,0,,0,0,0,0,UP,1,1,0,0,0,92,0,,1,4,5,,1,,2,0,,1,L4OK,,0,0,0,0,1,0,0,,,,1,0,0,,,,,9,,,0,0,2,2,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,2,2,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devA,0,0,0,1,,1,150,368,,0,,0,0,0,0,UP,1,1,0,0,0,92,0,,1,4,6,,1,,2,0,,1,L4OK,,0,0,0,0,1,0,0,,,,1,0,0,,,,,4,,,0,0,1,2,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,1,2,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,BACKEND,0,0,0,1,26212,6,938,1886,0,0,,0,0,0,0,UP,6,6,0,,0,92,0,,1,4,0,,6,,1,0,,1,,,,0,2,0,4,0,0,,,,6,0,0,0,0,0,0,4,,,0,1,6,6,,,,,,,,,,,,,,http,,,,,,,,0,6,0,0,0,,,0,1,11,12,0,,,,,6,0,0,0,,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,2942,940,0,0,\n"
}
2025-08-07 18:23:51,790 p=1274933 u=ubuntu n=ansible | TASK [Test the HAProxy metrics (promex) path] *************************************************************************************************************************
2025-08-07 18:23:53,843 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:23:53,849 p=1274933 u=ubuntu n=ansible | TASK [Display the HAProxy metrics (promex) response content] **********************************************************************************************************
2025-08-07 18:23:53,883 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_metrics_path_result.stdout_lines": [
        "# HELP haproxy_process_nbthread Number of started threads (global.nbthread)",
        "# TYPE haproxy_process_nbthread gauge",
        "haproxy_process_nbthread 1",
        "# HELP haproxy_process_nbproc Number of started worker processes (historical, always 1)",
        "# TYPE haproxy_process_nbproc gauge",
        "haproxy_process_nbproc 1",
        "# HELP haproxy_process_relative_process_id Relative worker process number (1)",
        "# TYPE haproxy_process_relative_process_id gauge",
        "haproxy_process_relative_process_id 1",
        "# HELP haproxy_process_uptime_seconds How long ago this worker process was started (seconds)",
        "# TYPE haproxy_process_uptime_seconds gauge",
        "haproxy_process_uptime_seconds 94",
        "# HELP haproxy_process_pool_failures_total Number of failed pool allocations since this worker was started",
        "# TYPE haproxy_process_pool_failures_total counter",
        "haproxy_process_pool_failures_total 0",
        "# HELP haproxy_process_max_fds Hard limit on the number of per-process file descriptors",
        "# TYPE haproxy_process_max_fds gauge",
        "haproxy_process_max_fds 524287",
        "# HELP haproxy_process_max_sockets Hard limit on the number of per-process sockets",
        "# TYPE haproxy_process_max_sockets gauge",
        "haproxy_process_max_sockets 524287",
        "# HELP haproxy_process_max_connections Hard limit on the number of per-process connections (configured or imposed by Ulimit-n)",
        "# TYPE haproxy_process_max_connections gauge",
        "haproxy_process_max_connections 262111",
        "# HELP haproxy_process_hard_max_connections Hard limit on the number of per-process connections (imposed by Memmax_MB or Ulimit-n)",
        "# TYPE haproxy_process_hard_max_connections gauge",
        "haproxy_process_hard_max_connections 262111",
        "# HELP haproxy_process_current_connections Current number of connections on this worker process",
        "# TYPE haproxy_process_current_connections gauge",
        "haproxy_process_current_connections 1",
        "# HELP haproxy_process_connections_total Total number of connections on this worker process since started",
        "# TYPE haproxy_process_connections_total counter",
        "haproxy_process_connections_total 295",
        "# HELP haproxy_process_requests_total Total number of requests on this worker process since started",
        "# TYPE haproxy_process_requests_total counter",
        "haproxy_process_requests_total 12",
        "# HELP haproxy_process_max_ssl_connections Hard limit on the number of per-process SSL endpoints (front+back), 0=unlimited",
        "# TYPE haproxy_process_max_ssl_connections gauge",
        "haproxy_process_max_ssl_connections 0",
        "# HELP haproxy_process_current_ssl_connections Current number of SSL endpoints on this worker process (front+back)",
        "# TYPE haproxy_process_current_ssl_connections gauge",
        "haproxy_process_current_ssl_connections 0",
        "# HELP haproxy_process_ssl_connections_total Total number of SSL endpoints on this worker process since started (front+back)",
        "# TYPE haproxy_process_ssl_connections_total counter",
        "haproxy_process_ssl_connections_total 0",
        "# HELP haproxy_process_max_pipes Hard limit on the number of pipes for splicing, 0=unlimited",
        "# TYPE haproxy_process_max_pipes gauge",
        "haproxy_process_max_pipes 0",
        "# HELP haproxy_process_pipes_used_total Current number of pipes in use in this worker process",
        "# TYPE haproxy_process_pipes_used_total counter",
        "haproxy_process_pipes_used_total 0",
        "# HELP haproxy_process_pipes_free_total Current number of allocated and available pipes in this worker process",
        "# TYPE haproxy_process_pipes_free_total counter",
        "haproxy_process_pipes_free_total 0",
        "# HELP haproxy_process_current_connection_rate Number of front connections created on this worker process over the last second",
        "# TYPE haproxy_process_current_connection_rate gauge",
        "haproxy_process_current_connection_rate 1",
        "# HELP haproxy_process_limit_connection_rate Hard limit for ConnRate (global.maxconnrate)",
        "# TYPE haproxy_process_limit_connection_rate gauge",
        "haproxy_process_limit_connection_rate 0",
        "# HELP haproxy_process_max_connection_rate Highest ConnRate reached on this worker process since started (in connections per second)",
        "# TYPE haproxy_process_max_connection_rate gauge",
        "haproxy_process_max_connection_rate 1",
        "# HELP haproxy_process_current_session_rate Number of sessions created on this worker process over the last second",
        "# TYPE haproxy_process_current_session_rate gauge",
        "haproxy_process_current_session_rate 1",
        "# HELP haproxy_process_limit_session_rate Hard limit for SessRate (global.maxsessrate)",
        "# TYPE haproxy_process_limit_session_rate gauge",
        "haproxy_process_limit_session_rate 0",
        "# HELP haproxy_process_max_session_rate Highest SessRate reached on this worker process since started (in sessions per second)",
        "# TYPE haproxy_process_max_session_rate gauge",
        "haproxy_process_max_session_rate 1",
        "# HELP haproxy_process_current_ssl_rate Number of SSL connections created on this worker process over the last second",
        "# TYPE haproxy_process_current_ssl_rate gauge",
        "haproxy_process_current_ssl_rate 0",
        "# HELP haproxy_process_limit_ssl_rate Hard limit for SslRate (global.maxsslrate)",
        "# TYPE haproxy_process_limit_ssl_rate gauge",
        "haproxy_process_limit_ssl_rate 0",
        "# HELP haproxy_process_max_ssl_rate Highest SslRate reached on this worker process since started (in connections per second)",
        "# TYPE haproxy_process_max_ssl_rate gauge",
        "haproxy_process_max_ssl_rate 0",
        "# HELP haproxy_process_current_frontend_ssl_key_rate Number of SSL keys created on frontends in this worker process over the last second",
        "# TYPE haproxy_process_current_frontend_ssl_key_rate gauge",
        "haproxy_process_current_frontend_ssl_key_rate 0",
        "# HELP haproxy_process_max_frontend_ssl_key_rate Highest SslFrontendKeyRate reached on this worker process since started (in SSL keys per second)",
        "# TYPE haproxy_process_max_frontend_ssl_key_rate gauge",
        "haproxy_process_max_frontend_ssl_key_rate 0",
        "# HELP haproxy_process_frontend_ssl_reuse Percent of frontend SSL connections which did not require a new key",
        "# TYPE haproxy_process_frontend_ssl_reuse gauge",
        "haproxy_process_frontend_ssl_reuse 0",
        "# HELP haproxy_process_current_backend_ssl_key_rate Number of SSL keys created on backends in this worker process over the last second",
        "# TYPE haproxy_process_current_backend_ssl_key_rate gauge",
        "haproxy_process_current_backend_ssl_key_rate 0",
        "# HELP haproxy_process_max_backend_ssl_key_rate Highest SslBackendKeyRate reached on this worker process since started (in SSL keys per second)",
        "# TYPE haproxy_process_max_backend_ssl_key_rate gauge",
        "haproxy_process_max_backend_ssl_key_rate 0",
        "# HELP haproxy_process_ssl_cache_lookups_total Total number of SSL session ID lookups in the SSL session cache on this worker since started",
        "# TYPE haproxy_process_ssl_cache_lookups_total counter",
        "haproxy_process_ssl_cache_lookups_total 0",
        "# HELP haproxy_process_ssl_cache_misses_total Total number of SSL session ID lookups that didn't find a session in the SSL session cache on this worker since started",
        "# TYPE haproxy_process_ssl_cache_misses_total counter",
        "haproxy_process_ssl_cache_misses_total 0",
        "# HELP haproxy_process_http_comp_bytes_in_total Number of bytes submitted to the HTTP compressor in this worker process over the last second",
        "# TYPE haproxy_process_http_comp_bytes_in_total counter",
        "haproxy_process_http_comp_bytes_in_total 0",
        "# HELP haproxy_process_http_comp_bytes_out_total Number of bytes emitted by the HTTP compressor in this worker process over the last second",
        "# TYPE haproxy_process_http_comp_bytes_out_total counter",
        "haproxy_process_http_comp_bytes_out_total 0",
        "# HELP haproxy_process_limit_http_comp Limit of CompressBpsOut beyond which HTTP compression is automatically disabled",
        "# TYPE haproxy_process_limit_http_comp gauge",
        "haproxy_process_limit_http_comp 0",
        "# HELP haproxy_process_current_zlib_memory Amount of memory currently used by HTTP compression on the current worker process (in bytes)",
        "# TYPE haproxy_process_current_zlib_memory gauge",
        "haproxy_process_current_zlib_memory NaN",
        "# HELP haproxy_process_max_zlib_memory Limit on the amount of memory used by HTTP compression above which it is automatically disabled (in bytes, see global.maxzlibmem)",
        "# TYPE haproxy_process_max_zlib_memory gauge",
        "haproxy_process_max_zlib_memory NaN",
        "# HELP haproxy_process_current_tasks Total number of tasks in the current worker process (active + sleeping)",
        "# TYPE haproxy_process_current_tasks gauge",
        "haproxy_process_current_tasks 22",
        "# HELP haproxy_process_current_run_queue Total number of active tasks+tasklets in the current worker process",
        "# TYPE haproxy_process_current_run_queue gauge",
        "haproxy_process_current_run_queue 0",
        "# HELP haproxy_process_idle_time_percent Percentage of last second spent waiting in the current worker thread",
        "# TYPE haproxy_process_idle_time_percent gauge",
        "haproxy_process_idle_time_percent 100",
        "# HELP haproxy_process_stopping 1 if the worker process is currently stopping, otherwise zero",
        "# TYPE haproxy_process_stopping gauge",
        "haproxy_process_stopping 0",
        "# HELP haproxy_process_jobs Current number of active jobs on the current worker process (frontend connections, master connections, listeners)",
        "# TYPE haproxy_process_jobs gauge",
        "haproxy_process_jobs 6",
        "# HELP haproxy_process_unstoppable_jobs Current number of unstoppable jobs on the current worker process (master connections)",
        "# TYPE haproxy_process_unstoppable_jobs gauge",
        "haproxy_process_unstoppable_jobs 1",
        "# HELP haproxy_process_listeners Current number of active listeners on the current worker process",
        "# TYPE haproxy_process_listeners gauge",
        "haproxy_process_listeners 4",
        "# HELP haproxy_process_active_peers Current number of verified active peers connections on the current worker process",
        "# TYPE haproxy_process_active_peers gauge",
        "haproxy_process_active_peers 0",
        "# HELP haproxy_process_connected_peers Current number of peers having passed the connection step on the current worker process",
        "# TYPE haproxy_process_connected_peers gauge",
        "haproxy_process_connected_peers 0",
        "# HELP haproxy_process_dropped_logs_total Total number of dropped logs for current worker process since started",
        "# TYPE haproxy_process_dropped_logs_total counter",
        "haproxy_process_dropped_logs_total 0",
        "# HELP haproxy_process_busy_polling_enabled 1 if busy-polling is currently in use on the worker process, otherwise zero (config.busy-polling)",
        "# TYPE haproxy_process_busy_polling_enabled gauge",
        "haproxy_process_busy_polling_enabled 0",
        "# HELP haproxy_process_failed_resolutions Total number of failed DNS resolutions in current worker process since started",
        "# TYPE haproxy_process_failed_resolutions counter",
        "haproxy_process_failed_resolutions 0",
        "# HELP haproxy_process_bytes_out_total Total number of bytes emitted by current worker process since started",
        "# TYPE haproxy_process_bytes_out_total counter",
        "haproxy_process_bytes_out_total 9916",
        "# HELP haproxy_process_spliced_bytes_out_total Total number of bytes emitted by current worker process through a kernel pipe since started",
        "# TYPE haproxy_process_spliced_bytes_out_total counter",
        "haproxy_process_spliced_bytes_out_total 0",
        "# HELP haproxy_process_bytes_out_rate Number of bytes emitted by current worker process over the last second",
        "# TYPE haproxy_process_bytes_out_rate gauge",
        "haproxy_process_bytes_out_rate 0",
        "# HELP haproxy_process_recv_logs_total Total number of log messages received by log-forwarding listeners on this worker process since started",
        "# TYPE haproxy_process_recv_logs_total counter",
        "haproxy_process_recv_logs_total 0",
        "# HELP haproxy_process_build_info Build info",
        "# TYPE haproxy_process_build_info gauge",
        "haproxy_process_build_info{version=\"2.9.15-1ppa1~focal\"} 1",
        "# HELP haproxy_process_max_memory_bytes Worker process's hard limit on memory usage in byes (-m on command line)",
        "# TYPE haproxy_process_max_memory_bytes gauge",
        "haproxy_process_max_memory_bytes 0",
        "# HELP haproxy_process_pool_allocated_bytes Amount of memory allocated in pools (in bytes)",
        "# TYPE haproxy_process_pool_allocated_bytes gauge",
        "haproxy_process_pool_allocated_bytes 73960",
        "# HELP haproxy_process_pool_used_bytes Amount of pool memory currently used (in bytes)",
        "# TYPE haproxy_process_pool_used_bytes gauge",
        "haproxy_process_pool_used_bytes 73960",
        "# HELP haproxy_process_start_time_seconds Start time in seconds",
        "# TYPE haproxy_process_start_time_seconds gauge",
        "haproxy_process_start_time_seconds 1754590939",
        "# HELP haproxy_frontend_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_frontend_current_sessions gauge",
        "haproxy_frontend_current_sessions{proxy=\"web_stats\"} 1",
        "haproxy_frontend_current_sessions{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_frontend_max_sessions gauge",
        "haproxy_frontend_max_sessions{proxy=\"web_stats\"} 1",
        "haproxy_frontend_max_sessions{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_frontend_limit_sessions gauge",
        "haproxy_frontend_limit_sessions{proxy=\"web_stats\"} 262111",
        "haproxy_frontend_limit_sessions{proxy=\"haproxy_frontend\"} 262111",
        "# HELP haproxy_frontend_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_frontend_sessions_total counter",
        "haproxy_frontend_sessions_total{proxy=\"web_stats\"} 6",
        "haproxy_frontend_sessions_total{proxy=\"haproxy_frontend\"} 6",
        "# HELP haproxy_frontend_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_frontend_bytes_in_total counter",
        "haproxy_frontend_bytes_in_total{proxy=\"web_stats\"} 542",
        "haproxy_frontend_bytes_in_total{proxy=\"haproxy_frontend\"} 938",
        "# HELP haproxy_frontend_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_frontend_bytes_out_total counter",
        "haproxy_frontend_bytes_out_total{proxy=\"web_stats\"} 7044",
        "haproxy_frontend_bytes_out_total{proxy=\"haproxy_frontend\"} 1886",
        "# HELP haproxy_frontend_requests_denied_total Total number of denied requests since process started",
        "# TYPE haproxy_frontend_requests_denied_total counter",
        "haproxy_frontend_requests_denied_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_requests_denied_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_frontend_responses_denied_total counter",
        "haproxy_frontend_responses_denied_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_responses_denied_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_request_errors_total Total number of invalid requests since process started",
        "# TYPE haproxy_frontend_request_errors_total counter",
        "haproxy_frontend_request_errors_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_request_errors_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_status Current status of the service, per state label value.",
        "# TYPE haproxy_frontend_status gauge",
        "haproxy_frontend_status{proxy=\"web_stats\",state=\"DOWN\"} 0",
        "haproxy_frontend_status{proxy=\"web_stats\",state=\"UP\"} 1",
        "haproxy_frontend_status{proxy=\"haproxy_frontend\",state=\"DOWN\"} 0",
        "haproxy_frontend_status{proxy=\"haproxy_frontend\",state=\"UP\"} 1",
        "# HELP haproxy_frontend_limit_session_rate Limit on the number of sessions accepted in a second (frontend only, 'rate-limit sessions' setting)",
        "# TYPE haproxy_frontend_limit_session_rate gauge",
        "haproxy_frontend_limit_session_rate{proxy=\"web_stats\"} 0",
        "haproxy_frontend_limit_session_rate{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_frontend_max_session_rate gauge",
        "haproxy_frontend_max_session_rate{proxy=\"web_stats\"} 1",
        "haproxy_frontend_max_session_rate{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_frontend_http_responses_total counter",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"1xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"1xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"2xx\"} 1",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"2xx\"} 2",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"3xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"3xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"4xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"4xx\"} 4",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"5xx\"} 4",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"5xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"other\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"other\"} 0",
        "# HELP haproxy_frontend_http_requests_rate_max Highest value of http requests observed since the worker process started",
        "# TYPE haproxy_frontend_http_requests_rate_max gauge",
        "haproxy_frontend_http_requests_rate_max{proxy=\"web_stats\"} 1",
        "haproxy_frontend_http_requests_rate_max{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_http_requests_total Total number of HTTP requests processed by this object since the worker process started",
        "# TYPE haproxy_frontend_http_requests_total counter",
        "haproxy_frontend_http_requests_total{proxy=\"web_stats\"} 6",
        "haproxy_frontend_http_requests_total{proxy=\"haproxy_frontend\"} 6",
        "# HELP haproxy_frontend_http_comp_bytes_in_total Total number of bytes submitted to the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_bytes_in_total counter",
        "haproxy_frontend_http_comp_bytes_in_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_in_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_bytes_out_total Total number of bytes emitted by the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_bytes_out_total counter",
        "haproxy_frontend_http_comp_bytes_out_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_out_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_bytes_bypassed_total Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)",
        "# TYPE haproxy_frontend_http_comp_bytes_bypassed_total counter",
        "haproxy_frontend_http_comp_bytes_bypassed_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_bypassed_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_responses_total Total number of HTTP responses that were compressed for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_responses_total counter",
        "haproxy_frontend_http_comp_responses_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_responses_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_connections_rate_max Highest value of connections per second observed since the worker process started",
        "# TYPE haproxy_frontend_connections_rate_max gauge",
        "haproxy_frontend_connections_rate_max{proxy=\"web_stats\"} 1",
        "haproxy_frontend_connections_rate_max{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_connections_total Total number of new connections accepted on this frontend since the worker process started",
        "# TYPE haproxy_frontend_connections_total counter",
        "haproxy_frontend_connections_total{proxy=\"web_stats\"} 6",
        "haproxy_frontend_connections_total{proxy=\"haproxy_frontend\"} 6",
        "# HELP haproxy_frontend_intercepted_requests_total Total number of HTTP requests intercepted on the frontend (redirects/stats/services) since the worker process started",
        "# TYPE haproxy_frontend_intercepted_requests_total counter",
        "haproxy_frontend_intercepted_requests_total{proxy=\"web_stats\"} 2",
        "haproxy_frontend_intercepted_requests_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_denied_connections_total Total number of incoming connections blocked on a listener/frontend by a tcp-request connection rule since the worker process started",
        "# TYPE haproxy_frontend_denied_connections_total counter",
        "haproxy_frontend_denied_connections_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_denied_connections_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_denied_sessions_total Total number of incoming sessions blocked on a listener/frontend by a tcp-request connection rule since the worker process started",
        "# TYPE haproxy_frontend_denied_sessions_total counter",
        "haproxy_frontend_denied_sessions_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_denied_sessions_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_frontend_failed_header_rewriting_total counter",
        "haproxy_frontend_failed_header_rewriting_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_failed_header_rewriting_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_cache_lookups_total Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_frontend_http_cache_lookups_total counter",
        "haproxy_frontend_http_cache_lookups_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_cache_lookups_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_cache_hits_total Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_frontend_http_cache_hits_total counter",
        "haproxy_frontend_http_cache_hits_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_cache_hits_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_frontend_internal_errors_total counter",
        "haproxy_frontend_internal_errors_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_internal_errors_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_backend_current_queue Number of current queued connections",
        "# TYPE haproxy_backend_current_queue gauge",
        "haproxy_backend_current_queue{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_queue Highest value of queued connections encountered since process started",
        "# TYPE haproxy_backend_max_queue gauge",
        "haproxy_backend_max_queue{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_backend_current_sessions gauge",
        "haproxy_backend_current_sessions{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_backend_max_sessions gauge",
        "haproxy_backend_max_sessions{proxy=\"haproxy_backend\"} 1",
        "# HELP haproxy_backend_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_backend_limit_sessions gauge",
        "haproxy_backend_limit_sessions{proxy=\"haproxy_backend\"} 26212",
        "# HELP haproxy_backend_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_backend_sessions_total counter",
        "haproxy_backend_sessions_total{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_backend_bytes_in_total counter",
        "haproxy_backend_bytes_in_total{proxy=\"haproxy_backend\"} 938",
        "# HELP haproxy_backend_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_backend_bytes_out_total counter",
        "haproxy_backend_bytes_out_total{proxy=\"haproxy_backend\"} 1886",
        "# HELP haproxy_backend_requests_denied_total Total number of denied requests since process started",
        "# TYPE haproxy_backend_requests_denied_total counter",
        "haproxy_backend_requests_denied_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_backend_responses_denied_total counter",
        "haproxy_backend_responses_denied_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_connection_errors_total Total number of failed connections to server since the worker process started",
        "# TYPE haproxy_backend_connection_errors_total counter",
        "haproxy_backend_connection_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_response_errors_total Total number of invalid responses since the worker process started",
        "# TYPE haproxy_backend_response_errors_total counter",
        "haproxy_backend_response_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_retry_warnings_total Total number of server connection retries since the worker process started",
        "# TYPE haproxy_backend_retry_warnings_total counter",
        "haproxy_backend_retry_warnings_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_redispatch_warnings_total Total number of server redispatches due to connection failures since the worker process started",
        "# TYPE haproxy_backend_redispatch_warnings_total counter",
        "haproxy_backend_redispatch_warnings_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_status Current status of the service, per state label value.",
        "# TYPE haproxy_backend_status gauge",
        "haproxy_backend_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_status{proxy=\"haproxy_backend\",state=\"UP\"} 1",
        "# HELP haproxy_backend_weight Server's effective weight, or sum of active servers' effective weights for a backend",
        "# TYPE haproxy_backend_weight gauge",
        "haproxy_backend_weight{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_active_servers Total number of active UP servers with a non-zero weight",
        "# TYPE haproxy_backend_active_servers gauge",
        "haproxy_backend_active_servers{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_backup_servers Total number of backup UP servers with a non-zero weight",
        "# TYPE haproxy_backend_backup_servers gauge",
        "haproxy_backend_backup_servers{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_check_up_down_total Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started",
        "# TYPE haproxy_backend_check_up_down_total counter",
        "haproxy_backend_check_up_down_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_check_last_change_seconds How long ago the last server state changed, in seconds",
        "# TYPE haproxy_backend_check_last_change_seconds gauge",
        "haproxy_backend_check_last_change_seconds{proxy=\"haproxy_backend\"} 94",
        "# HELP haproxy_backend_downtime_seconds_total Total time spent in DOWN state, for server or backend",
        "# TYPE haproxy_backend_downtime_seconds_total counter",
        "haproxy_backend_downtime_seconds_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_loadbalanced_total Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)",
        "# TYPE haproxy_backend_loadbalanced_total counter",
        "haproxy_backend_loadbalanced_total{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_backend_max_session_rate gauge",
        "haproxy_backend_max_session_rate{proxy=\"haproxy_backend\"} 1",
        "# HELP haproxy_backend_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_backend_http_responses_total counter",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"1xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"2xx\"} 2",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"3xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"4xx\"} 4",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"5xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"other\"} 0",
        "# HELP haproxy_backend_http_requests_total Total number of HTTP requests processed by this object since the worker process started",
        "# TYPE haproxy_backend_http_requests_total counter",
        "haproxy_backend_http_requests_total{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_client_aborts_total Total number of requests or connections aborted by the client since the worker process started",
        "# TYPE haproxy_backend_client_aborts_total counter",
        "haproxy_backend_client_aborts_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_server_aborts_total Total number of requests or connections aborted by the server since the worker process started",
        "# TYPE haproxy_backend_server_aborts_total counter",
        "haproxy_backend_server_aborts_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_in_total Total number of bytes submitted to the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_bytes_in_total counter",
        "haproxy_backend_http_comp_bytes_in_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_out_total Total number of bytes emitted by the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_bytes_out_total counter",
        "haproxy_backend_http_comp_bytes_out_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_bypassed_total Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)",
        "# TYPE haproxy_backend_http_comp_bytes_bypassed_total counter",
        "haproxy_backend_http_comp_bytes_bypassed_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_responses_total Total number of HTTP responses that were compressed for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_responses_total counter",
        "haproxy_backend_http_comp_responses_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_last_session_seconds How long ago some traffic was seen on this object on this worker process, in seconds",
        "# TYPE haproxy_backend_last_session_seconds gauge",
        "haproxy_backend_last_session_seconds{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_queue_time_average_seconds Avg. queue time for last 1024 successful connections.",
        "# TYPE haproxy_backend_queue_time_average_seconds gauge",
        "haproxy_backend_queue_time_average_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_connect_time_average_seconds Avg. connect time for last 1024 successful connections.",
        "# TYPE haproxy_backend_connect_time_average_seconds gauge",
        "haproxy_backend_connect_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_response_time_average_seconds Avg. response time for last 1024 successful connections.",
        "# TYPE haproxy_backend_response_time_average_seconds gauge",
        "haproxy_backend_response_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_total_time_average_seconds Avg. total time for last 1024 successful connections.",
        "# TYPE haproxy_backend_total_time_average_seconds gauge",
        "haproxy_backend_total_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_backend_failed_header_rewriting_total counter",
        "haproxy_backend_failed_header_rewriting_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_connection_attempts_total Total number of outgoing connection attempts on this backend/server since the worker process started",
        "# TYPE haproxy_backend_connection_attempts_total counter",
        "haproxy_backend_connection_attempts_total{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_connection_reuses_total Total number of reused connection on this backend/server since the worker process started",
        "# TYPE haproxy_backend_connection_reuses_total counter",
        "haproxy_backend_connection_reuses_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_cache_lookups_total Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_backend_http_cache_lookups_total counter",
        "haproxy_backend_http_cache_lookups_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_cache_hits_total Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_backend_http_cache_hits_total counter",
        "haproxy_backend_http_cache_hits_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_queue_time_seconds Maximum observed time spent in the queue",
        "# TYPE haproxy_backend_max_queue_time_seconds gauge",
        "haproxy_backend_max_queue_time_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_max_connect_time_seconds Maximum observed time spent waiting for a connection to complete",
        "# TYPE haproxy_backend_max_connect_time_seconds gauge",
        "haproxy_backend_max_connect_time_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_max_response_time_seconds Maximum observed time spent waiting for a server response",
        "# TYPE haproxy_backend_max_response_time_seconds gauge",
        "haproxy_backend_max_response_time_seconds{proxy=\"haproxy_backend\"} 0.011000",
        "# HELP haproxy_backend_max_total_time_seconds Maximum observed total request+response time (request+queue+connect+response+processing)",
        "# TYPE haproxy_backend_max_total_time_seconds gauge",
        "haproxy_backend_max_total_time_seconds{proxy=\"haproxy_backend\"} 0.012000",
        "# HELP haproxy_backend_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_backend_internal_errors_total counter",
        "haproxy_backend_internal_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_uweight Server's user weight, or sum of active servers' user weights for a backend",
        "# TYPE haproxy_backend_uweight gauge",
        "haproxy_backend_uweight{proxy=\"haproxy_backend\"} 6",
        "# HELP haproxy_backend_agg_server_status Backend's aggregated gauge of servers' status",
        "# TYPE haproxy_backend_agg_server_status gauge",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"UP\"} 6",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"MAINT\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"DRAIN\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"NOLB\"} 0",
        "# HELP haproxy_backend_agg_server_check_status [DEPRECATED] Backend's aggregated gauge of servers' status",
        "# TYPE haproxy_backend_agg_server_check_status gauge",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"UP\"} 6",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"MAINT\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"DRAIN\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"NOLB\"} 0",
        "# HELP haproxy_backend_agg_check_status Backend's aggregated gauge of servers' state check status",
        "# TYPE haproxy_backend_agg_check_status gauge",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"HANA\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"SOCKERR\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4OK\"} 6",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4CON\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6OK\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6RSP\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7RSP\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7OK\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7OKC\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7STS\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCERR\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCTOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCOK\"} 0",
        "# HELP haproxy_server_current_queue Number of current queued connections",
        "# TYPE haproxy_server_current_queue gauge",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_max_queue Highest value of queued connections encountered since process started",
        "# TYPE haproxy_server_max_queue gauge",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_server_current_sessions gauge",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_server_max_sessions gauge",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "# HELP haproxy_server_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_server_limit_sessions gauge",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} NaN",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} NaN",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} NaN",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "# HELP haproxy_server_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_server_sessions_total counter",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "# HELP haproxy_server_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_server_bytes_in_total counter",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 292",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 98",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 98",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 150",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 150",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 150",
        "# HELP haproxy_server_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_server_bytes_out_total counter",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 368",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 207",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 207",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 368",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 368",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 368",
        "# HELP haproxy_server_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_server_responses_denied_total counter",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_connection_errors_total Total number of failed connections to server since the worker process started",
        "# TYPE haproxy_server_connection_errors_total counter",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_response_errors_total Total number of invalid responses since the worker process started",
        "# TYPE haproxy_server_response_errors_total counter",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_retry_warnings_total Total number of server connection retries since the worker process started",
        "# TYPE haproxy_server_retry_warnings_total counter",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_redispatch_warnings_total Total number of server redispatches due to connection failures since the worker process started",
        "# TYPE haproxy_server_redispatch_warnings_total counter",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_status Current status of the service, per state label value.",
        "# TYPE haproxy_server_status gauge",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"NOLB\"} 0",
        "# HELP haproxy_server_weight Server's effective weight, or sum of active servers' effective weights for a backend",
        "# TYPE haproxy_server_weight gauge",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "# HELP haproxy_server_check_failures_total Total number of failed individual health checks per server/backend, since the worker process started",
        "# TYPE haproxy_server_check_failures_total counter",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_check_up_down_total Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started",
        "# TYPE haproxy_server_check_up_down_total counter",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_check_last_change_seconds How long ago the last server state changed, in seconds",
        "# TYPE haproxy_server_check_last_change_seconds gauge",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 94",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 94",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 94",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 94",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 94",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 94",
        "# HELP haproxy_server_downtime_seconds_total Total time spent in DOWN state, for server or backend",
        "# TYPE haproxy_server_downtime_seconds_total counter",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_queue_limit Limit on the number of connections in queue, for servers only (maxqueue argument)",
        "# TYPE haproxy_server_queue_limit gauge",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "# HELP haproxy_server_current_throttle Throttling ratio applied to a server's maxconn and weight during the slowstart period (0 to 100%)",
        "# TYPE haproxy_server_current_throttle gauge",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "# HELP haproxy_server_loadbalanced_total Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)",
        "# TYPE haproxy_server_loadbalanced_total counter",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "# HELP haproxy_server_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_server_max_session_rate gauge",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "# HELP haproxy_server_check_status Status of last health check, per state label value.",
        "# TYPE haproxy_server_check_status gauge",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev48\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev85\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_dev69\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCOK\"} 0",
        "# HELP haproxy_server_check_code layer5-7 code, if available of the last health check.",
        "# TYPE haproxy_server_check_code gauge",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_check_duration_seconds Total duration of the latest server health check, in seconds.",
        "# TYPE haproxy_server_check_duration_seconds gauge",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.001000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.000000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.000000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "# HELP haproxy_server_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_server_http_responses_total counter",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\",code=\"2xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\",code=\"2xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\",code=\"2xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"2xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"2xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"2xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\",code=\"4xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\",code=\"4xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\",code=\"4xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"4xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"4xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"4xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"other\"} 0",
        "# HELP haproxy_server_client_aborts_total Total number of requests or connections aborted by the client since the worker process started",
        "# TYPE haproxy_server_client_aborts_total counter",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_server_aborts_total Total number of requests or connections aborted by the server since the worker process started",
        "# TYPE haproxy_server_server_aborts_total counter",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_last_session_seconds How long ago some traffic was seen on this object on this worker process, in seconds",
        "# TYPE haproxy_server_last_session_seconds gauge",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 42",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 33",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 31",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 15",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 11",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 6",
        "# HELP haproxy_server_queue_time_average_seconds Avg. queue time for last 1024 successful connections.",
        "# TYPE haproxy_server_queue_time_average_seconds gauge",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "# HELP haproxy_server_connect_time_average_seconds Avg. connect time for last 1024 successful connections.",
        "# TYPE haproxy_server_connect_time_average_seconds gauge",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.001000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.000000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.000000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "# HELP haproxy_server_response_time_average_seconds Avg. response time for last 1024 successful connections.",
        "# TYPE haproxy_server_response_time_average_seconds gauge",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "# HELP haproxy_server_total_time_average_seconds Avg. total time for last 1024 successful connections.",
        "# TYPE haproxy_server_total_time_average_seconds gauge",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "# HELP haproxy_server_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_server_failed_header_rewriting_total counter",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_connection_attempts_total Total number of outgoing connection attempts on this backend/server since the worker process started",
        "# TYPE haproxy_server_connection_attempts_total counter",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "# HELP haproxy_server_connection_reuses_total Total number of reused connection on this backend/server since the worker process started",
        "# TYPE haproxy_server_connection_reuses_total counter",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_idle_connections_current Current number of idle connections available for reuse on this server",
        "# TYPE haproxy_server_idle_connections_current gauge",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_idle_connections_limit Limit on the number of available idle connections on this server (server 'pool_max_conn' directive)",
        "# TYPE haproxy_server_idle_connections_limit gauge",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "# HELP haproxy_server_max_queue_time_seconds Maximum observed time spent in the queue",
        "# TYPE haproxy_server_max_queue_time_seconds gauge",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "# HELP haproxy_server_max_connect_time_seconds Maximum observed time spent waiting for a connection to complete",
        "# TYPE haproxy_server_max_connect_time_seconds gauge",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.001000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.000000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.000000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "# HELP haproxy_server_max_response_time_seconds Maximum observed time spent waiting for a server response",
        "# TYPE haproxy_server_max_response_time_seconds gauge",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.009000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.005000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.011000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.003000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.002000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "# HELP haproxy_server_max_total_time_seconds Maximum observed total request+response time (request+queue+connect+response+processing)",
        "# TYPE haproxy_server_max_total_time_seconds gauge",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0.011000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0.006000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0.012000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.003000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.002000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.002000",
        "# HELP haproxy_server_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_server_internal_errors_total counter",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_unsafe_idle_connections_current Current number of unsafe idle connections",
        "# TYPE haproxy_server_unsafe_idle_connections_current gauge",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_safe_idle_connections_current Current number of safe idle connections",
        "# TYPE haproxy_server_safe_idle_connections_current gauge",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_used_connections_current Current number of connections in use",
        "# TYPE haproxy_server_used_connections_current gauge",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "# HELP haproxy_server_need_connections_current Estimated needed number of connections",
        "# TYPE haproxy_server_need_connections_current gauge",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "# HELP haproxy_server_uweight Server's user weight, or sum of active servers' user weights for a backend",
        "# TYPE haproxy_server_uweight gauge",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_dev48\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_dev85\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_dev69\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1"
    ]
}
2025-08-07 18:23:53,888 p=1274933 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-07 18:23:55,961 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:23:55,967 p=1274933 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-07 18:23:55,983 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54594 [07/Aug/2025:18:13:16.916] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/1/1 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:7910 cpu_ns_avg:1977 lat_ns_tot:3178 lat_ns_avg:794",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54608 [07/Aug/2025:18:13:16.918] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/1/1 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:5987 cpu_ns_avg:1496 lat_ns_tot:2734 lat_ns_avg:683",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54620 [07/Aug/2025:18:13:16.922] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/1/2 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:6295 cpu_ns_avg:1573 lat_ns_tot:2776 lat_ns_avg:694",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54634 [07/Aug/2025:18:13:16.925] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/1/1 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:4917 cpu_ns_avg:1229 lat_ns_tot:2120 lat_ns_avg:530",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54720 [07/Aug/2025:18:13:16.953] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:4996 cpu_ns_avg:1249 lat_ns_tot:2392 lat_ns_avg:598",
        "Aug  7 18:14:11 rev1-haproxy haproxy[10251]: 127.0.0.1:35636 [07/Aug/2025:18:14:11.737] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14705 cpu_ns_avg:3676 lat_ns_tot:3563 lat_ns_avg:890",
        "Aug  7 18:22:11 rev1-haproxy haproxy[10251]: 127.0.0.1:50868 [07/Aug/2025:18:22:11.738] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:16496 cpu_ns_avg:4124 lat_ns_tot:5825 lat_ns_avg:1456",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : path to executable is /usr/sbin/haproxy",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [WARNING]  (10249) : Exiting Master process...",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [ALERT]    (10249) : Current worker (10251) exited with code 143 (Terminated)",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [WARNING]  (10249) : All workers exited. Exiting... (0)",
        "Aug  7 18:22:19 rev1-haproxy haproxy[28549]: [NOTICE]   (28549) : New worker (28551) forked",
        "Aug  7 18:22:19 rev1-haproxy haproxy[28549]: [NOTICE]   (28549) : Loading success.",
        "Aug  7 18:23:11 rev1-haproxy haproxy[28551]: 127.0.0.1:40514 [07/Aug/2025:18:23:11.741] haproxy_frontend haproxy_backend/rev1_dev48 0/0/1/9/10 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:56924 cpu_ns_avg:14231 lat_ns_tot:17275 lat_ns_avg:4318",
        "Aug  7 18:23:20 rev1-haproxy haproxy[28551]: 185.62.207.61:35710 [07/Aug/2025:18:23:20.193] haproxy_frontend haproxy_backend/rev1_dev85 0/0/0/5/6 200 207 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13601 cpu_ns_avg:3400 lat_ns_tot:5738 lat_ns_avg:1434",
        "Aug  7 18:23:22 rev1-haproxy haproxy[28551]: 185.62.207.61:26473 [07/Aug/2025:18:23:22.285] haproxy_frontend haproxy_backend/rev1_dev69 0/0/0/11/12 200 207 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14399 cpu_ns_avg:3599 lat_ns_tot:5331 lat_ns_avg:1332",
        "Aug  7 18:23:38 rev1-haproxy haproxy[28551]: 185.62.207.61:24779 [07/Aug/2025:18:23:38.841] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/3/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10533 cpu_ns_avg:2633 lat_ns_tot:3213 lat_ns_avg:803",
        "Aug  7 18:23:42 rev1-haproxy haproxy[28551]: 185.62.207.61:22051 [07/Aug/2025:18:23:42.989] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12476 cpu_ns_avg:3119 lat_ns_tot:7591 lat_ns_avg:1897",
        "Aug  7 18:23:47 rev1-haproxy haproxy[28551]: 185.62.207.61:14594 [07/Aug/2025:18:23:47.214] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/1/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10126 cpu_ns_avg:2531 lat_ns_tot:4104 lat_ns_avg:1026"
    ]
}
2025-08-07 18:23:55,988 p=1274933 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-07 18:23:58,019 p=1274933 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 18:23:58,025 p=1274933 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-07 18:23:58,043 p=1274933 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54594 [07/Aug/2025:18:13:16.916] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/1/1 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:7910 cpu_ns_avg:1977 lat_ns_tot:3178 lat_ns_avg:794",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54608 [07/Aug/2025:18:13:16.918] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/1/1 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:5987 cpu_ns_avg:1496 lat_ns_tot:2734 lat_ns_avg:683",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54620 [07/Aug/2025:18:13:16.922] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/1/2 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:6295 cpu_ns_avg:1573 lat_ns_tot:2776 lat_ns_avg:694",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54634 [07/Aug/2025:18:13:16.925] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/1/1 200 206 - - ---- 2/2/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:4917 cpu_ns_avg:1229 lat_ns_tot:2120 lat_ns_avg:530",
        "Aug  7 18:13:16 rev1-haproxy haproxy[10251]: 13.48.133.136:54720 [07/Aug/2025:18:13:16.953] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:4996 cpu_ns_avg:1249 lat_ns_tot:2392 lat_ns_avg:598",
        "Aug  7 18:14:11 rev1-haproxy haproxy[10251]: 127.0.0.1:35636 [07/Aug/2025:18:14:11.737] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14705 cpu_ns_avg:3676 lat_ns_tot:3563 lat_ns_avg:890",
        "Aug  7 18:22:11 rev1-haproxy haproxy[10251]: 127.0.0.1:50868 [07/Aug/2025:18:22:11.738] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:16496 cpu_ns_avg:4124 lat_ns_tot:5825 lat_ns_avg:1456",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [NOTICE]   (10249) : path to executable is /usr/sbin/haproxy",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [WARNING]  (10249) : Exiting Master process...",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [ALERT]    (10249) : Current worker (10251) exited with code 143 (Terminated)",
        "Aug  7 18:22:19 rev1-haproxy haproxy[10249]: [WARNING]  (10249) : All workers exited. Exiting... (0)",
        "Aug  7 18:22:19 rev1-haproxy haproxy[28549]: [NOTICE]   (28549) : New worker (28551) forked",
        "Aug  7 18:22:19 rev1-haproxy haproxy[28549]: [NOTICE]   (28549) : Loading success.",
        "Aug  7 18:23:11 rev1-haproxy haproxy[28551]: 127.0.0.1:40514 [07/Aug/2025:18:23:11.741] haproxy_frontend haproxy_backend/rev1_dev48 0/0/1/9/10 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:56924 cpu_ns_avg:14231 lat_ns_tot:17275 lat_ns_avg:4318",
        "Aug  7 18:23:20 rev1-haproxy haproxy[28551]: 185.62.207.61:35710 [07/Aug/2025:18:23:20.193] haproxy_frontend haproxy_backend/rev1_dev85 0/0/0/5/6 200 207 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13601 cpu_ns_avg:3400 lat_ns_tot:5738 lat_ns_avg:1434",
        "Aug  7 18:23:22 rev1-haproxy haproxy[28551]: 185.62.207.61:26473 [07/Aug/2025:18:23:22.285] haproxy_frontend haproxy_backend/rev1_dev69 0/0/0/11/12 200 207 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14399 cpu_ns_avg:3599 lat_ns_tot:5331 lat_ns_avg:1332",
        "Aug  7 18:23:38 rev1-haproxy haproxy[28551]: 185.62.207.61:24779 [07/Aug/2025:18:23:38.841] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/3/3 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10533 cpu_ns_avg:2633 lat_ns_tot:3213 lat_ns_avg:803",
        "Aug  7 18:23:42 rev1-haproxy haproxy[28551]: 185.62.207.61:22051 [07/Aug/2025:18:23:42.989] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12476 cpu_ns_avg:3119 lat_ns_tot:7591 lat_ns_avg:1897",
        "Aug  7 18:23:47 rev1-haproxy haproxy[28551]: 185.62.207.61:14594 [07/Aug/2025:18:23:47.214] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/1/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:10126 cpu_ns_avg:2531 lat_ns_tot:4104 lat_ns_avg:1026"
    ]
}
2025-08-07 18:23:58,062 p=1274933 u=ubuntu n=ansible | PLAY [Test NGINX (snmp) proxy] ****************************************************************************************************************************************
2025-08-07 18:23:58,067 p=1274933 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 18:24:00,813 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 18:24:00,824 p=1274933 u=ubuntu n=ansible | TASK [Gather NGINX public IP address] *********************************************************************************************************************************
2025-08-07 18:24:02,818 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 18:24:02,824 p=1274933 u=ubuntu n=ansible | TASK [Send SNMP request to NGINX server and collect responses] ********************************************************************************************************
2025-08-07 18:24:04,582 p=1274933 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=0)
2025-08-07 18:24:06,301 p=1274933 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=1)
2025-08-07 18:24:08,081 p=1274933 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=2)
2025-08-07 18:24:08,088 p=1274933 u=ubuntu n=ansible | TASK [Display the NGINX response content] *****************************************************************************************************************************
2025-08-07 18:24:08,107 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=0) => {
    "ansible_loop_var": "item",
    "item": 0,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-dev48"
}
2025-08-07 18:24:08,111 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=1) => {
    "ansible_loop_var": "item",
    "item": 1,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-dev85"
}
2025-08-07 18:24:08,115 p=1274933 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=2) => {
    "ansible_loop_var": "item",
    "item": 2,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-dev69"
}
2025-08-07 18:24:08,134 p=1274933 u=ubuntu n=ansible | PLAY RECAP ************************************************************************************************************************************************************
2025-08-07 18:24:08,134 p=1274933 u=ubuntu n=ansible | rev1_HAproxy               : ok=45   changed=17   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1   
2025-08-07 18:24:08,134 p=1274933 u=ubuntu n=ansible | rev1_NGINX                 : ok=10   changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 18:24:08,134 p=1274933 u=ubuntu n=ansible | rev1_dev48                 : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 18:24:08,135 p=1274933 u=ubuntu n=ansible | rev1_dev69                 : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 18:24:08,135 p=1274933 u=ubuntu n=ansible | rev1_dev85                 : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 18:24:08,135 p=1274933 u=ubuntu n=ansible | rev1_devA                  : ok=14   changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 18:24:08,135 p=1274933 u=ubuntu n=ansible | rev1_devB                  : ok=14   changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 18:24:08,135 p=1274933 u=ubuntu n=ansible | rev1_devC                  : ok=14   changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 19:41:00,430 p=1301361 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] **************************************************************************************************************
2025-08-07 19:41:00,437 p=1301361 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 19:41:08,648 p=1301361 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-07 19:41:08,702 p=1301361 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-07 19:41:08,748 p=1301361 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-07 19:41:08,771 p=1301361 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-07 19:41:18,954 p=1301361 u=ubuntu n=ansible |  [ERROR]: User interrupted execution

2025-08-07 19:46:37,772 p=1305050 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] **************************************************************************************************************
2025-08-07 19:46:37,779 p=1305050 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 19:46:45,040 p=1305050 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-07 19:46:45,513 p=1305050 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-07 19:46:45,698 p=1305050 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-07 19:46:45,721 p=1305050 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-07 19:46:57,276 p=1305050 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3)
2025-08-07 19:46:58,449 p=1305050 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3)
2025-08-07 19:46:58,501 p=1305050 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3)
2025-08-07 19:47:06,665 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA] => (item=python3-pip)
2025-08-07 19:47:08,974 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB] => (item=python3-pip)
2025-08-07 19:47:09,066 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC] => (item=python3-pip)
2025-08-07 19:47:17,802 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmpd)
2025-08-07 19:47:21,499 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmpd)
2025-08-07 19:47:22,180 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmpd)
2025-08-07 19:47:26,137 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp)
2025-08-07 19:47:31,450 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp)
2025-08-07 19:47:31,572 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp)
2025-08-07 19:47:40,211 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp-mibs-downloader)
2025-08-07 19:47:46,696 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp-mibs-downloader)
2025-08-07 19:47:46,860 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp-mibs-downloader)
2025-08-07 19:47:46,866 p=1305050 u=ubuntu n=ansible | TASK [Install Flask] **************************************************************************************************************************************************
2025-08-07 19:47:53,326 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:47:53,414 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:47:53,990 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:47:53,996 p=1305050 u=ubuntu n=ansible | TASK [Deploy the Flask application config for TCP Load Balancing] *****************************************************************************************************
2025-08-07 19:48:00,544 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:00,821 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:01,111 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:01,119 p=1305050 u=ubuntu n=ansible | TASK [Start Flask app in background on port 5000] *********************************************************************************************************************
2025-08-07 19:48:04,830 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:04,978 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:05,095 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:05,100 p=1305050 u=ubuntu n=ansible | TASK [Check Flask app HTTP response on private IP] ********************************************************************************************************************
2025-08-07 19:48:08,499 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:08,824 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:08,952 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:08,958 p=1305050 u=ubuntu n=ansible | TASK [Display Flask app HTTP response on private IP] ******************************************************************************************************************
2025-08-07 19:48:08,994 p=1305050 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "19:48:07 10.1.1.55:55296 -- 10.1.1.55 (rev1-deva) 54"
}
2025-08-07 19:48:08,994 p=1305050 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "19:48:08 10.1.1.24:55180 -- 10.1.1.24 (rev1-devb) 79"
}
2025-08-07 19:48:09,005 p=1305050 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "19:48:08 10.1.1.9:44040 -- 10.1.1.9 (rev1-devc) 85"
}
2025-08-07 19:48:09,010 p=1305050 u=ubuntu n=ansible | TASK [Remove the existing agent address lines] ************************************************************************************************************************
2025-08-07 19:48:12,462 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:12,566 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:12,884 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:12,891 p=1305050 u=ubuntu n=ansible | TASK [Configure agent address (0.0.0.0) for SNMPd to listen on all UDP interfaces] ************************************************************************************
2025-08-07 19:48:16,248 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:16,497 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:16,660 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:16,665 p=1305050 u=ubuntu n=ansible | TASK [Check snmpd config File] ****************************************************************************************************************************************
2025-08-07 19:48:20,112 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:20,217 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:20,461 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:20,466 p=1305050 u=ubuntu n=ansible | TASK [Display snmpd configuration file] *******************************************************************************************************************************
2025-08-07 19:48:20,495 p=1305050 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003246",
        "end": "2025-08-07 19:48:19.564020",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 19:48:19.560774",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 19:48:20,497 p=1305050 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003757",
        "end": "2025-08-07 19:48:19.649234",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 19:48:19.645477",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 19:48:20,507 p=1305050 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003989",
        "end": "2025-08-07 19:48:19.849379",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 19:48:19.845390",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-07 19:48:20,512 p=1305050 u=ubuntu n=ansible | TASK [Restart SNMPD if agent address configuration is changed] ********************************************************************************************************
2025-08-07 19:48:24,526 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:24,534 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:24,732 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:24,738 p=1305050 u=ubuntu n=ansible | TASK [Test SNMPd with snmpget on 10.1.1.55] ***************************************************************************************************************************
2025-08-07 19:48:28,192 p=1305050 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-07 19:48:28,375 p=1305050 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-07 19:48:28,582 p=1305050 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-07 19:48:28,587 p=1305050 u=ubuntu n=ansible | TASK [Print SNMPd snmpget result] *************************************************************************************************************************************
2025-08-07 19:48:28,614 p=1305050 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-07 19:48:28,615 p=1305050 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-07 19:48:28,624 p=1305050 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-07 19:48:28,702 p=1305050 u=ubuntu n=ansible | PLAY [Set up HAProxy] *************************************************************************************************************************************************
2025-08-07 19:48:28,705 p=1305050 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 19:48:33,755 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:48:33,767 p=1305050 u=ubuntu n=ansible | TASK [Add HAProxy 2.9 PPA] ********************************************************************************************************************************************
2025-08-07 19:48:45,976 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:48:45,982 p=1305050 u=ubuntu n=ansible | TASK [Install HAProxy] ************************************************************************************************************************************************
2025-08-07 19:48:56,518 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:48:56,523 p=1305050 u=ubuntu n=ansible | TASK [Deploy stats web page password file] ****************************************************************************************************************************
2025-08-07 19:48:59,828 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:48:59,833 p=1305050 u=ubuntu n=ansible | TASK [Read stats page password from file] *****************************************************************************************************************************
2025-08-07 19:49:01,710 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:49:01,716 p=1305050 u=ubuntu n=ansible | TASK [Set up HAProxy stats secret variable] ***************************************************************************************************************************
2025-08-07 19:49:01,751 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:49:01,757 p=1305050 u=ubuntu n=ansible | TASK [Configure HAProxy] **********************************************************************************************************************************************
2025-08-07 19:49:05,107 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:05,112 p=1305050 u=ubuntu n=ansible | TASK [Deploy rsyslog 49-haproxy config file] **************************************************************************************************************************
2025-08-07 19:49:08,422 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:08,428 p=1305050 u=ubuntu n=ansible | TASK [Return 49_haproxy_conf to registered rsyslog_49_haproxy_conf] ***************************************************************************************************
2025-08-07 19:49:10,198 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:10,203 p=1305050 u=ubuntu n=ansible | TASK [debug] **********************************************************************************************************************************************************
2025-08-07 19:49:10,218 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "rsyslog_49_haproxy_conf.stdout_lines": [
        "# Create an additional socket in haproxy's chroot in order to allow logging via",
        "# /dev/log to chroot'ed HAProxy processes",
        "$AddUnixListenSocket /var/lib/haproxy/dev/log",
        "",
        "# Send HAProxy messages to a dedicated logfile",
        ":programname, startswith, \"haproxy\" {",
        "  /var/log/haproxy.log",
        "stop",
        "}"
    ]
}
2025-08-07 19:49:10,223 p=1305050 u=ubuntu n=ansible | TASK [Test HAProxy Configurations] ************************************************************************************************************************************
2025-08-07 19:49:12,019 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:12,024 p=1305050 u=ubuntu n=ansible | TASK [Display HAProxy config test result] *****************************************************************************************************************************
2025-08-07 19:49:12,041 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_configs_test_result": {
        "changed": true,
        "cmd": [
            "haproxy",
            "-f",
            "/etc/haproxy/haproxy.cfg",
            "-c"
        ],
        "delta": "0:00:00.020206",
        "end": "2025-08-07 19:49:11.751236",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-07 19:49:11.731030",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "",
        "stdout_lines": []
    }
}
2025-08-07 19:49:12,046 p=1305050 u=ubuntu n=ansible | TASK [Test HAProxy is running] ****************************************************************************************************************************************
2025-08-07 19:49:13,962 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:49:13,969 p=1305050 u=ubuntu n=ansible | TASK [Display the HAProxy service status] *****************************************************************************************************************************
2025-08-07 19:49:13,984 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_service_status.state": "started"
}
2025-08-07 19:49:13,989 p=1305050 u=ubuntu n=ansible | TASK [Check HAProxy server status] ************************************************************************************************************************************
2025-08-07 19:49:15,779 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:15,784 p=1305050 u=ubuntu n=ansible | TASK [Display HAProxy server status] **********************************************************************************************************************************
2025-08-07 19:49:15,799 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "systemctl_haproxy_service_status.stdout_lines": [
        "● haproxy.service - HAProxy Load Balancer",
        "     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Thu 2025-08-07 19:48:53 UTC; 22s ago",
        "       Docs: man:haproxy(1)",
        "             file:/usr/share/doc/haproxy/configuration.txt.gz",
        "   Main PID: 4462 (haproxy)",
        "     Status: \"Ready.\"",
        "      Tasks: 2 (limit: 4588)",
        "     Memory: 39.8M",
        "     CGroup: /system.slice/haproxy.service",
        "             ├─4462 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "             └─4482 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "",
        "Aug 07 19:48:53 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 07 19:48:53 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : New worker (4482) forked",
        "Aug 07 19:48:53 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 07 19:48:53 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : Loading success."
    ]
}
2025-08-07 19:49:15,804 p=1305050 u=ubuntu n=ansible | TASK [Check HAProxy config errors via journalctl] *********************************************************************************************************************
2025-08-07 19:49:17,559 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:17,565 p=1305050 u=ubuntu n=ansible | TASK [Display HAProxy config errors] **********************************************************************************************************************************
2025-08-07 19:49:17,581 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_journalctl_logs.stdout_lines": [
        "-- Logs begin at Thu 2025-08-07 19:44:52 UTC, end at Thu 2025-08-07 19:49:17 UTC. --",
        "Aug 07 19:48:53 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 07 19:48:53 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : New worker (4482) forked",
        "Aug 07 19:48:53 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 07 19:48:53 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : Loading success."
    ]
}
2025-08-07 19:49:17,586 p=1305050 u=ubuntu n=ansible | TASK [Check the HAProxy configuration file] ***************************************************************************************************************************
2025-08-07 19:49:19,336 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:19,341 p=1305050 u=ubuntu n=ansible | TASK [Display HAProxy configuration file] *****************************************************************************************************************************
2025-08-07 19:49:19,357 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_config_file.stdout_lines": [
        "global",
        "    profiling.tasks on #Enable HAProxy profiling (CPU time spent on processing a http request inside HAProxy)",
        "    nbthread 1 # 1 thread, IDs from 1 to 2, nbthread <number of CPU cores>",
        "    thread-groups 1",
        "    # declare threads",
        "    thread-group 1 1-1",
        "    # bind threads to cpu cores",
        "    cpu-map 1/all 0-0 # bind all threads to CPU 0 #syntax:cpu-map 1/1-<Number Of CPU Cores> 0-<Number of CPU Cores - 1>",
        "    # define logging",
        "    log /dev/log local0 info",
        "    #log /dev/log local0 emerg",
        "    #log /dev/log local1 alert",
        "    #log /dev/log local2 crit",
        "    #log /dev/log local3 err",
        "    #log /dev/log local4 warning",
        "    #log /dev/log local5 notice",
        "    #log /dev/log local6 info",
        "    #log /dev/log local7 debug",
        "    #Security Considerations",
        "    chroot /var/lib/haproxy #chroot statement pointing to a /var/lib/haproxy location",
        "    user haproxy # uid/user statement",
        "    group haproxy # gid/group statement",
        "    stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin",
        "    stats maxconn 20",
        "    stats timeout 30000",
        "    daemon",
        "    #maxconn 512",
        "        ",
        "defaults",
        "    mode http",
        "    timeout connect 5000ms",
        "    timeout client 5000ms",
        "    timeout server 5000ms",
        "    errorfile 400 /etc/haproxy/errors/400.http",
        "    errorfile 403 /etc/haproxy/errors/403.http",
        "    errorfile 408 /etc/haproxy/errors/408.http",
        "    errorfile 500 /etc/haproxy/errors/500.http",
        "    errorfile 502 /etc/haproxy/errors/502.http",
        "    errorfile 503 /etc/haproxy/errors/503.http",
        "    errorfile 504 /etc/haproxy/errors/504.http",
        "",
        "frontend web_stats",
        "    mode http",
        "    bind *:80 ",
        "    http-request use-service prometheus-exporter if { path /metrics }",
        "    stats enable # enable stats page",
        "    stats uri /stats # stats uri",
        "    stats hide-version",
        "    stats refresh 1s",
        "    stats auth admin:uipassword",
        "",
        "frontend haproxy_frontend",
        "    log global",
        "    bind *:80  thread 1/all shards by-thread  #bind this proxy to threads 1 to 1 or all",
        "    mode http",
        "    option httplog",
        "    #option dontlog-normal",
        "    #option logasap",
        "    #define custom log-format",
        "    log-format \"%ci:%cp [%tr] %ft %b/%s %TR/%Tw/%Tc/%Tr/%Ta %ST %B %CC %CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r %[http_first_req] cpu_calls:%[cpu_calls] cpu_ns_tot:%[cpu_ns_tot] cpu_ns_avg:%[cpu_ns_avg] lat_ns_tot:%[lat_ns_tot] lat_ns_avg:%[lat_ns_avg]\"",
        "    default_backend haproxy_backend",
        "    ",
        "backend haproxy_backend",
        "    retry-on all-retryable-errors # This works when conn-failure, empty-response, junk-response, response-timeout, rtt-rejected, 500, 502, 503, and 504",
        "    retries 3",
        "             server rev1_devA 10.1.1.55:5000 check #maxconn 64",
        "             server rev1_devB 10.1.1.24:5000 check #maxconn 64",
        "             server rev1_devC 10.1.1.9:5000 check #maxconn 64",
        "    "
    ]
}
2025-08-07 19:49:19,369 p=1305050 u=ubuntu n=ansible | RUNNING HANDLER [Restart HAProxy service] *****************************************************************************************************************************
2025-08-07 19:49:21,585 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:21,590 p=1305050 u=ubuntu n=ansible | RUNNING HANDLER [Restart rsyslog service] *****************************************************************************************************************************
2025-08-07 19:49:23,609 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:23,620 p=1305050 u=ubuntu n=ansible | PLAY [Install the Grafana Alloy Agent on HAproxy] *********************************************************************************************************************
2025-08-07 19:49:23,625 p=1305050 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 19:49:26,146 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:49:26,157 p=1305050 u=ubuntu n=ansible | TASK [Install the Grafana Alloy Agent] ********************************************************************************************************************************
2025-08-07 19:49:36,967 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:36,972 p=1305050 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-07 19:49:38,830 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:38,835 p=1305050 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-07 19:49:38,850 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Thu 2025-08-07 19:49:36 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 11649 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 38.2M",
        "     CGroup: /system.slice/alloy.service",
        "             └─11649 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.00951804Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=livedebugging duration=13.821µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009529108Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=ui duration=2.092µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009549887Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 duration=214.875934ms",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009868357Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.010954241Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.011733715Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.012006185Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.020874924Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=60130dba890803b7c579c2ad002bb4fd",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.0209121Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=60130dba890803b7c579c2ad002bb4fd duration=106.128µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.033987008Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 19:49:38,855 p=1305050 u=ubuntu n=ansible | TASK [DeployAlloy config file] ****************************************************************************************************************************************
2025-08-07 19:49:42,235 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:42,240 p=1305050 u=ubuntu n=ansible | TASK [Restart the Grafana alloy service] ******************************************************************************************************************************
2025-08-07 19:49:44,189 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:44,194 p=1305050 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *************************************************************************************************************************
2025-08-07 19:49:46,011 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:46,018 p=1305050 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] *******************************************************************************************************************************
2025-08-07 19:49:46,035 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Thu 2025-08-07 19:49:43 UTC; 1s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 13398 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 36.2M",
        "     CGroup: /system.slice/alloy.service",
        "             └─13398 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.21475672Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=82affc",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.218533401Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=4.721341ms",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.218657361Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 duration=77.107362ms",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.219174201Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.220834618Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.221354608Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.221881796Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.230644298Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=c9b15f20e2249edaf27697d9ece79cbd",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.230680861Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=c9b15f20e2249edaf27697d9ece79cbd duration=107.493µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.24380747Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 19:49:46,040 p=1305050 u=ubuntu n=ansible | TASK [Check the Grafana alloy logs] ***********************************************************************************************************************************
2025-08-07 19:49:47,855 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:47,861 p=1305050 u=ubuntu n=ansible | TASK [Display the Grafana alloy logs] *********************************************************************************************************************************
2025-08-07 19:49:47,878 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_logs_response.stdout_lines": [
        "-- Logs begin at Thu 2025-08-07 19:44:52 UTC, end at Thu 2025-08-07 19:49:47 UTC. --",
        "Aug 07 19:49:36 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.008758587Z level=info \"boringcrypto enabled\"=false",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:36.79406354Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009323735Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009342283Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009359606Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=loki.write.grafana_cloud_loki duration=4.578396ms",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009371584Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009392185Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=0",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009398981Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=82affc",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009407262Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009412614Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=prometheus.remote_write.metrics_service duration=9.246362ms",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009423425Z level=info msg=\"running usage stats reporter\"",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009427255Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=82affc",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009433229Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=remotecfg duration=200.071636ms",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.00943915Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009443435Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=http duration=41.14µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009449112Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=cluster duration=2.286µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009454439Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=otel duration=1.963µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009472913Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=labelstore duration=13.368µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009480897Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=tracing duration=8.202µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009491624Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=logging duration=736.646µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.00951804Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=livedebugging duration=13.821µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009529108Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 node_id=ui duration=2.092µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009549887Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=30af439c6a224c0fe521fdada22b07a5 duration=214.875934ms",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.009868357Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.010954241Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.011733715Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.012006185Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.020874924Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=60130dba890803b7c579c2ad002bb4fd",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.0209121Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=60130dba890803b7c579c2ad002bb4fd duration=106.128µs",
        "Aug 07 19:49:37 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:37.033987008Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: interrupt received",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.787041113Z level=info msg=\"http server closed\" service=http addr=127.0.0.1:12345 err=\"http: Server closed\"",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.78715257Z level=error msg=\"failed to start reporter\" err=\"context canceled\"",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.787179129Z level=info msg=\"node exited without error\" node=otel",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.787211546Z level=info msg=\"node exited without error\" node=labelstore",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.787226313Z level=info msg=\"node exited without error\" node=remotecfg",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.787262076Z level=info msg=\"node exited without error\" node=livedebugging",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.78730412Z level=info msg=\"node exited without error\" node=loki.write.grafana_cloud_loki",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.787505242Z level=info msg=\"node exited without error\" node=ui",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788466625Z level=info msg=\"Stopping remote storage...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.78852577Z level=info msg=\"http server closed\" service=http addr=memory err=\"http: Server closed\"",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788557985Z level=info msg=\"node exited without error\" node=http",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788588883Z level=info msg=\"node exited without error\" node=cluster",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788646249Z level=info msg=\"WAL watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=82affc",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788656653Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788672597Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788913365Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 19:49:43 rev1-haproxy alloy[11649]: ts=2025-08-07T19:49:43.788922802Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 07 19:49:43 rev1-haproxy systemd[1]: Stopping Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines...",
        "Aug 07 19:49:43 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 07 19:49:43 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 19:49:43 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.206679883Z level=info \"boringcrypto enabled\"=false",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.140828047Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.211048964Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.211141712Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.21122616Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=tracing duration=8.574µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.211304161Z level=info msg=\"running usage stats reporter\"",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.211786447Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=remotecfg duration=50.074693ms",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.211905092Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.21200034Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=http duration=13.946µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212080502Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=cluster duration=833ns",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212207085Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=labelstore duration=3.07µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212287499Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=loki.write.grafana_cloud_loki duration=643.311µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212366626Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212444918Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=1",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212519451Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=1",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212592407Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=82affc",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212668509Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212742121Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=prometheus.remote_write.metrics_service duration=14.172028ms",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212827383Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=logging duration=6.154138ms",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.212922119Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=otel duration=8.284µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.21302008Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=livedebugging duration=19.547µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.213104902Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=ui duration=3.568µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.213287428Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=101.381µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.21475672Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=82affc url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=82affc",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.218533401Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=4.721341ms",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.218657361Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=ac3c658f2c34d43bea63aeaa52275885 duration=77.107362ms",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.219174201Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.220834618Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.221354608Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.221881796Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.230644298Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=c9b15f20e2249edaf27697d9ece79cbd",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.230680861Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=c9b15f20e2249edaf27697d9ece79cbd duration=107.493µs",
        "Aug 07 19:49:44 rev1-haproxy alloy[13398]: ts=2025-08-07T19:49:44.24380747Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-07 19:49:47,883 p=1305050 u=ubuntu n=ansible | TASK [Check the Grafana alloy configuration file] *********************************************************************************************************************
2025-08-07 19:49:49,690 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:49:49,695 p=1305050 u=ubuntu n=ansible | TASK [Display the Grafana alloy config] *******************************************************************************************************************************
2025-08-07 19:49:49,710 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_config_response.stdout_lines": [
        "remotecfg {",
        "  url            = \"https://fleet-management-prod-016.grafana.net\"",
        "  id             = \"rev1-haproxy\"",
        "  poll_frequency = \"60s\"",
        "",
        "  basic_auth {",
        "    username = \"1303247\"",
        "    password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "  }",
        "}",
        "",
        "prometheus.remote_write \"metrics_service\" {",
        "  endpoint {",
        "    url = \"https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push\"",
        "    basic_auth {",
        "      username = \"2530729\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "loki.write \"grafana_cloud_loki\" {",
        "  endpoint {",
        "    url = \"https://logs-prod-025.grafana.net/loki/api/v1/push\"",
        "    basic_auth {",
        "      username = \"1261041\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "discovery.relabel \"metrics_integrations_integrations_haproxy\" {",
        "  targets = [{",
        "    __address__ = \"127.0.0.1:80\",",
        "  }]",
        "",
        "  rule {",
        "    target_label = \"instance\"",
        "    replacement  = constants.hostname",
        "  }",
        "}",
        "",
        "prometheus.scrape \"metrics_integrations_integrations_haproxy\" {",
        "  targets    = discovery.relabel.metrics_integrations_integrations_haproxy.output",
        "  forward_to = [prometheus.remote_write.metrics_service.receiver]",
        "  job_name   = \"integrations/haproxy\"",
        "}"
    ]
}
2025-08-07 19:49:49,726 p=1305050 u=ubuntu n=ansible | [WARNING]: Found variable using reserved name: timeout

2025-08-07 19:49:49,727 p=1305050 u=ubuntu n=ansible | PLAY [Install snmp, snmpd, NGINX UDP load balancer config for SNMP] ***************************************************************************************************
2025-08-07 19:49:49,729 p=1305050 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 19:49:55,800 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 19:49:55,813 p=1305050 u=ubuntu n=ansible | TASK [Install required packages] **************************************************************************************************************************************
2025-08-07 19:50:16,097 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=nginx)
2025-08-07 19:50:27,753 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmpd)
2025-08-07 19:50:36,503 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp)
2025-08-07 19:50:50,781 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp-mibs-downloader)
2025-08-07 19:50:50,789 p=1305050 u=ubuntu n=ansible | TASK [Deploy NGINX stream config for SNMP UDP load balancing] *********************************************************************************************************
2025-08-07 19:50:54,840 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-07 19:50:54,845 p=1305050 u=ubuntu n=ansible | TASK [Check nginx is running] *****************************************************************************************************************************************
2025-08-07 19:50:57,063 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 19:50:57,069 p=1305050 u=ubuntu n=ansible | TASK [display nginx status] *******************************************************************************************************************************************
2025-08-07 19:50:57,091 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX] => {
    "nginx_running_status": {
        "changed": false,
        "failed": false,
        "name": "nginx",
        "state": "started",
        "status": {
            "ActiveEnterTimestamp": "Thu 2025-08-07 19:50:12 UTC",
            "ActiveEnterTimestampMonotonic": "319823679",
            "ActiveExitTimestampMonotonic": "0",
            "ActiveState": "active",
            "After": "basic.target sysinit.target systemd-journald.socket system.slice network.target",
            "AllowIsolate": "no",
            "AllowedCPUs": "",
            "AllowedMemoryNodes": "",
            "AmbientCapabilities": "",
            "AssertResult": "yes",
            "AssertTimestamp": "Thu 2025-08-07 19:50:12 UTC",
            "AssertTimestampMonotonic": "319762186",
            "Before": "shutdown.target multi-user.target",
            "BlockIOAccounting": "no",
            "BlockIOWeight": "[not set]",
            "CPUAccounting": "no",
            "CPUAffinity": "",
            "CPUAffinityFromNUMA": "no",
            "CPUQuotaPerSecUSec": "infinity",
            "CPUQuotaPeriodUSec": "infinity",
            "CPUSchedulingPolicy": "0",
            "CPUSchedulingPriority": "0",
            "CPUSchedulingResetOnFork": "no",
            "CPUShares": "[not set]",
            "CPUUsageNSec": "[not set]",
            "CPUWeight": "[not set]",
            "CacheDirectoryMode": "0755",
            "CanIsolate": "no",
            "CanReload": "yes",
            "CanStart": "yes",
            "CanStop": "yes",
            "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read",
            "CleanResult": "success",
            "CollectMode": "inactive",
            "ConditionResult": "yes",
            "ConditionTimestamp": "Thu 2025-08-07 19:50:12 UTC",
            "ConditionTimestampMonotonic": "319762186",
            "ConfigurationDirectoryMode": "0755",
            "Conflicts": "shutdown.target",
            "ControlGroup": "/system.slice/nginx.service",
            "ControlPID": "0",
            "DefaultDependencies": "yes",
            "DefaultMemoryLow": "0",
            "DefaultMemoryMin": "0",
            "Delegate": "no",
            "Description": "A high performance web server and a reverse proxy server",
            "DevicePolicy": "auto",
            "Documentation": "man:nginx(8)",
            "DynamicUser": "no",
            "EffectiveCPUs": "",
            "EffectiveMemoryNodes": "",
            "ExecMainCode": "0",
            "ExecMainExitTimestampMonotonic": "0",
            "ExecMainPID": "3416",
            "ExecMainStartTimestamp": "Thu 2025-08-07 19:50:12 UTC",
            "ExecMainStartTimestampMonotonic": "319823656",
            "ExecMainStatus": "0",
            "ExecReload": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecReloadEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStart": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPre": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPreEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStop": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStopEx": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "FailureAction": "none",
            "FileDescriptorStoreMax": "0",
            "FinalKillSignal": "9",
            "FragmentPath": "/lib/systemd/system/nginx.service",
            "GID": "[not set]",
            "GuessMainPID": "yes",
            "IOAccounting": "no",
            "IOReadBytes": "18446744073709551615",
            "IOReadOperations": "18446744073709551615",
            "IOSchedulingClass": "0",
            "IOSchedulingPriority": "0",
            "IOWeight": "[not set]",
            "IOWriteBytes": "18446744073709551615",
            "IOWriteOperations": "18446744073709551615",
            "IPAccounting": "no",
            "IPEgressBytes": "[no data]",
            "IPEgressPackets": "[no data]",
            "IPIngressBytes": "[no data]",
            "IPIngressPackets": "[no data]",
            "Id": "nginx.service",
            "IgnoreOnIsolate": "no",
            "IgnoreSIGPIPE": "yes",
            "InactiveEnterTimestampMonotonic": "0",
            "InactiveExitTimestamp": "Thu 2025-08-07 19:50:12 UTC",
            "InactiveExitTimestampMonotonic": "319763816",
            "InvocationID": "668b2da58aff455d8646fa5d7d009444",
            "JobRunningTimeoutUSec": "infinity",
            "JobTimeoutAction": "none",
            "JobTimeoutUSec": "infinity",
            "KeyringMode": "private",
            "KillMode": "mixed",
            "KillSignal": "15",
            "LimitAS": "infinity",
            "LimitASSoft": "infinity",
            "LimitCORE": "infinity",
            "LimitCORESoft": "0",
            "LimitCPU": "infinity",
            "LimitCPUSoft": "infinity",
            "LimitDATA": "infinity",
            "LimitDATASoft": "infinity",
            "LimitFSIZE": "infinity",
            "LimitFSIZESoft": "infinity",
            "LimitLOCKS": "infinity",
            "LimitLOCKSSoft": "infinity",
            "LimitMEMLOCK": "65536",
            "LimitMEMLOCKSoft": "65536",
            "LimitMSGQUEUE": "819200",
            "LimitMSGQUEUESoft": "819200",
            "LimitNICE": "0",
            "LimitNICESoft": "0",
            "LimitNOFILE": "524288",
            "LimitNOFILESoft": "1024",
            "LimitNPROC": "15295",
            "LimitNPROCSoft": "15295",
            "LimitRSS": "infinity",
            "LimitRSSSoft": "infinity",
            "LimitRTPRIO": "0",
            "LimitRTPRIOSoft": "0",
            "LimitRTTIME": "infinity",
            "LimitRTTIMESoft": "infinity",
            "LimitSIGPENDING": "15295",
            "LimitSIGPENDINGSoft": "15295",
            "LimitSTACK": "infinity",
            "LimitSTACKSoft": "8388608",
            "LoadState": "loaded",
            "LockPersonality": "no",
            "LogLevelMax": "-1",
            "LogRateLimitBurst": "0",
            "LogRateLimitIntervalUSec": "0",
            "LogsDirectoryMode": "0755",
            "MainPID": "3416",
            "MemoryAccounting": "yes",
            "MemoryCurrent": "5447680",
            "MemoryDenyWriteExecute": "no",
            "MemoryHigh": "infinity",
            "MemoryLimit": "infinity",
            "MemoryLow": "0",
            "MemoryMax": "infinity",
            "MemoryMin": "0",
            "MemorySwapMax": "infinity",
            "MountAPIVFS": "no",
            "MountFlags": "",
            "NFileDescriptorStore": "0",
            "NRestarts": "0",
            "NUMAMask": "",
            "NUMAPolicy": "n/a",
            "Names": "nginx.service",
            "NeedDaemonReload": "no",
            "Nice": "0",
            "NoNewPrivileges": "no",
            "NonBlocking": "no",
            "NotifyAccess": "none",
            "OOMPolicy": "stop",
            "OOMScoreAdjust": "0",
            "OnFailureJobMode": "replace",
            "PIDFile": "/run/nginx.pid",
            "Perpetual": "no",
            "PrivateDevices": "no",
            "PrivateMounts": "no",
            "PrivateNetwork": "no",
            "PrivateTmp": "no",
            "PrivateUsers": "no",
            "ProtectControlGroups": "no",
            "ProtectHome": "no",
            "ProtectHostname": "no",
            "ProtectKernelLogs": "no",
            "ProtectKernelModules": "no",
            "ProtectKernelTunables": "no",
            "ProtectSystem": "no",
            "RefuseManualStart": "no",
            "RefuseManualStop": "no",
            "ReloadResult": "success",
            "RemainAfterExit": "no",
            "RemoveIPC": "no",
            "Requires": "sysinit.target system.slice",
            "Restart": "no",
            "RestartKillSignal": "15",
            "RestartUSec": "100ms",
            "RestrictNamespaces": "no",
            "RestrictRealtime": "no",
            "RestrictSUIDSGID": "no",
            "Result": "success",
            "RootDirectoryStartOnly": "no",
            "RuntimeDirectoryMode": "0755",
            "RuntimeDirectoryPreserve": "no",
            "RuntimeMaxUSec": "infinity",
            "SameProcessGroup": "no",
            "SecureBits": "0",
            "SendSIGHUP": "no",
            "SendSIGKILL": "yes",
            "Slice": "system.slice",
            "StandardError": "inherit",
            "StandardInput": "null",
            "StandardInputData": "",
            "StandardOutput": "journal",
            "StartLimitAction": "none",
            "StartLimitBurst": "5",
            "StartLimitIntervalUSec": "10s",
            "StartupBlockIOWeight": "[not set]",
            "StartupCPUShares": "[not set]",
            "StartupCPUWeight": "[not set]",
            "StartupIOWeight": "[not set]",
            "StateChangeTimestamp": "Thu 2025-08-07 19:50:12 UTC",
            "StateChangeTimestampMonotonic": "319823679",
            "StateDirectoryMode": "0755",
            "StatusErrno": "0",
            "StopWhenUnneeded": "no",
            "SubState": "running",
            "SuccessAction": "none",
            "SyslogFacility": "3",
            "SyslogLevel": "6",
            "SyslogLevelPrefix": "yes",
            "SyslogPriority": "30",
            "SystemCallErrorNumber": "0",
            "TTYReset": "no",
            "TTYVHangup": "no",
            "TTYVTDisallocate": "no",
            "TasksAccounting": "yes",
            "TasksCurrent": "2",
            "TasksMax": "4588",
            "TimeoutAbortUSec": "5s",
            "TimeoutCleanUSec": "infinity",
            "TimeoutStartUSec": "1min 30s",
            "TimeoutStopUSec": "5s",
            "TimerSlackNSec": "50000",
            "Transient": "no",
            "Type": "forking",
            "UID": "[not set]",
            "UMask": "0022",
            "UnitFilePreset": "enabled",
            "UnitFileState": "enabled",
            "UtmpMode": "init",
            "WantedBy": "multi-user.target",
            "WatchdogSignal": "6",
            "WatchdogTimestampMonotonic": "0",
            "WatchdogUSec": "0"
        }
    }
}
2025-08-07 19:50:57,103 p=1305050 u=ubuntu n=ansible | RUNNING HANDLER [Reload NGINX] ****************************************************************************************************************************************
2025-08-07 19:50:59,411 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-07 19:50:59,420 p=1305050 u=ubuntu n=ansible | PLAY [Test HAProxy (http), HAProxy Web stats (STATS)+ Metrics (PROMEX) and HAProxy logs] ******************************************************************************
2025-08-07 19:50:59,427 p=1305050 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 19:51:02,207 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:51:02,219 p=1305050 u=ubuntu n=ansible | TASK [Gather HAProxy server public IP address] ************************************************************************************************************************
2025-08-07 19:51:04,392 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:51:04,397 p=1305050 u=ubuntu n=ansible | TASK [Send HTTP request to HAProxy and collect response] **************************************************************************************************************
2025-08-07 19:51:18,037 p=1305050 u=ubuntu n=ansible | failed: [rev1_HAproxy] (item=0) => {"ansible_loop_var": "item", "attempts": 3, "cache_control": "no-cache", "changed": false, "content": "<html><body><h1>503 Service Unavailable</h1>\nNo server is available to handle this request.\n</body></html>\n\n", "content_type": "text/html", "elapsed": 0, "item": 0, "msg": "Status code was 503 and not [200]: HTTP Error 503: Service Unavailable", "redirected": false, "status": 503, "url": "http://185.62.207.61"}
2025-08-07 19:51:19,883 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1)
2025-08-07 19:51:21,734 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2)
2025-08-07 19:51:21,735 p=1305050 u=ubuntu n=ansible | ...ignoring
2025-08-07 19:51:21,740 p=1305050 u=ubuntu n=ansible | TASK [Display the HAProxy response] ***********************************************************************************************************************************
2025-08-07 19:51:21,759 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "<html><body><h1>503 Service Unavailable</h1>\nNo server is available to handle this request.\n</body></html>\n\n",
    "item": 0
}
2025-08-07 19:51:21,763 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "19:51:19 10.1.1.17:59386 -- 10.1.1.24 (rev1-devb) 35\n",
    "item": 1
}
2025-08-07 19:51:21,768 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "19:51:21 10.1.1.17:52292 -- 10.1.1.9 (rev1-devc) 49\n",
    "item": 2
}
2025-08-07 19:51:21,774 p=1305050 u=ubuntu n=ansible | TASK [Send HTTP requests to HAProxy stats page and collect responses] *************************************************************************************************
2025-08-07 19:51:23,649 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-07 19:51:23,655 p=1305050 u=ubuntu n=ansible | TASK [Display the stats response content] *****************************************************************************************************************************
2025-08-07 19:51:23,670 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_stats_response.content": "# pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,agent_status,agent_code,agent_duration,check_desc,agent_desc,check_rise,check_fall,check_health,agent_rise,agent_fall,agent_health,addr,cookie,mode,algo,conn_rate,conn_rate_max,conn_tot,intercepted,dcon,dses,wrew,connect,reuse,cache_lookups,cache_hits,srv_icur,src_ilim,qtime_max,ctime_max,rtime_max,ttime_max,eint,idle_conn_cur,safe_conn_cur,used_conn_cur,need_conn_est,uweight,agg_server_status,agg_server_check_status,agg_check_status,srid,sess_other,h1sess,h2sess,h3sess,req_other,h1req,h2req,h3req,proto,-,ssl_sess,ssl_reused_sess,ssl_failed_handshake,quic_rxbuf_full,quic_dropped_pkt,quic_dropped_pkt_bufoverrun,quic_dropped_parsing_pkt,quic_socket_full,quic_sendto_err,quic_sendto_err_unknwn,quic_sent_pkt,quic_lost_pkt,quic_too_short_dgram,quic_retry_sent,quic_retry_validated,quic_retry_error,quic_half_open_conn,quic_hdshk_fail,quic_stless_rst_sent,quic_conn_migration_done,quic_transp_err_no_error,quic_transp_err_internal_error,quic_transp_err_connection_refused,quic_transp_err_flow_control_error,quic_transp_err_stream_limit_error,quic_transp_err_stream_state_error,quic_transp_err_final_size_error,quic_transp_err_frame_encoding_error,quic_transp_err_transport_parameter_error,quic_transp_err_connection_id_limit,quic_transp_err_protocol_violation_error,quic_transp_err_invalid_token,quic_transp_err_application_error,quic_transp_err_crypto_buffer_exceeded,quic_transp_err_key_update_error,quic_transp_err_aead_limit_reached,quic_transp_err_no_viable_path,quic_transp_err_crypto_error,quic_transp_err_unknown_error,quic_data_blocked,quic_stream_data_blocked,quic_streams_blocked_bidi,quic_streams_blocked_uni,h3_data,h3_headers,h3_cancel_push,h3_push_promise,h3_max_push_id,h3_goaway,h3_settings,h3_no_error,h3_general_protocol_error,h3_internal_error,h3_stream_creation_error,h3_closed_critical_stream,h3_frame_unexpected,h3_frame_error,h3_excessive_load,h3_id_error,h3_settings_error,h3_missing_settings,h3_request_rejected,h3_request_cancelled,h3_request_incomplete,h3_message_error,h3_connect_error,h3_version_fallback,pack_decompression_failed,qpack_encoder_stream_error,qpack_decoder_stream_error,h2_headers_rcvd,h2_data_rcvd,h2_settings_rcvd,h2_rst_stream_rcvd,h2_goaway_rcvd,h2_detected_conn_protocol_errors,h2_detected_strm_protocol_errors,h2_rst_stream_resp,h2_goaway_resp,h2_open_connections,h2_backend_open_streams,h2_total_connections,h2_backend_total_streams,h1_open_connections,h1_open_streams,h1_total_connections,h1_total_streams,h1_bytes_in,h1_bytes_out,h1_spliced_bytes_in,h1_spliced_bytes_out,\nweb_stats,FRONTEND,,,1,1,262113,6,684,62893,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,1,0,1,,,,0,1,0,0,4,0,,1,1,6,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,1,1,6,2,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,6,0,0,0,6,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,6,6,923,62839,0,0,\nhaproxy_frontend,FRONTEND,,,0,1,262113,3,488,779,0,0,0,,,,,OPEN,,,,,,,,,1,3,0,,,,0,0,0,1,,,,0,2,0,1,0,0,,0,1,3,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,3,0,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,3,0,0,0,3,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,525,1132,0,0,\nhaproxy_backend,rev1_devA,0,0,0,1,,1,292,368,,0,,0,0,0,0,UP,1,1,0,0,0,122,0,,1,4,1,,1,,2,0,,1,L4OK,,0,0,0,0,1,0,0,,,,1,0,0,,,,,12,,,0,0,2,2,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,2,2,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devB,0,0,0,1,,1,98,206,,0,,0,0,0,0,UP,1,1,0,0,0,122,0,,1,4,2,,1,,2,0,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,4,,,0,0,4,4,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,4,4,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devC,0,0,0,1,,1,98,205,,0,,0,0,0,0,UP,1,1,0,0,0,122,0,,1,4,3,,1,,2,1,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,2,,,0,0,4,4,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,4,4,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,BACKEND,0,0,0,1,26212,3,488,779,0,0,,0,0,0,0,UP,3,3,0,,0,122,0,,1,4,0,,3,,1,0,,1,,,,0,2,0,1,0,0,,,,3,0,0,0,0,0,0,2,,,0,0,4,4,,,,,,,,,,,,,,http,,,,,,,,0,3,0,0,0,,,0,0,4,4,0,,,,,3,0,0,0,,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,1151,487,0,0,\n"
}
2025-08-07 19:51:23,675 p=1305050 u=ubuntu n=ansible | TASK [Test the HAProxy metrics (promex) path] *************************************************************************************************************************
2025-08-07 19:51:25,475 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:51:25,481 p=1305050 u=ubuntu n=ansible | TASK [Display the HAProxy metrics (promex) response content] **********************************************************************************************************
2025-08-07 19:51:25,516 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_metrics_path_result.stdout_lines": [
        "# HELP haproxy_process_nbthread Number of started threads (global.nbthread)",
        "# TYPE haproxy_process_nbthread gauge",
        "haproxy_process_nbthread 1",
        "# HELP haproxy_process_nbproc Number of started worker processes (historical, always 1)",
        "# TYPE haproxy_process_nbproc gauge",
        "haproxy_process_nbproc 1",
        "# HELP haproxy_process_relative_process_id Relative worker process number (1)",
        "# TYPE haproxy_process_relative_process_id gauge",
        "haproxy_process_relative_process_id 1",
        "# HELP haproxy_process_uptime_seconds How long ago this worker process was started (seconds)",
        "# TYPE haproxy_process_uptime_seconds gauge",
        "haproxy_process_uptime_seconds 123",
        "# HELP haproxy_process_pool_failures_total Number of failed pool allocations since this worker was started",
        "# TYPE haproxy_process_pool_failures_total counter",
        "haproxy_process_pool_failures_total 0",
        "# HELP haproxy_process_max_fds Hard limit on the number of per-process file descriptors",
        "# TYPE haproxy_process_max_fds gauge",
        "haproxy_process_max_fds 524288",
        "# HELP haproxy_process_max_sockets Hard limit on the number of per-process sockets",
        "# TYPE haproxy_process_max_sockets gauge",
        "haproxy_process_max_sockets 524288",
        "# HELP haproxy_process_max_connections Hard limit on the number of per-process connections (configured or imposed by Ulimit-n)",
        "# TYPE haproxy_process_max_connections gauge",
        "haproxy_process_max_connections 262113",
        "# HELP haproxy_process_hard_max_connections Hard limit on the number of per-process connections (imposed by Memmax_MB or Ulimit-n)",
        "# TYPE haproxy_process_hard_max_connections gauge",
        "haproxy_process_hard_max_connections 262113",
        "# HELP haproxy_process_current_connections Current number of connections on this worker process",
        "# TYPE haproxy_process_current_connections gauge",
        "haproxy_process_current_connections 1",
        "# HELP haproxy_process_connections_total Total number of connections on this worker process since started",
        "# TYPE haproxy_process_connections_total counter",
        "haproxy_process_connections_total 196",
        "# HELP haproxy_process_requests_total Total number of requests on this worker process since started",
        "# TYPE haproxy_process_requests_total counter",
        "haproxy_process_requests_total 10",
        "# HELP haproxy_process_max_ssl_connections Hard limit on the number of per-process SSL endpoints (front+back), 0=unlimited",
        "# TYPE haproxy_process_max_ssl_connections gauge",
        "haproxy_process_max_ssl_connections 0",
        "# HELP haproxy_process_current_ssl_connections Current number of SSL endpoints on this worker process (front+back)",
        "# TYPE haproxy_process_current_ssl_connections gauge",
        "haproxy_process_current_ssl_connections 0",
        "# HELP haproxy_process_ssl_connections_total Total number of SSL endpoints on this worker process since started (front+back)",
        "# TYPE haproxy_process_ssl_connections_total counter",
        "haproxy_process_ssl_connections_total 0",
        "# HELP haproxy_process_max_pipes Hard limit on the number of pipes for splicing, 0=unlimited",
        "# TYPE haproxy_process_max_pipes gauge",
        "haproxy_process_max_pipes 0",
        "# HELP haproxy_process_pipes_used_total Current number of pipes in use in this worker process",
        "# TYPE haproxy_process_pipes_used_total counter",
        "haproxy_process_pipes_used_total 0",
        "# HELP haproxy_process_pipes_free_total Current number of allocated and available pipes in this worker process",
        "# TYPE haproxy_process_pipes_free_total counter",
        "haproxy_process_pipes_free_total 0",
        "# HELP haproxy_process_current_connection_rate Number of front connections created on this worker process over the last second",
        "# TYPE haproxy_process_current_connection_rate gauge",
        "haproxy_process_current_connection_rate 1",
        "# HELP haproxy_process_limit_connection_rate Hard limit for ConnRate (global.maxconnrate)",
        "# TYPE haproxy_process_limit_connection_rate gauge",
        "haproxy_process_limit_connection_rate 0",
        "# HELP haproxy_process_max_connection_rate Highest ConnRate reached on this worker process since started (in connections per second)",
        "# TYPE haproxy_process_max_connection_rate gauge",
        "haproxy_process_max_connection_rate 1",
        "# HELP haproxy_process_current_session_rate Number of sessions created on this worker process over the last second",
        "# TYPE haproxy_process_current_session_rate gauge",
        "haproxy_process_current_session_rate 1",
        "# HELP haproxy_process_limit_session_rate Hard limit for SessRate (global.maxsessrate)",
        "# TYPE haproxy_process_limit_session_rate gauge",
        "haproxy_process_limit_session_rate 0",
        "# HELP haproxy_process_max_session_rate Highest SessRate reached on this worker process since started (in sessions per second)",
        "# TYPE haproxy_process_max_session_rate gauge",
        "haproxy_process_max_session_rate 1",
        "# HELP haproxy_process_current_ssl_rate Number of SSL connections created on this worker process over the last second",
        "# TYPE haproxy_process_current_ssl_rate gauge",
        "haproxy_process_current_ssl_rate 0",
        "# HELP haproxy_process_limit_ssl_rate Hard limit for SslRate (global.maxsslrate)",
        "# TYPE haproxy_process_limit_ssl_rate gauge",
        "haproxy_process_limit_ssl_rate 0",
        "# HELP haproxy_process_max_ssl_rate Highest SslRate reached on this worker process since started (in connections per second)",
        "# TYPE haproxy_process_max_ssl_rate gauge",
        "haproxy_process_max_ssl_rate 0",
        "# HELP haproxy_process_current_frontend_ssl_key_rate Number of SSL keys created on frontends in this worker process over the last second",
        "# TYPE haproxy_process_current_frontend_ssl_key_rate gauge",
        "haproxy_process_current_frontend_ssl_key_rate 0",
        "# HELP haproxy_process_max_frontend_ssl_key_rate Highest SslFrontendKeyRate reached on this worker process since started (in SSL keys per second)",
        "# TYPE haproxy_process_max_frontend_ssl_key_rate gauge",
        "haproxy_process_max_frontend_ssl_key_rate 0",
        "# HELP haproxy_process_frontend_ssl_reuse Percent of frontend SSL connections which did not require a new key",
        "# TYPE haproxy_process_frontend_ssl_reuse gauge",
        "haproxy_process_frontend_ssl_reuse 0",
        "# HELP haproxy_process_current_backend_ssl_key_rate Number of SSL keys created on backends in this worker process over the last second",
        "# TYPE haproxy_process_current_backend_ssl_key_rate gauge",
        "haproxy_process_current_backend_ssl_key_rate 0",
        "# HELP haproxy_process_max_backend_ssl_key_rate Highest SslBackendKeyRate reached on this worker process since started (in SSL keys per second)",
        "# TYPE haproxy_process_max_backend_ssl_key_rate gauge",
        "haproxy_process_max_backend_ssl_key_rate 0",
        "# HELP haproxy_process_ssl_cache_lookups_total Total number of SSL session ID lookups in the SSL session cache on this worker since started",
        "# TYPE haproxy_process_ssl_cache_lookups_total counter",
        "haproxy_process_ssl_cache_lookups_total 0",
        "# HELP haproxy_process_ssl_cache_misses_total Total number of SSL session ID lookups that didn't find a session in the SSL session cache on this worker since started",
        "# TYPE haproxy_process_ssl_cache_misses_total counter",
        "haproxy_process_ssl_cache_misses_total 0",
        "# HELP haproxy_process_http_comp_bytes_in_total Number of bytes submitted to the HTTP compressor in this worker process over the last second",
        "# TYPE haproxy_process_http_comp_bytes_in_total counter",
        "haproxy_process_http_comp_bytes_in_total 0",
        "# HELP haproxy_process_http_comp_bytes_out_total Number of bytes emitted by the HTTP compressor in this worker process over the last second",
        "# TYPE haproxy_process_http_comp_bytes_out_total counter",
        "haproxy_process_http_comp_bytes_out_total 0",
        "# HELP haproxy_process_limit_http_comp Limit of CompressBpsOut beyond which HTTP compression is automatically disabled",
        "# TYPE haproxy_process_limit_http_comp gauge",
        "haproxy_process_limit_http_comp 0",
        "# HELP haproxy_process_current_zlib_memory Amount of memory currently used by HTTP compression on the current worker process (in bytes)",
        "# TYPE haproxy_process_current_zlib_memory gauge",
        "haproxy_process_current_zlib_memory NaN",
        "# HELP haproxy_process_max_zlib_memory Limit on the amount of memory used by HTTP compression above which it is automatically disabled (in bytes, see global.maxzlibmem)",
        "# TYPE haproxy_process_max_zlib_memory gauge",
        "haproxy_process_max_zlib_memory NaN",
        "# HELP haproxy_process_current_tasks Total number of tasks in the current worker process (active + sleeping)",
        "# TYPE haproxy_process_current_tasks gauge",
        "haproxy_process_current_tasks 19",
        "# HELP haproxy_process_current_run_queue Total number of active tasks+tasklets in the current worker process",
        "# TYPE haproxy_process_current_run_queue gauge",
        "haproxy_process_current_run_queue 0",
        "# HELP haproxy_process_idle_time_percent Percentage of last second spent waiting in the current worker thread",
        "# TYPE haproxy_process_idle_time_percent gauge",
        "haproxy_process_idle_time_percent 100",
        "# HELP haproxy_process_stopping 1 if the worker process is currently stopping, otherwise zero",
        "# TYPE haproxy_process_stopping gauge",
        "haproxy_process_stopping 0",
        "# HELP haproxy_process_jobs Current number of active jobs on the current worker process (frontend connections, master connections, listeners)",
        "# TYPE haproxy_process_jobs gauge",
        "haproxy_process_jobs 6",
        "# HELP haproxy_process_unstoppable_jobs Current number of unstoppable jobs on the current worker process (master connections)",
        "# TYPE haproxy_process_unstoppable_jobs gauge",
        "haproxy_process_unstoppable_jobs 1",
        "# HELP haproxy_process_listeners Current number of active listeners on the current worker process",
        "# TYPE haproxy_process_listeners gauge",
        "haproxy_process_listeners 4",
        "# HELP haproxy_process_active_peers Current number of verified active peers connections on the current worker process",
        "# TYPE haproxy_process_active_peers gauge",
        "haproxy_process_active_peers 0",
        "# HELP haproxy_process_connected_peers Current number of peers having passed the connection step on the current worker process",
        "# TYPE haproxy_process_connected_peers gauge",
        "haproxy_process_connected_peers 0",
        "# HELP haproxy_process_dropped_logs_total Total number of dropped logs for current worker process since started",
        "# TYPE haproxy_process_dropped_logs_total counter",
        "haproxy_process_dropped_logs_total 0",
        "# HELP haproxy_process_busy_polling_enabled 1 if busy-polling is currently in use on the worker process, otherwise zero (config.busy-polling)",
        "# TYPE haproxy_process_busy_polling_enabled gauge",
        "haproxy_process_busy_polling_enabled 0",
        "# HELP haproxy_process_failed_resolutions Total number of failed DNS resolutions in current worker process since started",
        "# TYPE haproxy_process_failed_resolutions counter",
        "haproxy_process_failed_resolutions 0",
        "# HELP haproxy_process_bytes_out_total Total number of bytes emitted by current worker process since started",
        "# TYPE haproxy_process_bytes_out_total counter",
        "haproxy_process_bytes_out_total 69369",
        "# HELP haproxy_process_spliced_bytes_out_total Total number of bytes emitted by current worker process through a kernel pipe since started",
        "# TYPE haproxy_process_spliced_bytes_out_total counter",
        "haproxy_process_spliced_bytes_out_total 0",
        "# HELP haproxy_process_bytes_out_rate Number of bytes emitted by current worker process over the last second",
        "# TYPE haproxy_process_bytes_out_rate gauge",
        "haproxy_process_bytes_out_rate 0",
        "# HELP haproxy_process_recv_logs_total Total number of log messages received by log-forwarding listeners on this worker process since started",
        "# TYPE haproxy_process_recv_logs_total counter",
        "haproxy_process_recv_logs_total 0",
        "# HELP haproxy_process_build_info Build info",
        "# TYPE haproxy_process_build_info gauge",
        "haproxy_process_build_info{version=\"2.9.15-1ppa1~focal\"} 1",
        "# HELP haproxy_process_max_memory_bytes Worker process's hard limit on memory usage in byes (-m on command line)",
        "# TYPE haproxy_process_max_memory_bytes gauge",
        "haproxy_process_max_memory_bytes 0",
        "# HELP haproxy_process_pool_allocated_bytes Amount of memory allocated in pools (in bytes)",
        "# TYPE haproxy_process_pool_allocated_bytes gauge",
        "haproxy_process_pool_allocated_bytes 73624",
        "# HELP haproxy_process_pool_used_bytes Amount of pool memory currently used (in bytes)",
        "# TYPE haproxy_process_pool_used_bytes gauge",
        "haproxy_process_pool_used_bytes 73624",
        "# HELP haproxy_process_start_time_seconds Start time in seconds",
        "# TYPE haproxy_process_start_time_seconds gauge",
        "haproxy_process_start_time_seconds 1754596161",
        "# HELP haproxy_frontend_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_frontend_current_sessions gauge",
        "haproxy_frontend_current_sessions{proxy=\"web_stats\"} 1",
        "haproxy_frontend_current_sessions{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_frontend_max_sessions gauge",
        "haproxy_frontend_max_sessions{proxy=\"web_stats\"} 1",
        "haproxy_frontend_max_sessions{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_frontend_limit_sessions gauge",
        "haproxy_frontend_limit_sessions{proxy=\"web_stats\"} 262113",
        "haproxy_frontend_limit_sessions{proxy=\"haproxy_frontend\"} 262113",
        "# HELP haproxy_frontend_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_frontend_sessions_total counter",
        "haproxy_frontend_sessions_total{proxy=\"web_stats\"} 7",
        "haproxy_frontend_sessions_total{proxy=\"haproxy_frontend\"} 3",
        "# HELP haproxy_frontend_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_frontend_bytes_in_total counter",
        "haproxy_frontend_bytes_in_total{proxy=\"web_stats\"} 834",
        "haproxy_frontend_bytes_in_total{proxy=\"haproxy_frontend\"} 488",
        "# HELP haproxy_frontend_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_frontend_bytes_out_total counter",
        "haproxy_frontend_bytes_out_total{proxy=\"web_stats\"} 68083",
        "haproxy_frontend_bytes_out_total{proxy=\"haproxy_frontend\"} 779",
        "# HELP haproxy_frontend_requests_denied_total Total number of denied requests since process started",
        "# TYPE haproxy_frontend_requests_denied_total counter",
        "haproxy_frontend_requests_denied_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_requests_denied_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_frontend_responses_denied_total counter",
        "haproxy_frontend_responses_denied_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_responses_denied_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_request_errors_total Total number of invalid requests since process started",
        "# TYPE haproxy_frontend_request_errors_total counter",
        "haproxy_frontend_request_errors_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_request_errors_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_status Current status of the service, per state label value.",
        "# TYPE haproxy_frontend_status gauge",
        "haproxy_frontend_status{proxy=\"web_stats\",state=\"DOWN\"} 0",
        "haproxy_frontend_status{proxy=\"web_stats\",state=\"UP\"} 1",
        "haproxy_frontend_status{proxy=\"haproxy_frontend\",state=\"DOWN\"} 0",
        "haproxy_frontend_status{proxy=\"haproxy_frontend\",state=\"UP\"} 1",
        "# HELP haproxy_frontend_limit_session_rate Limit on the number of sessions accepted in a second (frontend only, 'rate-limit sessions' setting)",
        "# TYPE haproxy_frontend_limit_session_rate gauge",
        "haproxy_frontend_limit_session_rate{proxy=\"web_stats\"} 0",
        "haproxy_frontend_limit_session_rate{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_frontend_max_session_rate gauge",
        "haproxy_frontend_max_session_rate{proxy=\"web_stats\"} 1",
        "haproxy_frontend_max_session_rate{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_frontend_http_responses_total counter",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"1xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"1xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"2xx\"} 2",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"2xx\"} 2",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"3xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"3xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"4xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"4xx\"} 1",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"5xx\"} 4",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"5xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"other\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"other\"} 0",
        "# HELP haproxy_frontend_http_requests_rate_max Highest value of http requests observed since the worker process started",
        "# TYPE haproxy_frontend_http_requests_rate_max gauge",
        "haproxy_frontend_http_requests_rate_max{proxy=\"web_stats\"} 1",
        "haproxy_frontend_http_requests_rate_max{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_http_requests_total Total number of HTTP requests processed by this object since the worker process started",
        "# TYPE haproxy_frontend_http_requests_total counter",
        "haproxy_frontend_http_requests_total{proxy=\"web_stats\"} 7",
        "haproxy_frontend_http_requests_total{proxy=\"haproxy_frontend\"} 3",
        "# HELP haproxy_frontend_http_comp_bytes_in_total Total number of bytes submitted to the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_bytes_in_total counter",
        "haproxy_frontend_http_comp_bytes_in_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_in_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_bytes_out_total Total number of bytes emitted by the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_bytes_out_total counter",
        "haproxy_frontend_http_comp_bytes_out_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_out_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_bytes_bypassed_total Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)",
        "# TYPE haproxy_frontend_http_comp_bytes_bypassed_total counter",
        "haproxy_frontend_http_comp_bytes_bypassed_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_bypassed_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_responses_total Total number of HTTP responses that were compressed for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_responses_total counter",
        "haproxy_frontend_http_comp_responses_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_responses_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_connections_rate_max Highest value of connections per second observed since the worker process started",
        "# TYPE haproxy_frontend_connections_rate_max gauge",
        "haproxy_frontend_connections_rate_max{proxy=\"web_stats\"} 1",
        "haproxy_frontend_connections_rate_max{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_connections_total Total number of new connections accepted on this frontend since the worker process started",
        "# TYPE haproxy_frontend_connections_total counter",
        "haproxy_frontend_connections_total{proxy=\"web_stats\"} 7",
        "haproxy_frontend_connections_total{proxy=\"haproxy_frontend\"} 3",
        "# HELP haproxy_frontend_intercepted_requests_total Total number of HTTP requests intercepted on the frontend (redirects/stats/services) since the worker process started",
        "# TYPE haproxy_frontend_intercepted_requests_total counter",
        "haproxy_frontend_intercepted_requests_total{proxy=\"web_stats\"} 3",
        "haproxy_frontend_intercepted_requests_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_denied_connections_total Total number of incoming connections blocked on a listener/frontend by a tcp-request connection rule since the worker process started",
        "# TYPE haproxy_frontend_denied_connections_total counter",
        "haproxy_frontend_denied_connections_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_denied_connections_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_denied_sessions_total Total number of incoming sessions blocked on a listener/frontend by a tcp-request connection rule since the worker process started",
        "# TYPE haproxy_frontend_denied_sessions_total counter",
        "haproxy_frontend_denied_sessions_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_denied_sessions_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_frontend_failed_header_rewriting_total counter",
        "haproxy_frontend_failed_header_rewriting_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_failed_header_rewriting_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_cache_lookups_total Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_frontend_http_cache_lookups_total counter",
        "haproxy_frontend_http_cache_lookups_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_cache_lookups_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_cache_hits_total Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_frontend_http_cache_hits_total counter",
        "haproxy_frontend_http_cache_hits_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_cache_hits_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_frontend_internal_errors_total counter",
        "haproxy_frontend_internal_errors_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_internal_errors_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_backend_current_queue Number of current queued connections",
        "# TYPE haproxy_backend_current_queue gauge",
        "haproxy_backend_current_queue{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_queue Highest value of queued connections encountered since process started",
        "# TYPE haproxy_backend_max_queue gauge",
        "haproxy_backend_max_queue{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_backend_current_sessions gauge",
        "haproxy_backend_current_sessions{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_backend_max_sessions gauge",
        "haproxy_backend_max_sessions{proxy=\"haproxy_backend\"} 1",
        "# HELP haproxy_backend_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_backend_limit_sessions gauge",
        "haproxy_backend_limit_sessions{proxy=\"haproxy_backend\"} 26212",
        "# HELP haproxy_backend_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_backend_sessions_total counter",
        "haproxy_backend_sessions_total{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_backend_bytes_in_total counter",
        "haproxy_backend_bytes_in_total{proxy=\"haproxy_backend\"} 488",
        "# HELP haproxy_backend_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_backend_bytes_out_total counter",
        "haproxy_backend_bytes_out_total{proxy=\"haproxy_backend\"} 779",
        "# HELP haproxy_backend_requests_denied_total Total number of denied requests since process started",
        "# TYPE haproxy_backend_requests_denied_total counter",
        "haproxy_backend_requests_denied_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_backend_responses_denied_total counter",
        "haproxy_backend_responses_denied_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_connection_errors_total Total number of failed connections to server since the worker process started",
        "# TYPE haproxy_backend_connection_errors_total counter",
        "haproxy_backend_connection_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_response_errors_total Total number of invalid responses since the worker process started",
        "# TYPE haproxy_backend_response_errors_total counter",
        "haproxy_backend_response_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_retry_warnings_total Total number of server connection retries since the worker process started",
        "# TYPE haproxy_backend_retry_warnings_total counter",
        "haproxy_backend_retry_warnings_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_redispatch_warnings_total Total number of server redispatches due to connection failures since the worker process started",
        "# TYPE haproxy_backend_redispatch_warnings_total counter",
        "haproxy_backend_redispatch_warnings_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_status Current status of the service, per state label value.",
        "# TYPE haproxy_backend_status gauge",
        "haproxy_backend_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_status{proxy=\"haproxy_backend\",state=\"UP\"} 1",
        "# HELP haproxy_backend_weight Server's effective weight, or sum of active servers' effective weights for a backend",
        "# TYPE haproxy_backend_weight gauge",
        "haproxy_backend_weight{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_active_servers Total number of active UP servers with a non-zero weight",
        "# TYPE haproxy_backend_active_servers gauge",
        "haproxy_backend_active_servers{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_backup_servers Total number of backup UP servers with a non-zero weight",
        "# TYPE haproxy_backend_backup_servers gauge",
        "haproxy_backend_backup_servers{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_check_up_down_total Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started",
        "# TYPE haproxy_backend_check_up_down_total counter",
        "haproxy_backend_check_up_down_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_check_last_change_seconds How long ago the last server state changed, in seconds",
        "# TYPE haproxy_backend_check_last_change_seconds gauge",
        "haproxy_backend_check_last_change_seconds{proxy=\"haproxy_backend\"} 124",
        "# HELP haproxy_backend_downtime_seconds_total Total time spent in DOWN state, for server or backend",
        "# TYPE haproxy_backend_downtime_seconds_total counter",
        "haproxy_backend_downtime_seconds_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_loadbalanced_total Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)",
        "# TYPE haproxy_backend_loadbalanced_total counter",
        "haproxy_backend_loadbalanced_total{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_backend_max_session_rate gauge",
        "haproxy_backend_max_session_rate{proxy=\"haproxy_backend\"} 1",
        "# HELP haproxy_backend_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_backend_http_responses_total counter",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"1xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"2xx\"} 2",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"3xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"4xx\"} 1",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"5xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"other\"} 0",
        "# HELP haproxy_backend_http_requests_total Total number of HTTP requests processed by this object since the worker process started",
        "# TYPE haproxy_backend_http_requests_total counter",
        "haproxy_backend_http_requests_total{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_client_aborts_total Total number of requests or connections aborted by the client since the worker process started",
        "# TYPE haproxy_backend_client_aborts_total counter",
        "haproxy_backend_client_aborts_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_server_aborts_total Total number of requests or connections aborted by the server since the worker process started",
        "# TYPE haproxy_backend_server_aborts_total counter",
        "haproxy_backend_server_aborts_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_in_total Total number of bytes submitted to the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_bytes_in_total counter",
        "haproxy_backend_http_comp_bytes_in_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_out_total Total number of bytes emitted by the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_bytes_out_total counter",
        "haproxy_backend_http_comp_bytes_out_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_bypassed_total Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)",
        "# TYPE haproxy_backend_http_comp_bytes_bypassed_total counter",
        "haproxy_backend_http_comp_bytes_bypassed_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_responses_total Total number of HTTP responses that were compressed for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_responses_total counter",
        "haproxy_backend_http_comp_responses_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_last_session_seconds How long ago some traffic was seen on this object on this worker process, in seconds",
        "# TYPE haproxy_backend_last_session_seconds gauge",
        "haproxy_backend_last_session_seconds{proxy=\"haproxy_backend\"} 4",
        "# HELP haproxy_backend_queue_time_average_seconds Avg. queue time for last 1024 successful connections.",
        "# TYPE haproxy_backend_queue_time_average_seconds gauge",
        "haproxy_backend_queue_time_average_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_connect_time_average_seconds Avg. connect time for last 1024 successful connections.",
        "# TYPE haproxy_backend_connect_time_average_seconds gauge",
        "haproxy_backend_connect_time_average_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_response_time_average_seconds Avg. response time for last 1024 successful connections.",
        "# TYPE haproxy_backend_response_time_average_seconds gauge",
        "haproxy_backend_response_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_total_time_average_seconds Avg. total time for last 1024 successful connections.",
        "# TYPE haproxy_backend_total_time_average_seconds gauge",
        "haproxy_backend_total_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_backend_failed_header_rewriting_total counter",
        "haproxy_backend_failed_header_rewriting_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_connection_attempts_total Total number of outgoing connection attempts on this backend/server since the worker process started",
        "# TYPE haproxy_backend_connection_attempts_total counter",
        "haproxy_backend_connection_attempts_total{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_connection_reuses_total Total number of reused connection on this backend/server since the worker process started",
        "# TYPE haproxy_backend_connection_reuses_total counter",
        "haproxy_backend_connection_reuses_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_cache_lookups_total Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_backend_http_cache_lookups_total counter",
        "haproxy_backend_http_cache_lookups_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_cache_hits_total Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_backend_http_cache_hits_total counter",
        "haproxy_backend_http_cache_hits_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_queue_time_seconds Maximum observed time spent in the queue",
        "# TYPE haproxy_backend_max_queue_time_seconds gauge",
        "haproxy_backend_max_queue_time_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_max_connect_time_seconds Maximum observed time spent waiting for a connection to complete",
        "# TYPE haproxy_backend_max_connect_time_seconds gauge",
        "haproxy_backend_max_connect_time_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_max_response_time_seconds Maximum observed time spent waiting for a server response",
        "# TYPE haproxy_backend_max_response_time_seconds gauge",
        "haproxy_backend_max_response_time_seconds{proxy=\"haproxy_backend\"} 0.004000",
        "# HELP haproxy_backend_max_total_time_seconds Maximum observed total request+response time (request+queue+connect+response+processing)",
        "# TYPE haproxy_backend_max_total_time_seconds gauge",
        "haproxy_backend_max_total_time_seconds{proxy=\"haproxy_backend\"} 0.004000",
        "# HELP haproxy_backend_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_backend_internal_errors_total counter",
        "haproxy_backend_internal_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_uweight Server's user weight, or sum of active servers' user weights for a backend",
        "# TYPE haproxy_backend_uweight gauge",
        "haproxy_backend_uweight{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_agg_server_status Backend's aggregated gauge of servers' status",
        "# TYPE haproxy_backend_agg_server_status gauge",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"UP\"} 3",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"MAINT\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"DRAIN\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"NOLB\"} 0",
        "# HELP haproxy_backend_agg_server_check_status [DEPRECATED] Backend's aggregated gauge of servers' status",
        "# TYPE haproxy_backend_agg_server_check_status gauge",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"UP\"} 3",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"MAINT\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"DRAIN\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"NOLB\"} 0",
        "# HELP haproxy_backend_agg_check_status Backend's aggregated gauge of servers' state check status",
        "# TYPE haproxy_backend_agg_check_status gauge",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"HANA\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"SOCKERR\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4OK\"} 3",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4CON\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6OK\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6RSP\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7RSP\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7OK\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7OKC\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7STS\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCERR\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCTOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCOK\"} 0",
        "# HELP haproxy_server_current_queue Number of current queued connections",
        "# TYPE haproxy_server_current_queue gauge",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_max_queue Highest value of queued connections encountered since process started",
        "# TYPE haproxy_server_max_queue gauge",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_server_current_sessions gauge",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_server_max_sessions gauge",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_server_limit_sessions gauge",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "# HELP haproxy_server_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_server_sessions_total counter",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_server_bytes_in_total counter",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 292",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 98",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 98",
        "# HELP haproxy_server_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_server_bytes_out_total counter",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 368",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 206",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 205",
        "# HELP haproxy_server_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_server_responses_denied_total counter",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_connection_errors_total Total number of failed connections to server since the worker process started",
        "# TYPE haproxy_server_connection_errors_total counter",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_response_errors_total Total number of invalid responses since the worker process started",
        "# TYPE haproxy_server_response_errors_total counter",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_retry_warnings_total Total number of server connection retries since the worker process started",
        "# TYPE haproxy_server_retry_warnings_total counter",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_redispatch_warnings_total Total number of server redispatches due to connection failures since the worker process started",
        "# TYPE haproxy_server_redispatch_warnings_total counter",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_status Current status of the service, per state label value.",
        "# TYPE haproxy_server_status gauge",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"NOLB\"} 0",
        "# HELP haproxy_server_weight Server's effective weight, or sum of active servers' effective weights for a backend",
        "# TYPE haproxy_server_weight gauge",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_check_failures_total Total number of failed individual health checks per server/backend, since the worker process started",
        "# TYPE haproxy_server_check_failures_total counter",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_check_up_down_total Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started",
        "# TYPE haproxy_server_check_up_down_total counter",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_check_last_change_seconds How long ago the last server state changed, in seconds",
        "# TYPE haproxy_server_check_last_change_seconds gauge",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 124",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 124",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 124",
        "# HELP haproxy_server_downtime_seconds_total Total time spent in DOWN state, for server or backend",
        "# TYPE haproxy_server_downtime_seconds_total counter",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_queue_limit Limit on the number of connections in queue, for servers only (maxqueue argument)",
        "# TYPE haproxy_server_queue_limit gauge",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "# HELP haproxy_server_current_throttle Throttling ratio applied to a server's maxconn and weight during the slowstart period (0 to 100%)",
        "# TYPE haproxy_server_current_throttle gauge",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "# HELP haproxy_server_loadbalanced_total Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)",
        "# TYPE haproxy_server_loadbalanced_total counter",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_server_max_session_rate gauge",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_check_status Status of last health check, per state label value.",
        "# TYPE haproxy_server_check_status gauge",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCOK\"} 0",
        "# HELP haproxy_server_check_code layer5-7 code, if available of the last health check.",
        "# TYPE haproxy_server_check_code gauge",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_check_duration_seconds Total duration of the latest server health check, in seconds.",
        "# TYPE haproxy_server_check_duration_seconds gauge",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.001000",
        "# HELP haproxy_server_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_server_http_responses_total counter",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"2xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"2xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"2xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"4xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"4xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"4xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"other\"} 0",
        "# HELP haproxy_server_client_aborts_total Total number of requests or connections aborted by the client since the worker process started",
        "# TYPE haproxy_server_client_aborts_total counter",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_server_aborts_total Total number of requests or connections aborted by the server since the worker process started",
        "# TYPE haproxy_server_server_aborts_total counter",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_last_session_seconds How long ago some traffic was seen on this object on this worker process, in seconds",
        "# TYPE haproxy_server_last_session_seconds gauge",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 14",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 6",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 4",
        "# HELP haproxy_server_queue_time_average_seconds Avg. queue time for last 1024 successful connections.",
        "# TYPE haproxy_server_queue_time_average_seconds gauge",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_connect_time_average_seconds Avg. connect time for last 1024 successful connections.",
        "# TYPE haproxy_server_connect_time_average_seconds gauge",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_response_time_average_seconds Avg. response time for last 1024 successful connections.",
        "# TYPE haproxy_server_response_time_average_seconds gauge",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.001000",
        "# HELP haproxy_server_total_time_average_seconds Avg. total time for last 1024 successful connections.",
        "# TYPE haproxy_server_total_time_average_seconds gauge",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.001000",
        "# HELP haproxy_server_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_server_failed_header_rewriting_total counter",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_connection_attempts_total Total number of outgoing connection attempts on this backend/server since the worker process started",
        "# TYPE haproxy_server_connection_attempts_total counter",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_connection_reuses_total Total number of reused connection on this backend/server since the worker process started",
        "# TYPE haproxy_server_connection_reuses_total counter",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_idle_connections_current Current number of idle connections available for reuse on this server",
        "# TYPE haproxy_server_idle_connections_current gauge",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_idle_connections_limit Limit on the number of available idle connections on this server (server 'pool_max_conn' directive)",
        "# TYPE haproxy_server_idle_connections_limit gauge",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "# HELP haproxy_server_max_queue_time_seconds Maximum observed time spent in the queue",
        "# TYPE haproxy_server_max_queue_time_seconds gauge",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_max_connect_time_seconds Maximum observed time spent waiting for a connection to complete",
        "# TYPE haproxy_server_max_connect_time_seconds gauge",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_max_response_time_seconds Maximum observed time spent waiting for a server response",
        "# TYPE haproxy_server_max_response_time_seconds gauge",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.002000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.004000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.004000",
        "# HELP haproxy_server_max_total_time_seconds Maximum observed total request+response time (request+queue+connect+response+processing)",
        "# TYPE haproxy_server_max_total_time_seconds gauge",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.002000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.004000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.004000",
        "# HELP haproxy_server_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_server_internal_errors_total counter",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_unsafe_idle_connections_current Current number of unsafe idle connections",
        "# TYPE haproxy_server_unsafe_idle_connections_current gauge",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_safe_idle_connections_current Current number of safe idle connections",
        "# TYPE haproxy_server_safe_idle_connections_current gauge",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_used_connections_current Current number of connections in use",
        "# TYPE haproxy_server_used_connections_current gauge",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_need_connections_current Estimated needed number of connections",
        "# TYPE haproxy_server_need_connections_current gauge",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_uweight Server's user weight, or sum of active servers' user weights for a backend",
        "# TYPE haproxy_server_uweight gauge",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1"
    ]
}
2025-08-07 19:51:25,521 p=1305050 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-07 19:51:27,320 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:51:27,325 p=1305050 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-07 19:51:27,341 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : path to executable is /usr/sbin/haproxy",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [WARNING]  (4462) : Exiting Master process...",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [ALERT]    (4462) : Current worker (4482) exited with code 143 (Terminated)",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [WARNING]  (4462) : All workers exited. Exiting... (0)",
        "Aug  7 19:49:21 rev1-haproxy haproxy[10224]: [NOTICE]   (10224) : New worker (10226) forked",
        "Aug  7 19:49:21 rev1-haproxy haproxy[10224]: [NOTICE]   (10224) : Loading success.",
        "Aug  7 19:51:11 rev1-haproxy haproxy[10226]: 127.0.0.1:45972 [07/Aug/2025:19:51:11.738] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:16122 cpu_ns_avg:4030 lat_ns_tot:4527 lat_ns_avg:1131",
        "Aug  7 19:51:19 rev1-haproxy haproxy[10226]: 185.62.207.61:11803 [07/Aug/2025:19:51:19.604] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/4/4 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13443 cpu_ns_avg:3360 lat_ns_tot:5173 lat_ns_avg:1293",
        "Aug  7 19:51:21 rev1-haproxy haproxy[10226]: 185.62.207.61:33979 [07/Aug/2025:19:51:21.457] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/4/4 200 205 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:16136 cpu_ns_avg:4034 lat_ns_tot:3382 lat_ns_avg:845"
    ]
}
2025-08-07 19:51:27,346 p=1305050 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] **********************************************************************************************************************************
2025-08-07 19:51:29,118 p=1305050 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-07 19:51:29,123 p=1305050 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] **********************************************************************************************************************************
2025-08-07 19:51:29,139 p=1305050 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [NOTICE]   (4462) : path to executable is /usr/sbin/haproxy",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [WARNING]  (4462) : Exiting Master process...",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [ALERT]    (4462) : Current worker (4482) exited with code 143 (Terminated)",
        "Aug  7 19:49:21 rev1-haproxy haproxy[4462]: [WARNING]  (4462) : All workers exited. Exiting... (0)",
        "Aug  7 19:49:21 rev1-haproxy haproxy[10224]: [NOTICE]   (10224) : New worker (10226) forked",
        "Aug  7 19:49:21 rev1-haproxy haproxy[10224]: [NOTICE]   (10224) : Loading success.",
        "Aug  7 19:51:11 rev1-haproxy haproxy[10226]: 127.0.0.1:45972 [07/Aug/2025:19:51:11.738] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:16122 cpu_ns_avg:4030 lat_ns_tot:4527 lat_ns_avg:1131",
        "Aug  7 19:51:19 rev1-haproxy haproxy[10226]: 185.62.207.61:11803 [07/Aug/2025:19:51:19.604] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/4/4 200 206 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13443 cpu_ns_avg:3360 lat_ns_tot:5173 lat_ns_avg:1293",
        "Aug  7 19:51:21 rev1-haproxy haproxy[10226]: 185.62.207.61:33979 [07/Aug/2025:19:51:21.457] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/4/4 200 205 - - ---- 1/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:16136 cpu_ns_avg:4034 lat_ns_tot:3382 lat_ns_avg:845"
    ]
}
2025-08-07 19:51:29,155 p=1305050 u=ubuntu n=ansible | PLAY [Test NGINX (snmp) proxy] ****************************************************************************************************************************************
2025-08-07 19:51:29,161 p=1305050 u=ubuntu n=ansible | TASK [Gathering Facts] ************************************************************************************************************************************************
2025-08-07 19:51:32,325 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 19:51:32,338 p=1305050 u=ubuntu n=ansible | TASK [Gather NGINX public IP address] *********************************************************************************************************************************
2025-08-07 19:51:34,610 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-07 19:51:34,616 p=1305050 u=ubuntu n=ansible | TASK [Send SNMP request to NGINX server and collect responses] ********************************************************************************************************
2025-08-07 19:51:36,876 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=0)
2025-08-07 19:51:38,986 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=1)
2025-08-07 19:51:41,019 p=1305050 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=2)
2025-08-07 19:51:41,025 p=1305050 u=ubuntu n=ansible | TASK [Display the NGINX response content] *****************************************************************************************************************************
2025-08-07 19:51:41,043 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=0) => {
    "ansible_loop_var": "item",
    "item": 0,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-07 19:51:41,046 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=1) => {
    "ansible_loop_var": "item",
    "item": 1,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-07 19:51:41,049 p=1305050 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=2) => {
    "ansible_loop_var": "item",
    "item": 2,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-07 19:51:41,066 p=1305050 u=ubuntu n=ansible | PLAY RECAP ************************************************************************************************************************************************************
2025-08-07 19:51:41,066 p=1305050 u=ubuntu n=ansible | rev1_HAproxy               : ok=46   changed=22   unreachable=0    failed=0    skipped=0    rescued=0    ignored=1   
2025-08-07 19:51:41,066 p=1305050 u=ubuntu n=ansible | rev1_NGINX                 : ok=10   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 19:51:41,066 p=1305050 u=ubuntu n=ansible | rev1_devA                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 19:51:41,066 p=1305050 u=ubuntu n=ansible | rev1_devB                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-07 19:51:41,066 p=1305050 u=ubuntu n=ansible | rev1_devC                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 09:22:42,182 p=1384604 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] *********************************************************************************************************************************************************************
2025-08-12 09:22:42,190 p=1384604 u=ubuntu n=ansible | TASK [Gathering Facts] *******************************************************************************************************************************************************************************************************
2025-08-12 09:22:53,860 p=1384604 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-12 09:22:54,552 p=1384604 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-12 09:22:54,593 p=1384604 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-12 09:22:54,617 p=1384604 u=ubuntu n=ansible | TASK [Install required packages] *********************************************************************************************************************************************************************************************
2025-08-12 09:23:06,937 p=1384604 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3)
2025-08-12 09:23:07,207 p=1384604 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3)
2025-08-12 09:23:09,236 p=1384604 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3)
2025-08-12 09:23:17,301 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB] => (item=python3-pip)
2025-08-12 09:23:17,695 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA] => (item=python3-pip)
2025-08-12 09:23:20,069 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC] => (item=python3-pip)
2025-08-12 09:23:29,701 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmpd)
2025-08-12 09:23:30,719 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmpd)
2025-08-12 09:23:33,548 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmpd)
2025-08-12 09:23:38,172 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp)
2025-08-12 09:23:39,831 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp)
2025-08-12 09:23:43,670 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp)
2025-08-12 09:23:52,422 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp-mibs-downloader)
2025-08-12 09:23:54,299 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp-mibs-downloader)
2025-08-12 09:23:59,208 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp-mibs-downloader)
2025-08-12 09:24:01,639 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB] => (item=apache2-utils)
2025-08-12 09:24:03,711 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA] => (item=apache2-utils)
2025-08-12 09:24:09,491 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC] => (item=apache2-utils)
2025-08-12 09:24:09,868 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB] => (item=httperf)
2025-08-12 09:24:12,532 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA] => (item=httperf)
2025-08-12 09:24:19,275 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC] => (item=httperf)
2025-08-12 09:24:19,283 p=1384604 u=ubuntu n=ansible | TASK [Install Flask] *********************************************************************************************************************************************************************************************************
2025-08-12 09:24:26,214 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:24:26,684 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:24:27,100 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:24:27,105 p=1384604 u=ubuntu n=ansible | TASK [Deploy the Flask application config for TCP Load Balancing] ************************************************************************************************************************************************************
2025-08-12 09:24:35,233 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:24:35,583 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:24:36,089 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:24:36,094 p=1384604 u=ubuntu n=ansible | TASK [Start Flask app in background on port 5000] ****************************************************************************************************************************************************************************
2025-08-12 09:24:40,236 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:24:40,954 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:24:41,019 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:24:41,024 p=1384604 u=ubuntu n=ansible | TASK [Check Flask app HTTP response on private IP] ***************************************************************************************************************************************************************************
2025-08-12 09:24:45,775 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:24:45,887 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:24:45,999 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:24:46,004 p=1384604 u=ubuntu n=ansible | TASK [Display Flask app HTTP response on private IP] *************************************************************************************************************************************************************************
2025-08-12 09:24:46,030 p=1384604 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "09:24:45 10.1.1.50:35482 -- 10.1.1.50 (rev1-deva) 27"
}
2025-08-12 09:24:46,030 p=1384604 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "09:24:45 10.1.1.54:52020 -- 10.1.1.54 (rev1-devb) 3"
}
2025-08-12 09:24:46,039 p=1384604 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "09:24:45 10.1.1.44:44764 -- 10.1.1.44 (rev1-devc) 45"
}
2025-08-12 09:24:46,045 p=1384604 u=ubuntu n=ansible | TASK [Remove the existing agent address lines] *******************************************************************************************************************************************************************************
2025-08-12 09:24:51,114 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:24:51,242 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:24:51,247 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:24:51,252 p=1384604 u=ubuntu n=ansible | TASK [Configure agent address (0.0.0.0) for SNMPd to listen on all UDP interfaces] *******************************************************************************************************************************************
2025-08-12 09:24:55,412 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:24:55,648 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:24:55,949 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:24:55,954 p=1384604 u=ubuntu n=ansible | TASK [Check snmpd config File] ***********************************************************************************************************************************************************************************************
2025-08-12 09:25:00,266 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:25:00,774 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:25:00,879 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:25:00,884 p=1384604 u=ubuntu n=ansible | TASK [Display snmpd configuration file] **************************************************************************************************************************************************************************************
2025-08-12 09:25:00,910 p=1384604 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.002799",
        "end": "2025-08-12 09:24:59.579092",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 09:24:59.576293",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 09:25:00,915 p=1384604 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003232",
        "end": "2025-08-12 09:25:00.165469",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 09:25:00.162237",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 09:25:00,926 p=1384604 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003932",
        "end": "2025-08-12 09:25:00.066236",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 09:25:00.062304",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 09:25:00,931 p=1384604 u=ubuntu n=ansible | TASK [Restart SNMPD if agent address configuration is changed] ***************************************************************************************************************************************************************
2025-08-12 09:25:05,926 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:25:06,363 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:25:06,371 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:25:06,379 p=1384604 u=ubuntu n=ansible | TASK [Test SNMPd with snmpget on 10.1.1.50] **********************************************************************************************************************************************************************************
2025-08-12 09:25:10,846 p=1384604 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 09:25:11,277 p=1384604 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 09:25:11,435 p=1384604 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 09:25:11,440 p=1384604 u=ubuntu n=ansible | TASK [Print SNMPd snmpget result] ********************************************************************************************************************************************************************************************
2025-08-12 09:25:11,465 p=1384604 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-12 09:25:11,467 p=1384604 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-12 09:25:11,476 p=1384604 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-12 09:25:11,549 p=1384604 u=ubuntu n=ansible | PLAY [Set up HAProxy] ********************************************************************************************************************************************************************************************************
2025-08-12 09:25:11,552 p=1384604 u=ubuntu n=ansible | TASK [Gathering Facts] *******************************************************************************************************************************************************************************************************
2025-08-12 09:25:19,078 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 09:25:19,090 p=1384604 u=ubuntu n=ansible | TASK [Add HAProxy 2.9 PPA] ***************************************************************************************************************************************************************************************************
2025-08-12 09:25:37,374 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:25:37,380 p=1384604 u=ubuntu n=ansible | TASK [Install HAProxy] *******************************************************************************************************************************************************************************************************
2025-08-12 09:25:54,716 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:25:54,721 p=1384604 u=ubuntu n=ansible | TASK [Deploy stats web page password file] ***********************************************************************************************************************************************************************************
2025-08-12 09:25:59,245 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:25:59,251 p=1384604 u=ubuntu n=ansible | TASK [Read stats page password from file] ************************************************************************************************************************************************************************************
2025-08-12 09:26:01,890 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 09:26:01,897 p=1384604 u=ubuntu n=ansible | TASK [Set up HAProxy stats secret variable] **********************************************************************************************************************************************************************************
2025-08-12 09:26:01,962 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 09:26:01,968 p=1384604 u=ubuntu n=ansible | TASK [Configure HAProxy] *****************************************************************************************************************************************************************************************************
2025-08-12 09:26:06,879 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:06,884 p=1384604 u=ubuntu n=ansible | TASK [Deploy rsyslog 49-haproxy config file] *********************************************************************************************************************************************************************************
2025-08-12 09:26:11,717 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:11,723 p=1384604 u=ubuntu n=ansible | TASK [Return 49_haproxy_conf to registered rsyslog_49_haproxy_conf] **********************************************************************************************************************************************************
2025-08-12 09:26:14,213 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:14,218 p=1384604 u=ubuntu n=ansible | TASK [debug] *****************************************************************************************************************************************************************************************************************
2025-08-12 09:26:14,233 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "rsyslog_49_haproxy_conf.stdout_lines": [
        "# Create an additional socket in haproxy's chroot in order to allow logging via",
        "# /dev/log to chroot'ed HAProxy processes",
        "$AddUnixListenSocket /var/lib/haproxy/dev/log",
        "",
        "# Send HAProxy messages to a dedicated logfile",
        ":programname, startswith, \"haproxy\" {",
        "  /var/log/haproxy.log",
        "stop",
        "}"
    ]
}
2025-08-12 09:26:14,237 p=1384604 u=ubuntu n=ansible | TASK [Test HAProxy Configurations] *******************************************************************************************************************************************************************************************
2025-08-12 09:26:16,990 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:16,995 p=1384604 u=ubuntu n=ansible | TASK [Display HAProxy config test result] ************************************************************************************************************************************************************************************
2025-08-12 09:26:17,010 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_configs_test_result": {
        "changed": true,
        "cmd": [
            "haproxy",
            "-f",
            "/etc/haproxy/haproxy.cfg",
            "-c"
        ],
        "delta": "0:00:00.045670",
        "end": "2025-08-12 09:26:16.566872",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 09:26:16.521202",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "",
        "stdout_lines": []
    }
}
2025-08-12 09:26:17,017 p=1384604 u=ubuntu n=ansible | TASK [Test HAProxy is running] ***********************************************************************************************************************************************************************************************
2025-08-12 09:26:20,056 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 09:26:20,061 p=1384604 u=ubuntu n=ansible | TASK [Display the HAProxy service status] ************************************************************************************************************************************************************************************
2025-08-12 09:26:20,076 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_service_status.state": "started"
}
2025-08-12 09:26:20,081 p=1384604 u=ubuntu n=ansible | TASK [Check HAProxy server status] *******************************************************************************************************************************************************************************************
2025-08-12 09:26:22,837 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:22,842 p=1384604 u=ubuntu n=ansible | TASK [Display HAProxy server status] *****************************************************************************************************************************************************************************************
2025-08-12 09:26:22,858 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "systemctl_haproxy_service_status.stdout_lines": [
        "● haproxy.service - HAProxy Load Balancer",
        "     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Tue 2025-08-12 09:25:49 UTC; 33s ago",
        "       Docs: man:haproxy(1)",
        "             file:/usr/share/doc/haproxy/configuration.txt.gz",
        "   Main PID: 4481 (haproxy)",
        "     Status: \"Ready.\"",
        "      Tasks: 2 (limit: 4588)",
        "     Memory: 39.8M",
        "     CGroup: /system.slice/haproxy.service",
        "             ├─4481 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "             └─4501 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "",
        "Aug 12 09:25:49 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 12 09:25:49 rev1-haproxy haproxy[4481]: [NOTICE]   (4481) : New worker (4501) forked",
        "Aug 12 09:25:49 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 12 09:25:49 rev1-haproxy haproxy[4481]: [NOTICE]   (4481) : Loading success."
    ]
}
2025-08-12 09:26:22,863 p=1384604 u=ubuntu n=ansible | TASK [Check HAProxy config errors via journalctl] ****************************************************************************************************************************************************************************
2025-08-12 09:26:25,708 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:25,714 p=1384604 u=ubuntu n=ansible | TASK [Display HAProxy config errors] *****************************************************************************************************************************************************************************************
2025-08-12 09:26:25,729 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_journalctl_logs.stdout_lines": [
        "-- Logs begin at Tue 2025-08-12 09:21:04 UTC, end at Tue 2025-08-12 09:26:25 UTC. --",
        "Aug 12 09:25:49 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 12 09:25:49 rev1-haproxy haproxy[4481]: [NOTICE]   (4481) : New worker (4501) forked",
        "Aug 12 09:25:49 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 12 09:25:49 rev1-haproxy haproxy[4481]: [NOTICE]   (4481) : Loading success."
    ]
}
2025-08-12 09:26:25,734 p=1384604 u=ubuntu n=ansible | TASK [Check the HAProxy configuration file] **********************************************************************************************************************************************************************************
2025-08-12 09:26:28,586 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:28,591 p=1384604 u=ubuntu n=ansible | TASK [Display HAProxy configuration file] ************************************************************************************************************************************************************************************
2025-08-12 09:26:28,608 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_config_file.stdout_lines": [
        "global",
        "    profiling.tasks on #Enable HAProxy profiling (CPU time spent on processing a http request inside HAProxy)",
        "    nbthread 1 # 1 thread, IDs from 1 to 2, nbthread <number of CPU cores>",
        "    thread-groups 1",
        "    # declare threads",
        "    thread-group 1 1-1",
        "    # bind threads to cpu cores",
        "    cpu-map 1/all 0-0 # bind all threads to CPU 0 #syntax:cpu-map 1/1-<Number Of CPU Cores> 0-<Number of CPU Cores - 1>",
        "    # define logging",
        "    log /dev/log local0 info",
        "    #log /dev/log local0 emerg",
        "    #log /dev/log local1 alert",
        "    #log /dev/log local2 crit",
        "    #log /dev/log local3 err",
        "    #log /dev/log local4 warning",
        "    #log /dev/log local5 notice",
        "    #log /dev/log local6 info",
        "    #log /dev/log local7 debug",
        "    #Security Considerations",
        "    chroot /var/lib/haproxy #chroot statement pointing to a /var/lib/haproxy location",
        "    user haproxy # uid/user statement",
        "    group haproxy # gid/group statement",
        "    stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin",
        "    stats maxconn 20",
        "    stats timeout 30000",
        "    daemon",
        "    maxconn 192",
        "        ",
        "defaults",
        "    mode http",
        "    timeout connect 5000ms",
        "    timeout client 5000ms",
        "    timeout server 5000ms",
        "    errorfile 400 /etc/haproxy/errors/400.http",
        "    errorfile 403 /etc/haproxy/errors/403.http",
        "    errorfile 408 /etc/haproxy/errors/408.http",
        "    errorfile 500 /etc/haproxy/errors/500.http",
        "    errorfile 502 /etc/haproxy/errors/502.http",
        "    errorfile 503 /etc/haproxy/errors/503.http",
        "    errorfile 504 /etc/haproxy/errors/504.http",
        "",
        "frontend web_stats",
        "    mode http",
        "    bind *:80 ",
        "    http-request use-service prometheus-exporter if { path /metrics }",
        "    stats enable # enable stats page",
        "    stats uri /stats # stats uri",
        "    stats hide-version",
        "    stats refresh 1s",
        "    stats auth admin:uipassword",
        "",
        "frontend haproxy_frontend",
        "    log global",
        "    bind *:80  thread 1/all shards by-thread  #bind this proxy to threads 1 to 1 or all",
        "    mode http",
        "    option httplog",
        "    #option dontlog-normal",
        "    #option logasap",
        "    #define custom log-format",
        "    log-format \"%ci:%cp [%tr] %ft %b/%s %TR/%Tw/%Tc/%Tr/%Ta %ST %B %CC %CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r %[http_first_req] cpu_calls:%[cpu_calls] cpu_ns_tot:%[cpu_ns_tot] cpu_ns_avg:%[cpu_ns_avg] lat_ns_tot:%[lat_ns_tot] lat_ns_avg:%[lat_ns_avg]\"",
        "    default_backend haproxy_backend",
        "    ",
        "backend haproxy_backend",
        "    retry-on all-retryable-errors # This works when conn-failure, empty-response, junk-response, response-timeout, rtt-rejected, 500, 502, 503, and 504",
        "    retries 3",
        "             server rev1_devA 10.1.1.50:5000 check maxconn 64",
        "             server rev1_devB 10.1.1.54:5000 check maxconn 64",
        "             server rev1_devC 10.1.1.44:5000 check maxconn 64",
        "    "
    ]
}
2025-08-12 09:26:28,620 p=1384604 u=ubuntu n=ansible | RUNNING HANDLER [Restart HAProxy service] ************************************************************************************************************************************************************************************
2025-08-12 09:26:31,857 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:31,862 p=1384604 u=ubuntu n=ansible | RUNNING HANDLER [Restart rsyslog service] ************************************************************************************************************************************************************************************
2025-08-12 09:26:35,031 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:35,040 p=1384604 u=ubuntu n=ansible | PLAY [Install the Grafana Alloy Agent on HAproxy] ****************************************************************************************************************************************************************************
2025-08-12 09:26:35,045 p=1384604 u=ubuntu n=ansible | TASK [Gathering Facts] *******************************************************************************************************************************************************************************************************
2025-08-12 09:26:38,556 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 09:26:38,567 p=1384604 u=ubuntu n=ansible | TASK [Install the Grafana Alloy Agent] ***************************************************************************************************************************************************************************************
2025-08-12 09:26:51,395 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:51,400 p=1384604 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] ********************************************************************************************************************************************************************************
2025-08-12 09:26:54,093 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:54,099 p=1384604 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] **************************************************************************************************************************************************************************************
2025-08-12 09:26:54,114 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Tue 2025-08-12 09:26:50 UTC; 3s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 12074 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 40.4M",
        "     CGroup: /system.slice/alloy.service",
        "             └─12074 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.355278597Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=otel duration=11.966µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.355428982Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 duration=277.932691ms",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.361859588Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=69fe78",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.362604021Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.366876874Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.374516673Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.375211279Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.386299341Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff60994880ac38dde32d0c28710d317f",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.386769298Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff60994880ac38dde32d0c28710d317f duration=645.774µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.402831119Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 09:26:54,119 p=1384604 u=ubuntu n=ansible | TASK [DeployAlloy config file] ***********************************************************************************************************************************************************************************************
2025-08-12 09:26:59,161 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:26:59,166 p=1384604 u=ubuntu n=ansible | TASK [Restart the Grafana alloy service] *************************************************************************************************************************************************************************************
2025-08-12 09:27:02,165 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:27:02,172 p=1384604 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] ********************************************************************************************************************************************************************************
2025-08-12 09:27:04,872 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:27:04,877 p=1384604 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] **************************************************************************************************************************************************************************************
2025-08-12 09:27:04,893 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Tue 2025-08-12 09:27:01 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 13826 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 37.6M",
        "     CGroup: /system.slice/alloy.service",
        "             └─13826 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255715229Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=ui duration=10.469µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255735469Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=labelstore duration=7.158µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255747364Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 duration=124.689592ms",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.257077448Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.260801025Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.261190691Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.261548335Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.266951853Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=1d345fd1764bb59c233b2c26ea4a8ce2",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.26730332Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=1d345fd1764bb59c233b2c26ea4a8ce2 duration=443.49µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.313783464Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 09:27:04,898 p=1384604 u=ubuntu n=ansible | TASK [Check the Grafana alloy logs] ******************************************************************************************************************************************************************************************
2025-08-12 09:27:07,644 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:27:07,649 p=1384604 u=ubuntu n=ansible | TASK [Display the Grafana alloy logs] ****************************************************************************************************************************************************************************************
2025-08-12 09:27:07,667 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_logs_response.stdout_lines": [
        "-- Logs begin at Tue 2025-08-12 09:21:04 UTC, end at Tue 2025-08-12 09:27:07 UTC. --",
        "Aug 12 09:26:50 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.348874527Z level=info \"boringcrypto enabled\"=false",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.076080448Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.351941533Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.352182592Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.352410521Z level=info msg=\"running usage stats reporter\"",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.352595596Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=remotecfg duration=245.832722ms",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.352782417Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.35292979Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=http duration=52.709µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.353107564Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=cluster duration=1.315µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.353291197Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=livedebugging duration=11.682µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.353475528Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=ui duration=1.013µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.353650536Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=labelstore duration=8.815µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.353822074Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=loki.write.grafana_cloud_loki duration=1.248286ms",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.354002691Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.354272308Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=0",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.354449873Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=69fe78",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.354630797Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.354806844Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=prometheus.remote_write.metrics_service duration=23.79284ms",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.354947083Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=tracing duration=28.712µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.355100299Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=logging duration=6.246258ms",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.355278597Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 node_id=otel duration=11.966µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.355428982Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=b3b46547b9f1bdfef717ea6eafb83721 duration=277.932691ms",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.361859588Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=69fe78",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.362604021Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.366876874Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.374516673Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.375211279Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.386299341Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff60994880ac38dde32d0c28710d317f",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.386769298Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff60994880ac38dde32d0c28710d317f duration=645.774µs",
        "Aug 12 09:26:51 rev1-haproxy alloy[12074]: ts=2025-08-12T09:26:51.402831119Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 09:27:01 rev1-haproxy systemd[1]: Stopping Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines...",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: interrupt received",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.413080975Z level=info msg=\"http server closed\" service=http addr=127.0.0.1:12345 err=\"http: Server closed\"",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.414984448Z level=error msg=\"failed to start reporter\" err=\"context canceled\"",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.415781081Z level=info msg=\"node exited without error\" node=otel",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.415828181Z level=info msg=\"node exited without error\" node=labelstore",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.415859449Z level=info msg=\"node exited without error\" node=remotecfg",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.415938669Z level=info msg=\"node exited without error\" node=livedebugging",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.415961846Z level=info msg=\"node exited without error\" node=ui",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.416057307Z level=info msg=\"node exited without error\" node=loki.write.grafana_cloud_loki",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.419380965Z level=info msg=\"Stopping remote storage...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.419743367Z level=info msg=\"http server closed\" service=http addr=memory err=\"http: Server closed\"",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.4200239Z level=info msg=\"node exited without error\" node=http",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.420241679Z level=info msg=\"node exited without error\" node=cluster",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.420509403Z level=info msg=\"WAL watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=69fe78",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.420667172Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.420861421Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.42210473Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 09:27:01 rev1-haproxy alloy[12074]: ts=2025-08-12T09:27:01.422370063Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 12 09:27:01 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 12 09:27:01 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 09:27:01 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.132711316Z level=info \"boringcrypto enabled\"=false",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.12979744Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.135749066Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.136024643Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.136334187Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=246.383µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.136577201Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=loki.write.grafana_cloud_loki duration=1.090253ms",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.136794328Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=logging duration=4.08783ms",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.146756656Z level=info msg=\"running usage stats reporter\"",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.156958135Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.181391373Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=1",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.18211885Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=1",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.187364554Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=69fe78",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.187773826Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.188086057Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=prometheus.remote_write.metrics_service duration=51.082094ms",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.196890018Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=69fe78 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=69fe78",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.20689543Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=17.625688ms",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.207342054Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=tracing duration=45.111µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255423197Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=remotecfg duration=47.816394ms",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255560143Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.25557995Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=http duration=66.687µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255625924Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=cluster duration=27.57µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255647192Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=otel duration=5.267µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255674457Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=livedebugging duration=13.541µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255715229Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=ui duration=10.469µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255735469Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 node_id=labelstore duration=7.158µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.255747364Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=13eb13f30177e2bfedd38789fe6b01f9 duration=124.689592ms",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.257077448Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.260801025Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.261190691Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.261548335Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.266951853Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=1d345fd1764bb59c233b2c26ea4a8ce2",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.26730332Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=1d345fd1764bb59c233b2c26ea4a8ce2 duration=443.49µs",
        "Aug 12 09:27:02 rev1-haproxy alloy[13826]: ts=2025-08-12T09:27:02.313783464Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 09:27:07,673 p=1384604 u=ubuntu n=ansible | TASK [Check the Grafana alloy configuration file] ****************************************************************************************************************************************************************************
2025-08-12 09:27:10,289 p=1384604 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 09:27:10,295 p=1384604 u=ubuntu n=ansible | TASK [Display the Grafana alloy config] **************************************************************************************************************************************************************************************
2025-08-12 09:27:10,312 p=1384604 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_config_response.stdout_lines": [
        "remotecfg {",
        "  url            = \"https://fleet-management-prod-016.grafana.net\"",
        "  id             = \"rev1-haproxy\"",
        "  poll_frequency = \"60s\"",
        "",
        "  basic_auth {",
        "    username = \"1303247\"",
        "    password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "  }",
        "}",
        "",
        "prometheus.remote_write \"metrics_service\" {",
        "  endpoint {",
        "    url = \"https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push\"",
        "    basic_auth {",
        "      username = \"2530729\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "loki.write \"grafana_cloud_loki\" {",
        "  endpoint {",
        "    url = \"https://logs-prod-025.grafana.net/loki/api/v1/push\"",
        "    basic_auth {",
        "      username = \"1261041\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "discovery.relabel \"metrics_integrations_integrations_haproxy\" {",
        "  targets = [{",
        "    __address__ = \"127.0.0.1:80\",",
        "  }]",
        "",
        "  rule {",
        "    target_label = \"instance\"",
        "    replacement  = constants.hostname",
        "  }",
        "}",
        "",
        "prometheus.scrape \"metrics_integrations_integrations_haproxy\" {",
        "  targets    = discovery.relabel.metrics_integrations_integrations_haproxy.output",
        "  forward_to = [prometheus.remote_write.metrics_service.receiver]",
        "  job_name   = \"integrations/haproxy\"",
        "}"
    ]
}
2025-08-12 09:27:10,327 p=1384604 u=ubuntu n=ansible | [WARNING]: Found variable using reserved name: timeout

2025-08-12 09:27:10,328 p=1384604 u=ubuntu n=ansible | PLAY [Install snmp, snmpd, NGINX UDP load balancer config for SNMP] **********************************************************************************************************************************************************
2025-08-12 09:27:10,330 p=1384604 u=ubuntu n=ansible | TASK [Gathering Facts] *******************************************************************************************************************************************************************************************************
2025-08-12 09:27:10,350 p=1384604 u=ubuntu n=ansible | fatal: [rev1_NGINX]: UNREACHABLE! => {"changed": false, "msg": "Failed to connect to the host via ssh: OpenSSH_9.6p1 Ubuntu-3ubuntu13.11, OpenSSL 3.0.13 30 Jan 2024\r\ndebug1: Reading configuration data /home/ubuntu/.ssh/config\r\ndebug1: Reading configuration data /etc/ssh/ssh_config\r\ndebug1: /etc/ssh/ssh_config line 19: include /etc/ssh/ssh_config.d/*.conf matched no files\r\ndebug1: /etc/ssh/ssh_config line 21: Applying options for *\r\ndebug2: resolve_addr: could not resolve name  as address: Name or service not known\r\ndebug1: resolve_canonicalize: hostname  is an unrecognised address\r\ndebug2: resolving \"\" port 22\r\ndebug3: resolve_host: lookup :22\r\nssh: Could not resolve hostname : Name or service not known", "unreachable": true}
2025-08-12 09:27:10,350 p=1384604 u=ubuntu n=ansible | PLAY RECAP *******************************************************************************************************************************************************************************************************************
2025-08-12 09:27:10,351 p=1384604 u=ubuntu n=ansible | rev1_HAproxy               : ok=34   changed=19   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 09:27:10,351 p=1384604 u=ubuntu n=ansible | rev1_NGINX                 : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 09:27:10,351 p=1384604 u=ubuntu n=ansible | rev1_devA                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 09:27:10,351 p=1384604 u=ubuntu n=ansible | rev1_devB                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 09:27:10,351 p=1384604 u=ubuntu n=ansible | rev1_devC                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 10:44:17,771 p=1395911 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] **********************************************************************************************************************************
2025-08-12 10:44:17,779 p=1395911 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************
2025-08-12 10:44:30,115 p=1395911 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-12 10:44:30,222 p=1395911 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-12 10:44:30,231 p=1395911 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-12 10:44:30,255 p=1395911 u=ubuntu n=ansible | TASK [Install required packages] **********************************************************************************************************************************************************
2025-08-12 10:44:42,762 p=1395911 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3)
2025-08-12 10:44:43,313 p=1395911 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3)
2025-08-12 10:44:43,620 p=1395911 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3)
2025-08-12 10:44:54,771 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB] => (item=python3-pip)
2025-08-12 10:44:55,720 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA] => (item=python3-pip)
2025-08-12 10:44:56,181 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC] => (item=python3-pip)
2025-08-12 10:45:07,644 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmpd)
2025-08-12 10:45:08,728 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmpd)
2025-08-12 10:45:08,753 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmpd)
2025-08-12 10:45:17,014 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp)
2025-08-12 10:45:18,267 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp)
2025-08-12 10:45:18,875 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp)
2025-08-12 10:45:32,964 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp-mibs-downloader)
2025-08-12 10:45:33,835 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp-mibs-downloader)
2025-08-12 10:45:35,356 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp-mibs-downloader)
2025-08-12 10:45:42,846 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB] => (item=apache2-utils)
2025-08-12 10:45:43,257 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC] => (item=apache2-utils)
2025-08-12 10:45:44,895 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA] => (item=apache2-utils)
2025-08-12 10:45:51,804 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB] => (item=httperf)
2025-08-12 10:45:52,816 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC] => (item=httperf)
2025-08-12 10:45:53,965 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA] => (item=httperf)
2025-08-12 10:45:53,973 p=1395911 u=ubuntu n=ansible | TASK [Install Flask] **********************************************************************************************************************************************************************
2025-08-12 10:46:00,881 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:01,282 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:01,305 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:01,311 p=1395911 u=ubuntu n=ansible | TASK [Deploy the Flask application config for TCP Load Balancing] *************************************************************************************************************************
2025-08-12 10:46:09,401 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:09,569 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:10,246 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:10,252 p=1395911 u=ubuntu n=ansible | TASK [Start Flask app in background on port 5000] *****************************************************************************************************************************************
2025-08-12 10:46:14,771 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:15,240 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:15,268 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:15,273 p=1395911 u=ubuntu n=ansible | TASK [Check Flask app HTTP response on private IP] ****************************************************************************************************************************************
2025-08-12 10:46:19,387 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:19,974 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:20,095 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:20,101 p=1395911 u=ubuntu n=ansible | TASK [Display Flask app HTTP response on private IP] **************************************************************************************************************************************
2025-08-12 10:46:20,127 p=1395911 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "10:46:18 10.1.1.22:60956 -- 10.1.1.22 (rev1-deva) 94"
}
2025-08-12 10:46:20,127 p=1395911 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "10:46:19 10.1.1.60:47064 -- 10.1.1.60 (rev1-devb) 40"
}
2025-08-12 10:46:20,136 p=1395911 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "10:46:19 10.1.1.62:34970 -- 10.1.1.62 (rev1-devc) 63"
}
2025-08-12 10:46:20,141 p=1395911 u=ubuntu n=ansible | TASK [Remove the existing agent address lines] ********************************************************************************************************************************************
2025-08-12 10:46:24,700 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:24,912 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:24,965 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:24,970 p=1395911 u=ubuntu n=ansible | TASK [Configure agent address (0.0.0.0) for SNMPd to listen on all UDP interfaces] ********************************************************************************************************
2025-08-12 10:46:30,213 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:30,938 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:31,020 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:31,025 p=1395911 u=ubuntu n=ansible | TASK [Check snmpd config File] ************************************************************************************************************************************************************
2025-08-12 10:46:35,367 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:36,020 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:36,080 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:36,085 p=1395911 u=ubuntu n=ansible | TASK [Display snmpd configuration file] ***************************************************************************************************************************************************
2025-08-12 10:46:36,112 p=1395911 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003991",
        "end": "2025-08-12 10:46:34.586554",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 10:46:34.582563",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 10:46:36,114 p=1395911 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003040",
        "end": "2025-08-12 10:46:35.292559",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 10:46:35.289519",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 10:46:36,125 p=1395911 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003126",
        "end": "2025-08-12 10:46:35.388009",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 10:46:35.384883",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 10:46:36,130 p=1395911 u=ubuntu n=ansible | TASK [Restart SNMPD if agent address configuration is changed] ****************************************************************************************************************************
2025-08-12 10:46:41,336 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:41,537 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:41,547 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:41,554 p=1395911 u=ubuntu n=ansible | TASK [Test SNMPd with snmpget on 10.1.1.22] ***********************************************************************************************************************************************
2025-08-12 10:46:45,633 p=1395911 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 10:46:46,201 p=1395911 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 10:46:46,207 p=1395911 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 10:46:46,212 p=1395911 u=ubuntu n=ansible | TASK [Print SNMPd snmpget result] *********************************************************************************************************************************************************
2025-08-12 10:46:46,236 p=1395911 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-12 10:46:46,236 p=1395911 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-12 10:46:46,245 p=1395911 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-12 10:46:46,319 p=1395911 u=ubuntu n=ansible | PLAY [Set up HAProxy] *********************************************************************************************************************************************************************
2025-08-12 10:46:46,321 p=1395911 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************
2025-08-12 10:46:51,682 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:46:51,694 p=1395911 u=ubuntu n=ansible | TASK [Add HAProxy 2.9 PPA] ****************************************************************************************************************************************************************
2025-08-12 10:47:04,647 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:04,652 p=1395911 u=ubuntu n=ansible | TASK [Install HAProxy] ********************************************************************************************************************************************************************
2025-08-12 10:47:17,772 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:17,777 p=1395911 u=ubuntu n=ansible | TASK [Deploy stats web page password file] ************************************************************************************************************************************************
2025-08-12 10:47:21,464 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:21,470 p=1395911 u=ubuntu n=ansible | TASK [Read stats page password from file] *************************************************************************************************************************************************
2025-08-12 10:47:23,589 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:47:23,595 p=1395911 u=ubuntu n=ansible | TASK [Set up HAProxy stats secret variable] ***********************************************************************************************************************************************
2025-08-12 10:47:23,628 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:47:23,633 p=1395911 u=ubuntu n=ansible | TASK [Configure HAProxy] ******************************************************************************************************************************************************************
2025-08-12 10:47:27,403 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:27,409 p=1395911 u=ubuntu n=ansible | TASK [Deploy rsyslog 49-haproxy config file] **********************************************************************************************************************************************
2025-08-12 10:47:31,160 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:31,166 p=1395911 u=ubuntu n=ansible | TASK [Return 49_haproxy_conf to registered rsyslog_49_haproxy_conf] ***********************************************************************************************************************
2025-08-12 10:47:33,128 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:33,133 p=1395911 u=ubuntu n=ansible | TASK [debug] ******************************************************************************************************************************************************************************
2025-08-12 10:47:33,148 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "rsyslog_49_haproxy_conf.stdout_lines": [
        "# Create an additional socket in haproxy's chroot in order to allow logging via",
        "# /dev/log to chroot'ed HAProxy processes",
        "$AddUnixListenSocket /var/lib/haproxy/dev/log",
        "",
        "# Send HAProxy messages to a dedicated logfile",
        ":programname, startswith, \"haproxy\" {",
        "  /var/log/haproxy.log",
        "stop",
        "}"
    ]
}
2025-08-12 10:47:33,152 p=1395911 u=ubuntu n=ansible | TASK [Test HAProxy Configurations] ********************************************************************************************************************************************************
2025-08-12 10:47:35,210 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:35,215 p=1395911 u=ubuntu n=ansible | TASK [Display HAProxy config test result] *************************************************************************************************************************************************
2025-08-12 10:47:35,230 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_configs_test_result": {
        "changed": true,
        "cmd": [
            "haproxy",
            "-f",
            "/etc/haproxy/haproxy.cfg",
            "-c"
        ],
        "delta": "0:00:00.026996",
        "end": "2025-08-12 10:47:34.908086",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 10:47:34.881090",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "",
        "stdout_lines": []
    }
}
2025-08-12 10:47:35,237 p=1395911 u=ubuntu n=ansible | TASK [Test HAProxy is running] ************************************************************************************************************************************************************
2025-08-12 10:47:37,427 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:47:37,432 p=1395911 u=ubuntu n=ansible | TASK [Display the HAProxy service status] *************************************************************************************************************************************************
2025-08-12 10:47:37,447 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_service_status.state": "started"
}
2025-08-12 10:47:37,452 p=1395911 u=ubuntu n=ansible | TASK [Check HAProxy server status] ********************************************************************************************************************************************************
2025-08-12 10:47:39,577 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:39,582 p=1395911 u=ubuntu n=ansible | TASK [Display HAProxy server status] ******************************************************************************************************************************************************
2025-08-12 10:47:39,597 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "systemctl_haproxy_service_status.stdout_lines": [
        "● haproxy.service - HAProxy Load Balancer",
        "     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Tue 2025-08-12 10:47:14 UTC; 24s ago",
        "       Docs: man:haproxy(1)",
        "             file:/usr/share/doc/haproxy/configuration.txt.gz",
        "   Main PID: 4476 (haproxy)",
        "     Status: \"Ready.\"",
        "      Tasks: 2 (limit: 4588)",
        "     Memory: 39.8M",
        "     CGroup: /system.slice/haproxy.service",
        "             ├─4476 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "             └─4496 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "",
        "Aug 12 10:47:14 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 12 10:47:14 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : New worker (4496) forked",
        "Aug 12 10:47:14 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 12 10:47:14 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : Loading success."
    ]
}
2025-08-12 10:47:39,602 p=1395911 u=ubuntu n=ansible | TASK [Check HAProxy config errors via journalctl] *****************************************************************************************************************************************
2025-08-12 10:47:41,547 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:41,552 p=1395911 u=ubuntu n=ansible | TASK [Display HAProxy config errors] ******************************************************************************************************************************************************
2025-08-12 10:47:41,567 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_journalctl_logs.stdout_lines": [
        "-- Logs begin at Tue 2025-08-12 10:42:28 UTC, end at Tue 2025-08-12 10:47:41 UTC. --",
        "Aug 12 10:47:14 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 12 10:47:14 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : New worker (4496) forked",
        "Aug 12 10:47:14 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 12 10:47:14 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : Loading success."
    ]
}
2025-08-12 10:47:41,572 p=1395911 u=ubuntu n=ansible | TASK [Check the HAProxy configuration file] ***********************************************************************************************************************************************
2025-08-12 10:47:43,504 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:43,509 p=1395911 u=ubuntu n=ansible | TASK [Display HAProxy configuration file] *************************************************************************************************************************************************
2025-08-12 10:47:43,525 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_config_file.stdout_lines": [
        "global",
        "    profiling.tasks on #Enable HAProxy profiling (CPU time spent on processing a http request inside HAProxy)",
        "    nbthread 1 # 1 thread, IDs from 1 to 2, nbthread <number of CPU cores>",
        "    thread-groups 1",
        "    # declare threads",
        "    thread-group 1 1-1",
        "    # bind threads to cpu cores",
        "    cpu-map 1/all 0-0 # bind all threads to CPU 0 #syntax:cpu-map 1/1-<Number Of CPU Cores> 0-<Number of CPU Cores - 1>",
        "    # define logging",
        "    log /dev/log local0 debug",
        "    #log /dev/log local0 info",
        "    #log /dev/log local0 emerg",
        "    #log /dev/log local1 alert",
        "    #log /dev/log local2 crit",
        "    #log /dev/log local3 err",
        "    #log /dev/log local4 warning",
        "    #log /dev/log local5 notice",
        "    #log /dev/log local6 info",
        "    #log /dev/log local7 debug",
        "    #Security Considerations",
        "    chroot /var/lib/haproxy #chroot statement pointing to a /var/lib/haproxy location",
        "    user haproxy # uid/user statement",
        "    group haproxy # gid/group statement",
        "    stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin",
        "    stats maxconn 20",
        "    stats timeout 30000",
        "    daemon",
        "    maxconn 3000",
        "        ",
        "defaults",
        "    mode http",
        "    timeout connect 5s",
        "    timeout client 50s",
        "    timeout server 50s",
        "    errorfile 400 /etc/haproxy/errors/400.http",
        "    errorfile 403 /etc/haproxy/errors/403.http",
        "    errorfile 408 /etc/haproxy/errors/408.http",
        "    errorfile 500 /etc/haproxy/errors/500.http",
        "    errorfile 502 /etc/haproxy/errors/502.http",
        "    errorfile 503 /etc/haproxy/errors/503.http",
        "    errorfile 504 /etc/haproxy/errors/504.http",
        "",
        "frontend web_stats",
        "    mode http",
        "    bind *:80 ",
        "    http-request use-service prometheus-exporter if { path /metrics }",
        "    stats enable # enable stats page",
        "    stats uri /stats # stats uri",
        "    stats hide-version",
        "    stats refresh 1s",
        "    stats auth admin:uipassword",
        "",
        "frontend haproxy_frontend",
        "    log global",
        "    bind *:80  thread 1/all shards by-thread  #bind this proxy to threads 1 to 1 or all",
        "    mode http",
        "    option httplog",
        "    #option dontlog-normal",
        "    #option logasap",
        "    #define custom log-format",
        "    log-format \"%ci:%cp [%tr] %ft %b/%s %TR/%Tw/%Tc/%Tr/%Ta %ST %B %CC %CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r %[http_first_req] cpu_calls:%[cpu_calls] cpu_ns_tot:%[cpu_ns_tot] cpu_ns_avg:%[cpu_ns_avg] lat_ns_tot:%[lat_ns_tot] lat_ns_avg:%[lat_ns_avg]\"",
        "    default_backend haproxy_backend",
        "    ",
        "backend haproxy_backend",
        "    retry-on all-retryable-errors # This works when conn-failure, empty-response, junk-response, response-timeout, rtt-rejected, 500, 502, 503, and 504",
        "    retries 3",
        "             server rev1_devA 10.1.1.22:5000 check maxconn 1000",
        "             server rev1_devB 10.1.1.60:5000 check maxconn 1000",
        "             server rev1_devC 10.1.1.62:5000 check maxconn 1000",
        "    "
    ]
}
2025-08-12 10:47:43,537 p=1395911 u=ubuntu n=ansible | RUNNING HANDLER [Restart HAProxy service] *************************************************************************************************************************************************
2025-08-12 10:47:45,844 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:45,848 p=1395911 u=ubuntu n=ansible | RUNNING HANDLER [Restart rsyslog service] *************************************************************************************************************************************************
2025-08-12 10:47:48,081 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:47:48,090 p=1395911 u=ubuntu n=ansible | PLAY [Install the Grafana Alloy Agent on HAproxy] *****************************************************************************************************************************************
2025-08-12 10:47:48,095 p=1395911 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************
2025-08-12 10:47:50,824 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:47:50,834 p=1395911 u=ubuntu n=ansible | TASK [Install the Grafana Alloy Agent] ****************************************************************************************************************************************************
2025-08-12 10:48:00,928 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:48:00,933 p=1395911 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *********************************************************************************************************************************************
2025-08-12 10:48:03,088 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:48:03,093 p=1395911 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] ***************************************************************************************************************************************************
2025-08-12 10:48:03,108 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Tue 2025-08-12 10:47:59 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 11754 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 40.5M",
        "     CGroup: /system.slice/alloy.service",
        "             └─11754 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034266599Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=ui duration=2.901µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034284824Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=labelstore duration=7.938µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034297368Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 duration=251.577422ms",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.035486462Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.037811862Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.038105872Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.038460573Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.047788224Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff5b6546ef8dffedf93948895c394b25",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.047830894Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff5b6546ef8dffedf93948895c394b25 duration=95.678µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.061255953Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 10:48:03,113 p=1395911 u=ubuntu n=ansible | TASK [DeployAlloy config file] ************************************************************************************************************************************************************
2025-08-12 10:48:07,175 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:48:07,180 p=1395911 u=ubuntu n=ansible | TASK [Restart the Grafana alloy service] **************************************************************************************************************************************************
2025-08-12 10:48:09,441 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:48:09,448 p=1395911 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *********************************************************************************************************************************************
2025-08-12 10:48:11,526 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:48:11,532 p=1395911 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] ***************************************************************************************************************************************************
2025-08-12 10:48:11,549 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Tue 2025-08-12 10:48:08 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 13529 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 38.5M",
        "     CGroup: /system.slice/alloy.service",
        "             └─13529 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.419130243Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=cluster duration=5.627µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.419316798Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=ui duration=23.292µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.419490547Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae duration=55.812976ms",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.420225394Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.420770193Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.423432271Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.423781022Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.431531584Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=e69b7a0ce906e96d022ba28939521462",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.431728264Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=e69b7a0ce906e96d022ba28939521462 duration=301.521µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.446576751Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 10:48:11,554 p=1395911 u=ubuntu n=ansible | TASK [Check the Grafana alloy logs] *******************************************************************************************************************************************************
2025-08-12 10:48:13,808 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:48:13,813 p=1395911 u=ubuntu n=ansible | TASK [Display the Grafana alloy logs] *****************************************************************************************************************************************************
2025-08-12 10:48:13,830 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_logs_response.stdout_lines": [
        "-- Logs begin at Tue 2025-08-12 10:42:28 UTC, end at Tue 2025-08-12 10:48:13 UTC. --",
        "Aug 12 10:47:59 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.782936772Z level=info \"boringcrypto enabled\"=false",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.781557343Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.795099371Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.795301907Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.795494745Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=tracing duration=7.735µs",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.795668518Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=logging duration=12.726708ms",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.795884732Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=otel duration=3.388µs",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.811995435Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=loki.write.grafana_cloud_loki duration=15.955422ms",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.830938065Z level=info msg=\"running usage stats reporter\"",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.834421045Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.849962711Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=0",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.863070173Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f77065",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.867333503Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.867530564Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=prometheus.remote_write.metrics_service duration=55.342155ms",
        "Aug 12 10:48:00 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:00.870969687Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f77065",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034083669Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=remotecfg duration=166.326674ms",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034175906Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034190524Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=http duration=50.499µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034209889Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=cluster duration=4.428µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034247807Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=livedebugging duration=23.895µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034266599Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=ui duration=2.901µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034284824Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 node_id=labelstore duration=7.938µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.034297368Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=a64b4225be97ac74ae656dd4bc4c6bf3 duration=251.577422ms",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.035486462Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.037811862Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.038105872Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.038460573Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.047788224Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff5b6546ef8dffedf93948895c394b25",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.047830894Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ff5b6546ef8dffedf93948895c394b25 duration=95.678µs",
        "Aug 12 10:48:01 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:01.061255953Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: interrupt received",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.916160627Z level=info msg=\"node exited without error\" node=otel",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.916326065Z level=error msg=\"failed to start reporter\" err=\"context canceled\"",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.917684224Z level=info msg=\"Stopping remote storage...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 10:48:08 rev1-haproxy systemd[1]: Stopping Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines...",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.918722701Z level=info msg=\"node exited without error\" node=remotecfg",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.918959349Z level=info msg=\"node exited without error\" node=livedebugging",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.922467456Z level=info msg=\"http server closed\" service=http addr=127.0.0.1:12345 err=\"http: Server closed\"",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.923296891Z level=info msg=\"node exited without error\" node=ui",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.923695516Z level=info msg=\"node exited without error\" node=labelstore",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.923826368Z level=info msg=\"node exited without error\" node=loki.write.grafana_cloud_loki",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.923997471Z level=info msg=\"WAL watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f77065",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.924819863Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.925029281Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.925706042Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.926096407Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.924044559Z level=info msg=\"http server closed\" service=http addr=memory err=\"http: Server closed\"",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.926448059Z level=info msg=\"node exited without error\" node=http",
        "Aug 12 10:48:08 rev1-haproxy alloy[11754]: ts=2025-08-12T10:48:08.924171593Z level=info msg=\"node exited without error\" node=cluster",
        "Aug 12 10:48:08 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 12 10:48:08 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 10:48:08 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.380899253Z level=info \"boringcrypto enabled\"=false",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.36279294Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.382782511Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.382982441Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.383175262Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=livedebugging duration=23.744µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.383318027Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=loki.write.grafana_cloud_loki duration=714.267µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.383475818Z level=info msg=\"running usage stats reporter\"",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.383615349Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.38390515Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=1",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.384071294Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=1",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.384225125Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f77065",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.384373654Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.384514457Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=prometheus.remote_write.metrics_service duration=16.232924ms",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.384659943Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=logging duration=3.764894ms",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.384841081Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=otel duration=4.269µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.38500367Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=labelstore duration=11.026µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.385321238Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=166.967µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.387288649Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f77065 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f77065",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.391405464Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=5.92843ms",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.39159489Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=tracing duration=10.922µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.418504454Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=remotecfg duration=26.714518ms",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.418786537Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.418945361Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=http duration=175.917µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.419130243Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=cluster duration=5.627µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.419316798Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae node_id=ui duration=23.292µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.419490547Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=71c364bb38b4af9c8cf7476ff27827ae duration=55.812976ms",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.420225394Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.420770193Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.423432271Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.423781022Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.431531584Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=e69b7a0ce906e96d022ba28939521462",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.431728264Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=e69b7a0ce906e96d022ba28939521462 duration=301.521µs",
        "Aug 12 10:48:09 rev1-haproxy alloy[13529]: ts=2025-08-12T10:48:09.446576751Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 10:48:13,835 p=1395911 u=ubuntu n=ansible | TASK [Check the Grafana alloy configuration file] *****************************************************************************************************************************************
2025-08-12 10:48:15,895 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:48:15,901 p=1395911 u=ubuntu n=ansible | TASK [Display the Grafana alloy config] ***************************************************************************************************************************************************
2025-08-12 10:48:15,916 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_config_response.stdout_lines": [
        "remotecfg {",
        "  url            = \"https://fleet-management-prod-016.grafana.net\"",
        "  id             = \"rev1-haproxy\"",
        "  poll_frequency = \"60s\"",
        "",
        "  basic_auth {",
        "    username = \"1303247\"",
        "    password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "  }",
        "}",
        "",
        "prometheus.remote_write \"metrics_service\" {",
        "  endpoint {",
        "    url = \"https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push\"",
        "    basic_auth {",
        "      username = \"2530729\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "loki.write \"grafana_cloud_loki\" {",
        "  endpoint {",
        "    url = \"https://logs-prod-025.grafana.net/loki/api/v1/push\"",
        "    basic_auth {",
        "      username = \"1261041\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "discovery.relabel \"metrics_integrations_integrations_haproxy\" {",
        "  targets = [{",
        "    __address__ = \"127.0.0.1:80\",",
        "  }]",
        "",
        "  rule {",
        "    target_label = \"instance\"",
        "    replacement  = constants.hostname",
        "  }",
        "}",
        "",
        "prometheus.scrape \"metrics_integrations_integrations_haproxy\" {",
        "  targets    = discovery.relabel.metrics_integrations_integrations_haproxy.output",
        "  forward_to = [prometheus.remote_write.metrics_service.receiver]",
        "  job_name   = \"integrations/haproxy\"",
        "}"
    ]
}
2025-08-12 10:48:15,932 p=1395911 u=ubuntu n=ansible | [WARNING]: Found variable using reserved name: timeout

2025-08-12 10:48:15,933 p=1395911 u=ubuntu n=ansible | PLAY [Install snmp, snmpd, NGINX UDP load balancer config for SNMP] ***********************************************************************************************************************
2025-08-12 10:48:15,935 p=1395911 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************
2025-08-12 10:48:20,677 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 10:48:20,690 p=1395911 u=ubuntu n=ansible | TASK [Install required packages] **********************************************************************************************************************************************************
2025-08-12 10:48:39,318 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=nginx)
2025-08-12 10:48:49,153 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmpd)
2025-08-12 10:48:55,624 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp)
2025-08-12 10:49:07,186 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp-mibs-downloader)
2025-08-12 10:49:07,193 p=1395911 u=ubuntu n=ansible | TASK [Deploy NGINX stream config for SNMP UDP load balancing] *****************************************************************************************************************************
2025-08-12 10:49:10,688 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-12 10:49:10,692 p=1395911 u=ubuntu n=ansible | TASK [Check nginx is running] *************************************************************************************************************************************************************
2025-08-12 10:49:12,759 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 10:49:12,764 p=1395911 u=ubuntu n=ansible | TASK [display nginx status] ***************************************************************************************************************************************************************
2025-08-12 10:49:12,786 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX] => {
    "nginx_running_status": {
        "changed": false,
        "failed": false,
        "name": "nginx",
        "state": "started",
        "status": {
            "ActiveEnterTimestamp": "Tue 2025-08-12 10:48:36 UTC",
            "ActiveEnterTimestampMonotonic": "361020962",
            "ActiveExitTimestampMonotonic": "0",
            "ActiveState": "active",
            "After": "network.target basic.target systemd-journald.socket sysinit.target system.slice",
            "AllowIsolate": "no",
            "AllowedCPUs": "",
            "AllowedMemoryNodes": "",
            "AmbientCapabilities": "",
            "AssertResult": "yes",
            "AssertTimestamp": "Tue 2025-08-12 10:48:36 UTC",
            "AssertTimestampMonotonic": "360966620",
            "Before": "multi-user.target shutdown.target",
            "BlockIOAccounting": "no",
            "BlockIOWeight": "[not set]",
            "CPUAccounting": "no",
            "CPUAffinity": "",
            "CPUAffinityFromNUMA": "no",
            "CPUQuotaPerSecUSec": "infinity",
            "CPUQuotaPeriodUSec": "infinity",
            "CPUSchedulingPolicy": "0",
            "CPUSchedulingPriority": "0",
            "CPUSchedulingResetOnFork": "no",
            "CPUShares": "[not set]",
            "CPUUsageNSec": "[not set]",
            "CPUWeight": "[not set]",
            "CacheDirectoryMode": "0755",
            "CanIsolate": "no",
            "CanReload": "yes",
            "CanStart": "yes",
            "CanStop": "yes",
            "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read",
            "CleanResult": "success",
            "CollectMode": "inactive",
            "ConditionResult": "yes",
            "ConditionTimestamp": "Tue 2025-08-12 10:48:36 UTC",
            "ConditionTimestampMonotonic": "360966620",
            "ConfigurationDirectoryMode": "0755",
            "Conflicts": "shutdown.target",
            "ControlGroup": "/system.slice/nginx.service",
            "ControlPID": "0",
            "DefaultDependencies": "yes",
            "DefaultMemoryLow": "0",
            "DefaultMemoryMin": "0",
            "Delegate": "no",
            "Description": "A high performance web server and a reverse proxy server",
            "DevicePolicy": "auto",
            "Documentation": "man:nginx(8)",
            "DynamicUser": "no",
            "EffectiveCPUs": "",
            "EffectiveMemoryNodes": "",
            "ExecMainCode": "0",
            "ExecMainExitTimestampMonotonic": "0",
            "ExecMainPID": "3480",
            "ExecMainStartTimestamp": "Tue 2025-08-12 10:48:36 UTC",
            "ExecMainStartTimestampMonotonic": "361020946",
            "ExecMainStatus": "0",
            "ExecReload": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecReloadEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStart": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPre": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPreEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStop": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStopEx": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "FailureAction": "none",
            "FileDescriptorStoreMax": "0",
            "FinalKillSignal": "9",
            "FragmentPath": "/lib/systemd/system/nginx.service",
            "GID": "[not set]",
            "GuessMainPID": "yes",
            "IOAccounting": "no",
            "IOReadBytes": "18446744073709551615",
            "IOReadOperations": "18446744073709551615",
            "IOSchedulingClass": "0",
            "IOSchedulingPriority": "0",
            "IOWeight": "[not set]",
            "IOWriteBytes": "18446744073709551615",
            "IOWriteOperations": "18446744073709551615",
            "IPAccounting": "no",
            "IPEgressBytes": "[no data]",
            "IPEgressPackets": "[no data]",
            "IPIngressBytes": "[no data]",
            "IPIngressPackets": "[no data]",
            "Id": "nginx.service",
            "IgnoreOnIsolate": "no",
            "IgnoreSIGPIPE": "yes",
            "InactiveEnterTimestampMonotonic": "0",
            "InactiveExitTimestamp": "Tue 2025-08-12 10:48:36 UTC",
            "InactiveExitTimestampMonotonic": "360967739",
            "InvocationID": "34a10496c356405498b04c2f36ff311d",
            "JobRunningTimeoutUSec": "infinity",
            "JobTimeoutAction": "none",
            "JobTimeoutUSec": "infinity",
            "KeyringMode": "private",
            "KillMode": "mixed",
            "KillSignal": "15",
            "LimitAS": "infinity",
            "LimitASSoft": "infinity",
            "LimitCORE": "infinity",
            "LimitCORESoft": "0",
            "LimitCPU": "infinity",
            "LimitCPUSoft": "infinity",
            "LimitDATA": "infinity",
            "LimitDATASoft": "infinity",
            "LimitFSIZE": "infinity",
            "LimitFSIZESoft": "infinity",
            "LimitLOCKS": "infinity",
            "LimitLOCKSSoft": "infinity",
            "LimitMEMLOCK": "65536",
            "LimitMEMLOCKSoft": "65536",
            "LimitMSGQUEUE": "819200",
            "LimitMSGQUEUESoft": "819200",
            "LimitNICE": "0",
            "LimitNICESoft": "0",
            "LimitNOFILE": "524288",
            "LimitNOFILESoft": "1024",
            "LimitNPROC": "15295",
            "LimitNPROCSoft": "15295",
            "LimitRSS": "infinity",
            "LimitRSSSoft": "infinity",
            "LimitRTPRIO": "0",
            "LimitRTPRIOSoft": "0",
            "LimitRTTIME": "infinity",
            "LimitRTTIMESoft": "infinity",
            "LimitSIGPENDING": "15295",
            "LimitSIGPENDINGSoft": "15295",
            "LimitSTACK": "infinity",
            "LimitSTACKSoft": "8388608",
            "LoadState": "loaded",
            "LockPersonality": "no",
            "LogLevelMax": "-1",
            "LogRateLimitBurst": "0",
            "LogRateLimitIntervalUSec": "0",
            "LogsDirectoryMode": "0755",
            "MainPID": "3480",
            "MemoryAccounting": "yes",
            "MemoryCurrent": "5455872",
            "MemoryDenyWriteExecute": "no",
            "MemoryHigh": "infinity",
            "MemoryLimit": "infinity",
            "MemoryLow": "0",
            "MemoryMax": "infinity",
            "MemoryMin": "0",
            "MemorySwapMax": "infinity",
            "MountAPIVFS": "no",
            "MountFlags": "",
            "NFileDescriptorStore": "0",
            "NRestarts": "0",
            "NUMAMask": "",
            "NUMAPolicy": "n/a",
            "Names": "nginx.service",
            "NeedDaemonReload": "no",
            "Nice": "0",
            "NoNewPrivileges": "no",
            "NonBlocking": "no",
            "NotifyAccess": "none",
            "OOMPolicy": "stop",
            "OOMScoreAdjust": "0",
            "OnFailureJobMode": "replace",
            "PIDFile": "/run/nginx.pid",
            "Perpetual": "no",
            "PrivateDevices": "no",
            "PrivateMounts": "no",
            "PrivateNetwork": "no",
            "PrivateTmp": "no",
            "PrivateUsers": "no",
            "ProtectControlGroups": "no",
            "ProtectHome": "no",
            "ProtectHostname": "no",
            "ProtectKernelLogs": "no",
            "ProtectKernelModules": "no",
            "ProtectKernelTunables": "no",
            "ProtectSystem": "no",
            "RefuseManualStart": "no",
            "RefuseManualStop": "no",
            "ReloadResult": "success",
            "RemainAfterExit": "no",
            "RemoveIPC": "no",
            "Requires": "system.slice sysinit.target",
            "Restart": "no",
            "RestartKillSignal": "15",
            "RestartUSec": "100ms",
            "RestrictNamespaces": "no",
            "RestrictRealtime": "no",
            "RestrictSUIDSGID": "no",
            "Result": "success",
            "RootDirectoryStartOnly": "no",
            "RuntimeDirectoryMode": "0755",
            "RuntimeDirectoryPreserve": "no",
            "RuntimeMaxUSec": "infinity",
            "SameProcessGroup": "no",
            "SecureBits": "0",
            "SendSIGHUP": "no",
            "SendSIGKILL": "yes",
            "Slice": "system.slice",
            "StandardError": "inherit",
            "StandardInput": "null",
            "StandardInputData": "",
            "StandardOutput": "journal",
            "StartLimitAction": "none",
            "StartLimitBurst": "5",
            "StartLimitIntervalUSec": "10s",
            "StartupBlockIOWeight": "[not set]",
            "StartupCPUShares": "[not set]",
            "StartupCPUWeight": "[not set]",
            "StartupIOWeight": "[not set]",
            "StateChangeTimestamp": "Tue 2025-08-12 10:48:36 UTC",
            "StateChangeTimestampMonotonic": "361020962",
            "StateDirectoryMode": "0755",
            "StatusErrno": "0",
            "StopWhenUnneeded": "no",
            "SubState": "running",
            "SuccessAction": "none",
            "SyslogFacility": "3",
            "SyslogLevel": "6",
            "SyslogLevelPrefix": "yes",
            "SyslogPriority": "30",
            "SystemCallErrorNumber": "0",
            "TTYReset": "no",
            "TTYVHangup": "no",
            "TTYVTDisallocate": "no",
            "TasksAccounting": "yes",
            "TasksCurrent": "2",
            "TasksMax": "4588",
            "TimeoutAbortUSec": "5s",
            "TimeoutCleanUSec": "infinity",
            "TimeoutStartUSec": "1min 30s",
            "TimeoutStopUSec": "5s",
            "TimerSlackNSec": "50000",
            "Transient": "no",
            "Type": "forking",
            "UID": "[not set]",
            "UMask": "0022",
            "UnitFilePreset": "enabled",
            "UnitFileState": "enabled",
            "UtmpMode": "init",
            "WantedBy": "multi-user.target",
            "WatchdogSignal": "6",
            "WatchdogTimestampMonotonic": "0",
            "WatchdogUSec": "0"
        }
    }
}
2025-08-12 10:49:12,798 p=1395911 u=ubuntu n=ansible | RUNNING HANDLER [Reload NGINX] ************************************************************************************************************************************************************
2025-08-12 10:49:14,767 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-12 10:49:14,778 p=1395911 u=ubuntu n=ansible | PLAY [Test HAProxy (http), HAProxy Web stats (STATS)+ Metrics (PROMEX) and HAProxy logs] **************************************************************************************************
2025-08-12 10:49:14,784 p=1395911 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************
2025-08-12 10:49:17,656 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:49:17,667 p=1395911 u=ubuntu n=ansible | TASK [Gather HAProxy server public IP address] ********************************************************************************************************************************************
2025-08-12 10:49:20,138 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:49:20,148 p=1395911 u=ubuntu n=ansible | TASK [Send HTTP request to HAProxy and collect response] **********************************************************************************************************************************
2025-08-12 10:49:30,786 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0)
2025-08-12 10:49:37,038 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1)
2025-08-12 10:49:39,158 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2)
2025-08-12 10:49:39,164 p=1395911 u=ubuntu n=ansible | TASK [Display the HAProxy response] *******************************************************************************************************************************************************
2025-08-12 10:49:39,182 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "10:49:30 10.1.1.10:58652 -- 10.1.1.22 (rev1-deva) 57\n",
    "item": 0
}
2025-08-12 10:49:39,185 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "10:49:36 10.1.1.10:36376 -- 10.1.1.60 (rev1-devb) 47\n",
    "item": 1
}
2025-08-12 10:49:39,189 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "10:49:38 10.1.1.10:47442 -- 10.1.1.62 (rev1-devc) 15\n",
    "item": 2
}
2025-08-12 10:49:39,195 p=1395911 u=ubuntu n=ansible | TASK [Send HTTP requests to HAProxy stats page and collect responses] *********************************************************************************************************************
2025-08-12 10:49:45,396 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 10:49:45,402 p=1395911 u=ubuntu n=ansible | TASK [Display the stats response content] *************************************************************************************************************************************************
2025-08-12 10:49:45,417 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_stats_response.content": "# pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,agent_status,agent_code,agent_duration,check_desc,agent_desc,check_rise,check_fall,check_health,agent_rise,agent_fall,agent_health,addr,cookie,mode,algo,conn_rate,conn_rate_max,conn_tot,intercepted,dcon,dses,wrew,connect,reuse,cache_lookups,cache_hits,srv_icur,src_ilim,qtime_max,ctime_max,rtime_max,ttime_max,eint,idle_conn_cur,safe_conn_cur,used_conn_cur,need_conn_est,uweight,agg_server_status,agg_server_check_status,agg_check_status,srid,sess_other,h1sess,h2sess,h3sess,req_other,h1req,h2req,h3req,proto,-,ssl_sess,ssl_reused_sess,ssl_failed_handshake,quic_rxbuf_full,quic_dropped_pkt,quic_dropped_pkt_bufoverrun,quic_dropped_parsing_pkt,quic_socket_full,quic_sendto_err,quic_sendto_err_unknwn,quic_sent_pkt,quic_lost_pkt,quic_too_short_dgram,quic_retry_sent,quic_retry_validated,quic_retry_error,quic_half_open_conn,quic_hdshk_fail,quic_stless_rst_sent,quic_conn_migration_done,quic_transp_err_no_error,quic_transp_err_internal_error,quic_transp_err_connection_refused,quic_transp_err_flow_control_error,quic_transp_err_stream_limit_error,quic_transp_err_stream_state_error,quic_transp_err_final_size_error,quic_transp_err_frame_encoding_error,quic_transp_err_transport_parameter_error,quic_transp_err_connection_id_limit,quic_transp_err_protocol_violation_error,quic_transp_err_invalid_token,quic_transp_err_application_error,quic_transp_err_crypto_buffer_exceeded,quic_transp_err_key_update_error,quic_transp_err_aead_limit_reached,quic_transp_err_no_viable_path,quic_transp_err_crypto_error,quic_transp_err_unknown_error,quic_data_blocked,quic_stream_data_blocked,quic_streams_blocked_bidi,quic_streams_blocked_uni,h3_data,h3_headers,h3_cancel_push,h3_push_promise,h3_max_push_id,h3_goaway,h3_settings,h3_no_error,h3_general_protocol_error,h3_internal_error,h3_stream_creation_error,h3_closed_critical_stream,h3_frame_unexpected,h3_frame_error,h3_excessive_load,h3_id_error,h3_settings_error,h3_missing_settings,h3_request_rejected,h3_request_cancelled,h3_request_incomplete,h3_message_error,h3_connect_error,h3_version_fallback,pack_decompression_failed,qpack_encoder_stream_error,qpack_decoder_stream_error,h2_headers_rcvd,h2_data_rcvd,h2_settings_rcvd,h2_rst_stream_rcvd,h2_goaway_rcvd,h2_detected_conn_protocol_errors,h2_detected_strm_protocol_errors,h2_rst_stream_resp,h2_goaway_resp,h2_open_connections,h2_backend_open_streams,h2_total_connections,h2_backend_total_streams,h1_open_connections,h1_open_streams,h1_total_connections,h1_total_streams,h1_bytes_in,h1_bytes_out,h1_spliced_bytes_in,h1_spliced_bytes_out,\nweb_stats,FRONTEND,,,2,2,3000,5,592,62667,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,1,0,1,,,,0,1,0,0,3,0,,1,1,5,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,1,1,5,2,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,5,0,0,0,5,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,1,5,5,817,62635,0,0,\nhaproxy_frontend,FRONTEND,,,0,1,3000,4,452,986,0,0,0,,,,,OPEN,,,,,,,,,1,3,0,,,,0,0,0,1,,,,0,3,0,1,0,0,,0,1,4,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,4,0,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,4,0,0,0,4,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,520,1432,0,0,\nhaproxy_backend,rev1_devA,0,0,0,1,1000,2,252,574,,0,,0,0,0,0,UP,1,1,0,0,0,120,0,,1,4,1,,2,,2,0,,1,L4OK,,0,0,1,0,1,0,0,,,,2,0,0,,,,,5,,,0,0,3,3,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,2,0,,,0,,0,0,3,4,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devB,0,0,0,1,1000,1,100,206,,0,,0,0,0,0,UP,1,1,0,0,0,120,0,,1,4,2,,1,,2,0,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,9,,,0,0,3,4,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,3,4,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devC,0,0,0,1,1000,1,100,206,,0,,0,0,0,0,UP,1,1,0,0,0,120,0,,1,4,3,,1,,2,0,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,7,,,0,0,3,3,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,3,3,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,BACKEND,0,0,0,1,300,4,452,986,0,0,,0,0,0,0,UP,3,3,0,,0,120,0,,1,4,0,,4,,1,0,,1,,,,0,3,0,1,0,0,,,,4,0,0,0,0,0,0,5,,,0,0,3,4,,,,,,,,,,,,,,http,,,,,,,,0,4,0,0,0,,,0,0,3,4,0,,,,,3,0,0,0,,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,1432,444,0,0,\n"
}
2025-08-12 10:49:45,422 p=1395911 u=ubuntu n=ansible | TASK [Test the HAProxy metrics (promex) path] *********************************************************************************************************************************************
2025-08-12 10:49:47,450 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:49:47,456 p=1395911 u=ubuntu n=ansible | TASK [Display the HAProxy metrics (promex) response content] ******************************************************************************************************************************
2025-08-12 10:49:47,471 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_metrics_path_result.stdout_lines": [
        "<!doctype html>",
        "<html lang=en>",
        "<title>404 Not Found</title>",
        "<h1>Not Found</h1>",
        "<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>"
    ]
}
2025-08-12 10:49:47,478 p=1395911 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] ******************************************************************************************************************************************************
2025-08-12 10:49:49,588 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:49:49,593 p=1395911 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] ******************************************************************************************************************************************************
2025-08-12 10:49:49,608 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : path to executable is /usr/sbin/haproxy",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [WARNING]  (4476) : Exiting Master process...",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [ALERT]    (4476) : Current worker (4496) exited with code 143 (Terminated)",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [WARNING]  (4476) : All workers exited. Exiting... (0)",
        "Aug 12 10:47:45 rev1-haproxy haproxy[10236]: [NOTICE]   (10236) : New worker (10238) forked",
        "Aug 12 10:47:45 rev1-haproxy haproxy[10236]: [NOTICE]   (10236) : Loading success.",
        "Aug 12 10:49:30 rev1-haproxy haproxy[10238]: 188.240.223.140:50711 [12/Aug/2025:10:49:30.461] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/3/4 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13267 cpu_ns_avg:3316 lat_ns_tot:3976 lat_ns_avg:994",
        "Aug 12 10:49:36 rev1-haproxy haproxy[10238]: 188.240.223.140:60726 [12/Aug/2025:10:49:36.726] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/3/4 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13441 cpu_ns_avg:3360 lat_ns_tot:5817 lat_ns_avg:1454",
        "Aug 12 10:49:38 rev1-haproxy haproxy[10238]: 188.240.223.140:34708 [12/Aug/2025:10:49:38.835] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/3/3 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14863 cpu_ns_avg:3715 lat_ns_tot:3763 lat_ns_avg:940",
        "Aug 12 10:49:40 rev1-haproxy haproxy[10238]: 188.240.223.140:47631 [12/Aug/2025:10:49:40.940] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 404 368 - - ---- 2/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12130 cpu_ns_avg:3032 lat_ns_tot:3920 lat_ns_avg:980",
        "Aug 12 10:49:47 rev1-haproxy haproxy[10238]: 188.240.223.140:5227 [12/Aug/2025:10:49:47.139] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/4/4 404 368 - - ---- 2/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:7729 cpu_ns_avg:1932 lat_ns_tot:3724 lat_ns_avg:931"
    ]
}
2025-08-12 10:49:49,613 p=1395911 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] ******************************************************************************************************************************************************
2025-08-12 10:49:51,682 p=1395911 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 10:49:51,687 p=1395911 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] ******************************************************************************************************************************************************
2025-08-12 10:49:51,703 p=1395911 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [NOTICE]   (4476) : path to executable is /usr/sbin/haproxy",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [WARNING]  (4476) : Exiting Master process...",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [ALERT]    (4476) : Current worker (4496) exited with code 143 (Terminated)",
        "Aug 12 10:47:45 rev1-haproxy haproxy[4476]: [WARNING]  (4476) : All workers exited. Exiting... (0)",
        "Aug 12 10:47:45 rev1-haproxy haproxy[10236]: [NOTICE]   (10236) : New worker (10238) forked",
        "Aug 12 10:47:45 rev1-haproxy haproxy[10236]: [NOTICE]   (10236) : Loading success.",
        "Aug 12 10:49:30 rev1-haproxy haproxy[10238]: 188.240.223.140:50711 [12/Aug/2025:10:49:30.461] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/3/4 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13267 cpu_ns_avg:3316 lat_ns_tot:3976 lat_ns_avg:994",
        "Aug 12 10:49:36 rev1-haproxy haproxy[10238]: 188.240.223.140:60726 [12/Aug/2025:10:49:36.726] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/3/4 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13441 cpu_ns_avg:3360 lat_ns_tot:5817 lat_ns_avg:1454",
        "Aug 12 10:49:38 rev1-haproxy haproxy[10238]: 188.240.223.140:34708 [12/Aug/2025:10:49:38.835] haproxy_frontend haproxy_backend/rev1_devC 0/0/0/3/3 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:14863 cpu_ns_avg:3715 lat_ns_tot:3763 lat_ns_avg:940",
        "Aug 12 10:49:40 rev1-haproxy haproxy[10238]: 188.240.223.140:47631 [12/Aug/2025:10:49:40.940] haproxy_frontend haproxy_backend/rev1_devA 0/0/0/2/2 404 368 - - ---- 2/1/0/0/0 0/0 \"GET /stats;csv HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12130 cpu_ns_avg:3032 lat_ns_tot:3920 lat_ns_avg:980",
        "Aug 12 10:49:47 rev1-haproxy haproxy[10238]: 188.240.223.140:5227 [12/Aug/2025:10:49:47.139] haproxy_frontend haproxy_backend/rev1_devB 0/0/0/4/4 404 368 - - ---- 2/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:7729 cpu_ns_avg:1932 lat_ns_tot:3724 lat_ns_avg:931"
    ]
}
2025-08-12 10:49:51,719 p=1395911 u=ubuntu n=ansible | PLAY [Test NGINX (snmp) proxy] ************************************************************************************************************************************************************
2025-08-12 10:49:51,724 p=1395911 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************
2025-08-12 10:49:54,388 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 10:49:54,401 p=1395911 u=ubuntu n=ansible | TASK [Gather NGINX public IP address] *****************************************************************************************************************************************************
2025-08-12 10:49:56,563 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 10:49:56,568 p=1395911 u=ubuntu n=ansible | TASK [Send SNMP request to NGINX server and collect responses] ****************************************************************************************************************************
2025-08-12 10:49:58,489 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=0)
2025-08-12 10:50:00,385 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=1)
2025-08-12 10:50:02,317 p=1395911 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=2)
2025-08-12 10:50:02,324 p=1395911 u=ubuntu n=ansible | TASK [Display the NGINX response content] *************************************************************************************************************************************************
2025-08-12 10:50:02,344 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=0) => {
    "ansible_loop_var": "item",
    "item": 0,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-12 10:50:02,348 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=1) => {
    "ansible_loop_var": "item",
    "item": 1,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-12 10:50:02,352 p=1395911 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=2) => {
    "ansible_loop_var": "item",
    "item": 2,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-12 10:50:02,369 p=1395911 u=ubuntu n=ansible | PLAY RECAP ********************************************************************************************************************************************************************************
2025-08-12 10:50:02,369 p=1395911 u=ubuntu n=ansible | rev1_HAproxy               : ok=46   changed=22   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 10:50:02,369 p=1395911 u=ubuntu n=ansible | rev1_NGINX                 : ok=10   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 10:50:02,369 p=1395911 u=ubuntu n=ansible | rev1_devA                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 10:50:02,370 p=1395911 u=ubuntu n=ansible | rev1_devB                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 10:50:02,370 p=1395911 u=ubuntu n=ansible | rev1_devC                  : ok=14   changed=10   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 18:47:54,216 p=1421474 u=ubuntu n=ansible | PLAY [Set up Flask app servers and SNMPd for monitoring] **********************************************************************************************************************************************************************
2025-08-12 18:47:54,222 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:48:06,509 p=1421474 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-12 18:48:07,056 p=1421474 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-12 18:48:07,117 p=1421474 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-12 18:48:07,141 p=1421474 u=ubuntu n=ansible | TASK [Install required packages] **********************************************************************************************************************************************************************************************
2025-08-12 18:48:20,826 p=1421474 u=ubuntu n=ansible | ok: [rev1_devC] => (item=python3)
2025-08-12 18:48:21,023 p=1421474 u=ubuntu n=ansible | ok: [rev1_devB] => (item=python3)
2025-08-12 18:48:21,037 p=1421474 u=ubuntu n=ansible | ok: [rev1_devA] => (item=python3)
2025-08-12 18:48:30,798 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC] => (item=python3-pip)
2025-08-12 18:48:31,529 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB] => (item=python3-pip)
2025-08-12 18:48:32,108 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA] => (item=python3-pip)
2025-08-12 18:48:43,850 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmpd)
2025-08-12 18:48:44,780 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmpd)
2025-08-12 18:48:45,629 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmpd)
2025-08-12 18:48:52,530 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp)
2025-08-12 18:48:54,672 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp)
2025-08-12 18:48:55,058 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp)
2025-08-12 18:49:06,650 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC] => (item=snmp-mibs-downloader)
2025-08-12 18:49:09,222 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB] => (item=snmp-mibs-downloader)
2025-08-12 18:49:10,702 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA] => (item=snmp-mibs-downloader)
2025-08-12 18:49:15,552 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC] => (item=apache2-utils)
2025-08-12 18:49:18,981 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB] => (item=apache2-utils)
2025-08-12 18:49:20,577 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA] => (item=apache2-utils)
2025-08-12 18:49:24,108 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC] => (item=httperf)
2025-08-12 18:49:28,031 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB] => (item=httperf)
2025-08-12 18:49:29,832 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA] => (item=httperf)
2025-08-12 18:49:29,840 p=1421474 u=ubuntu n=ansible | TASK [Install Flask] **********************************************************************************************************************************************************************************************************
2025-08-12 18:49:37,420 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:49:37,529 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:49:37,550 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:49:37,556 p=1421474 u=ubuntu n=ansible | TASK [Deploy the Flask application config for TCP Load Balancing] *************************************************************************************************************************************************************
2025-08-12 18:49:46,435 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:49:46,912 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:49:46,977 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:49:46,982 p=1421474 u=ubuntu n=ansible | TASK [Start Flask app in background on port 5000] *****************************************************************************************************************************************************************************
2025-08-12 18:49:51,734 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:49:52,074 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:49:52,142 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:49:52,147 p=1421474 u=ubuntu n=ansible | TASK [Check Flask app HTTP response on private IP] ****************************************************************************************************************************************************************************
2025-08-12 18:49:56,597 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:49:56,828 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:49:56,909 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:49:56,915 p=1421474 u=ubuntu n=ansible | TASK [Display Flask app HTTP response on private IP] **************************************************************************************************************************************************************************
2025-08-12 18:49:56,940 p=1421474 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "18:49:55 10.1.1.45:55274 -- 10.1.1.45 (rev1-deva) 30"
}
2025-08-12 18:49:56,942 p=1421474 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "18:49:56 10.1.1.22:53794 -- 10.1.1.22 (rev1-devb) 31"
}
2025-08-12 18:49:56,952 p=1421474 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "18:49:56 10.1.1.9:57792 -- 10.1.1.9 (rev1-devc) 93"
}
2025-08-12 18:49:56,957 p=1421474 u=ubuntu n=ansible | TASK [Remove the existing agent address lines] ********************************************************************************************************************************************************************************
2025-08-12 18:50:02,014 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:50:02,092 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:50:02,166 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:50:02,171 p=1421474 u=ubuntu n=ansible | TASK [Configure agent address (0.0.0.0) for SNMPd to listen on all UDP interfaces] ********************************************************************************************************************************************
2025-08-12 18:50:06,618 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:50:06,815 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:50:07,003 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:50:07,008 p=1421474 u=ubuntu n=ansible | TASK [Check snmpd config File] ************************************************************************************************************************************************************************************************
2025-08-12 18:50:12,129 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:50:12,189 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:50:12,253 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:50:12,258 p=1421474 u=ubuntu n=ansible | TASK [Display snmpd configuration file] ***************************************************************************************************************************************************************************************
2025-08-12 18:50:12,285 p=1421474 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.004134",
        "end": "2025-08-12 18:50:11.293273",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 18:50:11.289139",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 18:50:12,289 p=1421474 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003097",
        "end": "2025-08-12 18:50:11.385406",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 18:50:11.382309",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 18:50:12,298 p=1421474 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "snmpd_config_file": {
        "changed": true,
        "cmd": "cat \"/etc/snmp/snmpd.conf\"",
        "delta": "0:00:00.003170",
        "end": "2025-08-12 18:50:11.356925",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 18:50:11.353755",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "###########################################################################\n#\n# snmpd.conf\n# An example configuration file for configuring the Net-SNMP agent ('snmpd')\n# See snmpd.conf(5) man page for details\n#\n###########################################################################\n# SECTION: System Information Setup\n#\n\n# syslocation: The [typically physical] location of the system.\n#   Note that setting this value here means that when trying to\n#   perform an snmp SET operation to the sysLocation.0 variable will make\n#   the agent return the \"notWritable\" error code.  IE, including\n#   this token in the snmpd.conf file will disable write access to\n#   the variable.\n#   arguments:  location_string\nsysLocation    Sitting on the Dock of the Bay\nsysContact     Me <me@example.org>\n\n# sysservices: The proper value for the sysServices object.\n#   arguments:  sysservices_number\nsysServices    72\n\n\n\n###########################################################################\n# SECTION: Agent Operating Mode\n#\n#   This section defines how the agent will operate when it\n#   is running.\n\n# master: Should the agent operate as a master agent or not.\n#   Currently, the only supported master agent type for this token\n#   is \"agentx\".\n#   \n#   arguments: (on|yes|agentx|all|off|no)\n\nmaster  agentx\n\n# agentaddress: The IP address and port number that the agent will listen on.\n#   By default the agent listens to any and all traffic from any\n#   interface on the default SNMP port (161).  This allows you to\n#   specify which address, interface, transport type and port(s) that you\n#   want the agent to listen on.  Multiple definitions of this token\n#   are concatenated together (using ':'s).\n#   arguments: [transport:]port[@interface/address],...\n\n\n\n\n###########################################################################\n# SECTION: Access Control Setup\n#\n#   This section defines who is allowed to talk to your running\n#   snmp agent.\n\n# Views \n#   arguments viewname included [oid]\n\n#  system + hrSystem groups only\nview   systemonly  included   .1.3.6.1.2.1.1\nview   systemonly  included   .1.3.6.1.2.1.25.1\n\n\n# rocommunity: a SNMPv1/SNMPv2c read-only access community name\n#   arguments:  community [default|hostname|network/bits] [oid | -V view]\n\n# Read-only access to everyone to the systemonly view\nrocommunity  public default -V systemonly\nrocommunity6 public default -V systemonly\n\n# SNMPv3 doesn't use communities, but users with (optionally) an\n# authentication and encryption string. This user needs to be created\n# with what they can view with rouser/rwuser lines in this file.\n#\n# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]\n# e.g.\n# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase\n#\n# This should be put into /var/lib/snmp/snmpd.conf \n#\n# rouser: a SNMPv3 read-only access username\n#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]\nrouser authPrivUser authpriv -V systemonly\nagentaddress  0.0.0.0:6000",
        "stdout_lines": [
            "###########################################################################",
            "#",
            "# snmpd.conf",
            "# An example configuration file for configuring the Net-SNMP agent ('snmpd')",
            "# See snmpd.conf(5) man page for details",
            "#",
            "###########################################################################",
            "# SECTION: System Information Setup",
            "#",
            "",
            "# syslocation: The [typically physical] location of the system.",
            "#   Note that setting this value here means that when trying to",
            "#   perform an snmp SET operation to the sysLocation.0 variable will make",
            "#   the agent return the \"notWritable\" error code.  IE, including",
            "#   this token in the snmpd.conf file will disable write access to",
            "#   the variable.",
            "#   arguments:  location_string",
            "sysLocation    Sitting on the Dock of the Bay",
            "sysContact     Me <me@example.org>",
            "",
            "# sysservices: The proper value for the sysServices object.",
            "#   arguments:  sysservices_number",
            "sysServices    72",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Agent Operating Mode",
            "#",
            "#   This section defines how the agent will operate when it",
            "#   is running.",
            "",
            "# master: Should the agent operate as a master agent or not.",
            "#   Currently, the only supported master agent type for this token",
            "#   is \"agentx\".",
            "#   ",
            "#   arguments: (on|yes|agentx|all|off|no)",
            "",
            "master  agentx",
            "",
            "# agentaddress: The IP address and port number that the agent will listen on.",
            "#   By default the agent listens to any and all traffic from any",
            "#   interface on the default SNMP port (161).  This allows you to",
            "#   specify which address, interface, transport type and port(s) that you",
            "#   want the agent to listen on.  Multiple definitions of this token",
            "#   are concatenated together (using ':'s).",
            "#   arguments: [transport:]port[@interface/address],...",
            "",
            "",
            "",
            "",
            "###########################################################################",
            "# SECTION: Access Control Setup",
            "#",
            "#   This section defines who is allowed to talk to your running",
            "#   snmp agent.",
            "",
            "# Views ",
            "#   arguments viewname included [oid]",
            "",
            "#  system + hrSystem groups only",
            "view   systemonly  included   .1.3.6.1.2.1.1",
            "view   systemonly  included   .1.3.6.1.2.1.25.1",
            "",
            "",
            "# rocommunity: a SNMPv1/SNMPv2c read-only access community name",
            "#   arguments:  community [default|hostname|network/bits] [oid | -V view]",
            "",
            "# Read-only access to everyone to the systemonly view",
            "rocommunity  public default -V systemonly",
            "rocommunity6 public default -V systemonly",
            "",
            "# SNMPv3 doesn't use communities, but users with (optionally) an",
            "# authentication and encryption string. This user needs to be created",
            "# with what they can view with rouser/rwuser lines in this file.",
            "#",
            "# createUser username (MD5|SHA|SHA-512|SHA-384|SHA-256|SHA-224) authpassphrase [DES|AES] [privpassphrase]",
            "# e.g.",
            "# createuser authPrivUser SHA-512 myauthphrase AES myprivphrase",
            "#",
            "# This should be put into /var/lib/snmp/snmpd.conf ",
            "#",
            "# rouser: a SNMPv3 read-only access username",
            "#    arguments: username [noauth|auth|priv [OID | -V VIEW [CONTEXT]]]",
            "rouser authPrivUser authpriv -V systemonly",
            "agentaddress  0.0.0.0:6000"
        ]
    }
}
2025-08-12 18:50:12,303 p=1421474 u=ubuntu n=ansible | TASK [Restart SNMPD if agent address configuration is changed] ****************************************************************************************************************************************************************
2025-08-12 18:50:17,589 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:50:17,925 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:50:18,024 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:50:18,031 p=1421474 u=ubuntu n=ansible | TASK [Test SNMPd with snmpget on 10.1.1.45] ***********************************************************************************************************************************************************************************
2025-08-12 18:50:22,754 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:50:22,942 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:50:23,042 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:50:23,047 p=1421474 u=ubuntu n=ansible | TASK [Print SNMPd snmpget result] *********************************************************************************************************************************************************************************************
2025-08-12 18:50:23,084 p=1421474 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-12 18:50:23,085 p=1421474 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-12 18:50:23,093 p=1421474 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "msg": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-12 18:50:23,166 p=1421474 u=ubuntu n=ansible | PLAY [Set up HAProxy] *********************************************************************************************************************************************************************************************************
2025-08-12 18:50:23,169 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:50:28,885 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:50:28,897 p=1421474 u=ubuntu n=ansible | TASK [Add HAProxy 2.9 PPA] ****************************************************************************************************************************************************************************************************
2025-08-12 18:50:41,942 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:50:41,947 p=1421474 u=ubuntu n=ansible | TASK [Install HAProxy] ********************************************************************************************************************************************************************************************************
2025-08-12 18:50:53,603 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:50:53,608 p=1421474 u=ubuntu n=ansible | TASK [Install performance testing required packages] **************************************************************************************************************************************************************************
2025-08-12 18:51:01,920 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy] => (item=apache2-utils)
2025-08-12 18:51:09,933 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy] => (item=httperf)
2025-08-12 18:51:09,938 p=1421474 u=ubuntu n=ansible | TASK [Deploy stats web page password file] ************************************************************************************************************************************************************************************
2025-08-12 18:51:13,735 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:13,740 p=1421474 u=ubuntu n=ansible | TASK [Read stats page password from file] *************************************************************************************************************************************************************************************
2025-08-12 18:51:16,030 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:51:16,035 p=1421474 u=ubuntu n=ansible | TASK [Set up HAProxy stats secret variable] ***********************************************************************************************************************************************************************************
2025-08-12 18:51:16,069 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:51:16,074 p=1421474 u=ubuntu n=ansible | TASK [Configure HAProxy] ******************************************************************************************************************************************************************************************************
2025-08-12 18:51:20,033 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:20,038 p=1421474 u=ubuntu n=ansible | TASK [Deploy rsyslog 49-haproxy config file] **********************************************************************************************************************************************************************************
2025-08-12 18:51:24,091 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:24,097 p=1421474 u=ubuntu n=ansible | TASK [Return 49_haproxy_conf to registered rsyslog_49_haproxy_conf] ***********************************************************************************************************************************************************
2025-08-12 18:51:26,591 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:26,596 p=1421474 u=ubuntu n=ansible | TASK [debug] ******************************************************************************************************************************************************************************************************************
2025-08-12 18:51:26,611 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "rsyslog_49_haproxy_conf.stdout_lines": [
        "# Create an additional socket in haproxy's chroot in order to allow logging via",
        "# /dev/log to chroot'ed HAProxy processes",
        "$AddUnixListenSocket /var/lib/haproxy/dev/log",
        "",
        "# Send HAProxy messages to a dedicated logfile",
        ":programname, startswith, \"haproxy\" {",
        "  /var/log/haproxy.log",
        "stop",
        "}"
    ]
}
2025-08-12 18:51:26,616 p=1421474 u=ubuntu n=ansible | TASK [Test HAProxy Configurations] ********************************************************************************************************************************************************************************************
2025-08-12 18:51:28,643 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:28,650 p=1421474 u=ubuntu n=ansible | TASK [Display HAProxy config test result] *************************************************************************************************************************************************************************************
2025-08-12 18:51:28,665 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_configs_test_result": {
        "changed": true,
        "cmd": [
            "haproxy",
            "-f",
            "/etc/haproxy/haproxy.cfg",
            "-c"
        ],
        "delta": "0:00:00.027613",
        "end": "2025-08-12 18:51:28.331364",
        "failed": false,
        "msg": "",
        "rc": 0,
        "start": "2025-08-12 18:51:28.303751",
        "stderr": "",
        "stderr_lines": [],
        "stdout": "",
        "stdout_lines": []
    }
}
2025-08-12 18:51:28,670 p=1421474 u=ubuntu n=ansible | TASK [Test HAProxy is running] ************************************************************************************************************************************************************************************************
2025-08-12 18:51:30,853 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:51:30,859 p=1421474 u=ubuntu n=ansible | TASK [Display the HAProxy service status] *************************************************************************************************************************************************************************************
2025-08-12 18:51:30,874 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_service_status.state": "started"
}
2025-08-12 18:51:30,879 p=1421474 u=ubuntu n=ansible | TASK [Check HAProxy server status] ********************************************************************************************************************************************************************************************
2025-08-12 18:51:32,791 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:32,796 p=1421474 u=ubuntu n=ansible | TASK [Display HAProxy server status] ******************************************************************************************************************************************************************************************
2025-08-12 18:51:32,812 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "systemctl_haproxy_service_status.stdout_lines": [
        "● haproxy.service - HAProxy Load Balancer",
        "     Loaded: loaded (/lib/systemd/system/haproxy.service; enabled; vendor preset: enabled)",
        "     Active: active (running) since Tue 2025-08-12 18:50:49 UTC; 42s ago",
        "       Docs: man:haproxy(1)",
        "             file:/usr/share/doc/haproxy/configuration.txt.gz",
        "   Main PID: 4459 (haproxy)",
        "     Status: \"Ready.\"",
        "      Tasks: 2 (limit: 4588)",
        "     Memory: 39.8M",
        "     CGroup: /system.slice/haproxy.service",
        "             ├─4459 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "             └─4479 /usr/sbin/haproxy -Ws -f /etc/haproxy/haproxy.cfg -p /run/haproxy.pid -S /run/haproxy-master.sock",
        "",
        "Aug 12 18:50:49 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 12 18:50:49 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : New worker (4479) forked",
        "Aug 12 18:50:49 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 12 18:50:49 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : Loading success."
    ]
}
2025-08-12 18:51:32,817 p=1421474 u=ubuntu n=ansible | TASK [Check HAProxy config errors via journalctl] *****************************************************************************************************************************************************************************
2025-08-12 18:51:34,800 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:34,805 p=1421474 u=ubuntu n=ansible | TASK [Display HAProxy config errors] ******************************************************************************************************************************************************************************************
2025-08-12 18:51:34,819 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_journalctl_logs.stdout_lines": [
        "-- Logs begin at Tue 2025-08-12 18:46:09 UTC, end at Tue 2025-08-12 18:51:34 UTC. --",
        "Aug 12 18:50:49 rev1-haproxy systemd[1]: Starting HAProxy Load Balancer...",
        "Aug 12 18:50:49 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : New worker (4479) forked",
        "Aug 12 18:50:49 rev1-haproxy systemd[1]: Started HAProxy Load Balancer.",
        "Aug 12 18:50:49 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : Loading success."
    ]
}
2025-08-12 18:51:34,824 p=1421474 u=ubuntu n=ansible | TASK [Check the HAProxy configuration file] ***********************************************************************************************************************************************************************************
2025-08-12 18:51:36,838 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:36,843 p=1421474 u=ubuntu n=ansible | TASK [Display HAProxy configuration file] *************************************************************************************************************************************************************************************
2025-08-12 18:51:36,859 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_config_file.stdout_lines": [
        "global",
        "    profiling.tasks on #Enable HAProxy profiling (CPU time spent on processing a http request inside HAProxy)",
        "    nbthread 1 # 1 thread, IDs from 1 to 2, nbthread <number of CPU cores>",
        "    thread-groups 1",
        "    # declare threads",
        "    thread-group 1 1-1",
        "    # bind threads to cpu cores",
        "    cpu-map 1/all 0-0 # bind all threads to CPU 0 #syntax:cpu-map 1/1-<Number Of CPU Cores> 0-<Number of CPU Cores - 1>",
        "    # define logging",
        "    log /dev/log local0 debug",
        "    #log /dev/log local0 info",
        "    #log /dev/log local0 emerg",
        "    #log /dev/log local1 alert",
        "    #log /dev/log local2 crit",
        "    #log /dev/log local3 err",
        "    #log /dev/log local4 warning",
        "    #log /dev/log local5 notice",
        "    #log /dev/log local6 info",
        "    #log /dev/log local7 debug",
        "    #Security Considerations",
        "    chroot /var/lib/haproxy #chroot statement pointing to a /var/lib/haproxy location",
        "    user haproxy # uid/user statement",
        "    group haproxy # gid/group statement",
        "    stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin",
        "    stats maxconn 20",
        "    stats timeout 30000",
        "    daemon",
        "    maxconn 3000",
        "        ",
        "defaults",
        "    mode http",
        "    timeout connect 5s",
        "    timeout client 50s",
        "    timeout server 50s",
        "    errorfile 400 /etc/haproxy/errors/400.http",
        "    errorfile 403 /etc/haproxy/errors/403.http",
        "    errorfile 408 /etc/haproxy/errors/408.http",
        "    errorfile 500 /etc/haproxy/errors/500.http",
        "    errorfile 502 /etc/haproxy/errors/502.http",
        "    errorfile 503 /etc/haproxy/errors/503.http",
        "    errorfile 504 /etc/haproxy/errors/504.http",
        "",
        "frontend web_stats",
        "    mode http",
        "    bind *:80 ",
        "    http-request use-service prometheus-exporter if { path /metrics }",
        "    stats enable # enable stats page",
        "    stats uri /stats # stats uri",
        "    stats hide-version",
        "    stats refresh 1s",
        "    stats auth admin:uipassword",
        "",
        "frontend haproxy_frontend",
        "    log global",
        "    bind *:80  thread 1/all shards by-thread  #bind this proxy to threads 1 to 1 or all",
        "    mode http",
        "    option httplog",
        "    #option dontlog-normal",
        "    #option logasap",
        "    #define custom log-format",
        "    log-format \"%ci:%cp [%tr] %ft %b/%s %T/%Th/%Ti/%TR/%Tq/%Tw/%Tc/%Tr/%Td/%Tl/%Ta/%Tt/%Ts %ST %B %CC %CS %tsc %ac/%fc/%bc/%sc/%rc %sq/%bq %hr %hs %{+Q}r %[http_first_req] cpu_calls:%[cpu_calls] cpu_ns_tot:%[cpu_ns_tot] cpu_ns_avg:%[cpu_ns_avg] lat_ns_tot:%[lat_ns_tot] lat_ns_avg:%[lat_ns_avg]\"",
        "    default_backend haproxy_backend",
        "    ",
        "backend haproxy_backend",
        "    retry-on all-retryable-errors # This works when conn-failure, empty-response, junk-response, response-timeout, rtt-rejected, 500, 502, 503, and 504",
        "    retries 3",
        "             server rev1_devA 10.1.1.45:5000 check maxconn 1000",
        "             server rev1_devB 10.1.1.22:5000 check maxconn 1000",
        "             server rev1_devC 10.1.1.9:5000 check maxconn 1000",
        "    "
    ]
}
2025-08-12 18:51:36,871 p=1421474 u=ubuntu n=ansible | RUNNING HANDLER [Restart HAProxy service] *************************************************************************************************************************************************************************************
2025-08-12 18:51:39,143 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:39,150 p=1421474 u=ubuntu n=ansible | RUNNING HANDLER [Restart rsyslog service] *************************************************************************************************************************************************************************************
2025-08-12 18:51:41,352 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:41,361 p=1421474 u=ubuntu n=ansible | PLAY [Install the Grafana Alloy Agent on HAproxy] *****************************************************************************************************************************************************************************
2025-08-12 18:51:41,366 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:51:44,108 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:51:44,119 p=1421474 u=ubuntu n=ansible | TASK [Install the Grafana Alloy Agent] ****************************************************************************************************************************************************************************************
2025-08-12 18:51:53,468 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:53,473 p=1421474 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *********************************************************************************************************************************************************************************
2025-08-12 18:51:55,438 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:55,443 p=1421474 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] ***************************************************************************************************************************************************************************************
2025-08-12 18:51:55,459 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Tue 2025-08-12 18:51:52 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 13858 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 38.2M",
        "     CGroup: /system.slice/alloy.service",
        "             └─13858 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.458926529Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=livedebugging duration=21.741µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459087454Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=ui duration=3.765µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459223294Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 duration=183.04887ms",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459751556Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459983872Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.471457731Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.471810665Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.472272972Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=4b867d42c84508283d18ea34eab167b8",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.472425557Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=4b867d42c84508283d18ea34eab167b8 duration=216.761µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.485936118Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 18:51:55,464 p=1421474 u=ubuntu n=ansible | TASK [DeployAlloy config file] ************************************************************************************************************************************************************************************************
2025-08-12 18:51:59,272 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:51:59,278 p=1421474 u=ubuntu n=ansible | TASK [Restart the Grafana alloy service] **************************************************************************************************************************************************************************************
2025-08-12 18:52:01,320 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:52:01,325 p=1421474 u=ubuntu n=ansible | TASK [Check the Grafana alloy running status] *********************************************************************************************************************************************************************************
2025-08-12 18:52:03,298 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:52:03,303 p=1421474 u=ubuntu n=ansible | TASK [Display the Grafana alloy status] ***************************************************************************************************************************************************************************************
2025-08-12 18:52:03,320 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_status_response.stdout_lines": [
        "● alloy.service - Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines",
        "     Loaded: loaded (/lib/systemd/system/alloy.service; enabled; vendor preset: enabled)",
        "    Drop-In: /etc/systemd/system/alloy.service.d",
        "             └─env.conf",
        "     Active: active (running) since Tue 2025-08-12 18:52:00 UTC; 2s ago",
        "       Docs: https://grafana.com/docs/alloy",
        "   Main PID: 15450 (alloy)",
        "      Tasks: 6 (limit: 4588)",
        "     Memory: 38.5M",
        "     CGroup: /system.slice/alloy.service",
        "             └─15450 /usr/bin/alloy run --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy",
        "",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.36151761Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f7a7c5",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.370243738Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=10.446588ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.370429565Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 duration=75.196462ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.370953551Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.371982415Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.37488029Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.375266963Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.380672064Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ebfd27472c5453019b38d848a7949886",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.383039019Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ebfd27472c5453019b38d848a7949886 duration=2.429362ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.398098277Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 18:52:03,325 p=1421474 u=ubuntu n=ansible | TASK [Check the Grafana alloy logs] *******************************************************************************************************************************************************************************************
2025-08-12 18:52:05,382 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:52:05,387 p=1421474 u=ubuntu n=ansible | TASK [Display the Grafana alloy logs] *****************************************************************************************************************************************************************************************
2025-08-12 18:52:05,404 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_logs_response.stdout_lines": [
        "-- Logs begin at Tue 2025-08-12 18:46:09 UTC, end at Tue 2025-08-12 18:52:05 UTC. --",
        "Aug 12 18:51:52 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.288754258Z level=info \"boringcrypto enabled\"=false",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.272869978Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.295312315Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.295477216Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.295621522Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=loki.write.grafana_cloud_loki duration=671.263µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.295739725Z level=info msg=\"running usage stats reporter\"",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.295868939Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.29597513Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=0",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.296073841Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f7a7c5",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.296246548Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.296348615Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=prometheus.remote_write.metrics_service duration=11.696367ms",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.296471574Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=labelstore duration=9.925µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.296582852Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=tracing duration=6.773µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.296687954Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=logging duration=7.945397ms",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.30443478Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f7a7c5",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.457889079Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=remotecfg duration=161.071956ms",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.458279711Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.458469286Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=http duration=223.634µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.458619376Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=cluster duration=4.399µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.458775373Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=otel duration=6.52µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.458926529Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=livedebugging duration=21.741µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459087454Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 node_id=ui duration=3.765µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459223294Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=3d57252fc5bb01d32364ac8d5b9253c9 duration=183.04887ms",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459751556Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.459983872Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.471457731Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.471810665Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.472272972Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=4b867d42c84508283d18ea34eab167b8",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.472425557Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=4b867d42c84508283d18ea34eab167b8 duration=216.761µs",
        "Aug 12 18:51:53 rev1-haproxy alloy[13858]: ts=2025-08-12T18:51:53.485936118Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: interrupt received",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.866336097Z level=info msg=\"node exited without error\" node=livedebugging",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.866523709Z level=error msg=\"failed to start reporter\" err=\"context canceled\"",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.866603366Z level=info msg=\"node exited without error\" node=ui",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.866704636Z level=info msg=\"node exited without error\" node=loki.write.grafana_cloud_loki",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.866734406Z level=info msg=\"node exited without error\" node=labelstore",
        "Aug 12 18:52:00 rev1-haproxy systemd[1]: Stopping Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines...",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.871198675Z level=info msg=\"node exited without error\" node=remotecfg",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.871580547Z level=info msg=\"http server closed\" service=http addr=127.0.0.1:12345 err=\"http: Server closed\"",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.871719877Z level=info msg=\"node exited without error\" node=otel",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.871789693Z level=info msg=\"node exited without error\" node=cluster",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.871832489Z level=info msg=\"http server closed\" service=http addr=memory err=\"http: Server closed\"",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.872899238Z level=info msg=\"node exited without error\" node=http",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.872267356Z level=info msg=\"Stopping remote storage...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.875083859Z level=info msg=\"WAL watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f7a7c5",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.87528616Z level=info msg=\"Stopping metadata watcher...\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.875444246Z level=info msg=\"Scraped metadata watcher stopped\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.876010077Z level=info msg=\"Remote storage stopped.\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 18:52:00 rev1-haproxy alloy[13858]: ts=2025-08-12T18:52:00.876171821Z level=info msg=\"node exited without error\" node=prometheus.remote_write.metrics_service",
        "Aug 12 18:52:00 rev1-haproxy systemd[1]: alloy.service: Succeeded.",
        "Aug 12 18:52:00 rev1-haproxy systemd[1]: Stopped Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 18:52:00 rev1-haproxy systemd[1]: Started Vendor-agnostic OpenTelemetry Collector distribution with programmable pipelines.",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.295447569Z level=info \"boringcrypto enabled\"=false",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.29432478Z level=info source=/go/pkg/mod/github.com/!kim!machine!gun/automemlimit@v0.7.1/memlimit/memlimit.go:175 msg=\"memory is not limited, skipping\" package=github.com/KimMachineGun/automemlimit/memlimit",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.297351851Z level=info msg=\"no peer discovery configured: both join and discover peers are empty\" service=cluster",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.297519991Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.297661957Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=tracing duration=6.603µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.297815537Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=logging duration=2.368778ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.297944296Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=otel duration=3.284µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.298108434Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=livedebugging duration=30.799µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.299049056Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=loki.write.grafana_cloud_loki duration=817.762µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.30144249Z level=info msg=\"running usage stats reporter\"",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.340896018Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=remotecfg duration=41.696271ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.340975067Z level=info msg=\"applying non-TLS config to HTTP server\" service=http",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.340983479Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=http duration=38.344µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.340994943Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=ui duration=1.819µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.341016101Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=cluster duration=1.615µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.341042662Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=labelstore duration=18.539µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.341210166Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=discovery.relabel.metrics_integrations_integrations_haproxy duration=152.162µs",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.353474736Z level=info msg=\"replaying WAL, this may take a while\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal dir=/var/lib/alloy/data/prometheus.remote_write.metrics_service/wal",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.356100366Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=0 maxSegment=1",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.356397347Z level=info msg=\"WAL segment loaded\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=wal segment=1 maxSegment=1",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.357584174Z level=info msg=\"Starting WAL watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f7a7c5",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.359087203Z level=info msg=\"Starting scraped metadata watcher\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.359250119Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=prometheus.remote_write.metrics_service duration=18.025694ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.36151761Z level=info msg=\"Replaying WAL\" component_path=/ component_id=prometheus.remote_write.metrics_service subcomponent=rw remote_name=f7a7c5 url=https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push queue=f7a7c5",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.370243738Z level=info msg=\"finished node evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 node_id=prometheus.scrape.metrics_integrations_integrations_haproxy duration=10.446588ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.370429565Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=\"\" trace_id=bcafea1d8c4fc2ceb37eaee76bf5ee37 duration=75.196462ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.370953551Z level=info msg=\"scheduling loaded components and services\"",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.371982415Z level=info msg=\"starting cluster node\" service=cluster peers_count=0 peers=\"\" advertise_addr=127.0.0.1:12345 minimum_cluster_size=0 minimum_size_wait_timeout=0s",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.37488029Z level=info msg=\"now listening for http traffic\" service=http addr=127.0.0.1:12345",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.375266963Z level=info msg=\"peers changed\" service=cluster peers_count=1 min_cluster_size=0 peers=rev1-haproxy",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.380672064Z level=info msg=\"starting complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ebfd27472c5453019b38d848a7949886",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.383039019Z level=info msg=\"finished complete graph evaluation\" controller_path=/ controller_id=remotecfg trace_id=ebfd27472c5453019b38d848a7949886 duration=2.429362ms",
        "Aug 12 18:52:01 rev1-haproxy alloy[15450]: ts=2025-08-12T18:52:01.398098277Z level=info msg=\"scheduling loaded components and services\""
    ]
}
2025-08-12 18:52:05,409 p=1421474 u=ubuntu n=ansible | TASK [Check the Grafana alloy configuration file] *****************************************************************************************************************************************************************************
2025-08-12 18:52:07,420 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:52:07,425 p=1421474 u=ubuntu n=ansible | TASK [Display the Grafana alloy config] ***************************************************************************************************************************************************************************************
2025-08-12 18:52:07,441 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "alloy_config_response.stdout_lines": [
        "remotecfg {",
        "  url            = \"https://fleet-management-prod-016.grafana.net\"",
        "  id             = \"rev1-haproxy\"",
        "  poll_frequency = \"60s\"",
        "",
        "  basic_auth {",
        "    username = \"1303247\"",
        "    password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "  }",
        "}",
        "",
        "prometheus.remote_write \"metrics_service\" {",
        "  endpoint {",
        "    url = \"https://prometheus-prod-39-prod-eu-north-0.grafana.net/api/prom/push\"",
        "    basic_auth {",
        "      username = \"2530729\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "loki.write \"grafana_cloud_loki\" {",
        "  endpoint {",
        "    url = \"https://logs-prod-025.grafana.net/loki/api/v1/push\"",
        "    basic_auth {",
        "      username = \"1261041\"",
        "      password = \"glc_eyJvIjoiMTQ3MDk3MCIsIm4iOiJzdGFjay0xMzAzMjQ3LWFsbG95LXRlc3QtYWxsb3ktMjUyNTA3IiwiayI6IjlHMEdReTRlR241OUQ2MW83c2dNNzBIbCIsIm0iOnsiciI6InByb2QtZXUtbm9ydGgtMCJ9fQ==\"",
        "    }",
        "  }",
        "}",
        "",
        "discovery.relabel \"metrics_integrations_integrations_haproxy\" {",
        "  targets = [{",
        "    __address__ = \"127.0.0.1:80\",",
        "  }]",
        "",
        "  rule {",
        "    target_label = \"instance\"",
        "    replacement  = constants.hostname",
        "  }",
        "}",
        "",
        "prometheus.scrape \"metrics_integrations_integrations_haproxy\" {",
        "  targets    = discovery.relabel.metrics_integrations_integrations_haproxy.output",
        "  forward_to = [prometheus.remote_write.metrics_service.receiver]",
        "  job_name   = \"integrations/haproxy\"",
        "}"
    ]
}
2025-08-12 18:52:07,457 p=1421474 u=ubuntu n=ansible | [WARNING]: Found variable using reserved name: timeout

2025-08-12 18:52:07,458 p=1421474 u=ubuntu n=ansible | PLAY [Install snmp, snmpd, NGINX UDP load balancer config for SNMP] ***********************************************************************************************************************************************************
2025-08-12 18:52:07,460 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:52:13,855 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 18:52:13,865 p=1421474 u=ubuntu n=ansible | TASK [Install required packages] **********************************************************************************************************************************************************************************************
2025-08-12 18:52:34,618 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=nginx)
2025-08-12 18:52:45,863 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmpd)
2025-08-12 18:52:54,454 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp)
2025-08-12 18:53:08,964 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=snmp-mibs-downloader)
2025-08-12 18:53:08,971 p=1421474 u=ubuntu n=ansible | TASK [Deploy NGINX stream config for SNMP UDP load balancing] *****************************************************************************************************************************************************************
2025-08-12 18:53:12,773 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-12 18:53:12,778 p=1421474 u=ubuntu n=ansible | TASK [Check nginx is running] *************************************************************************************************************************************************************************************************
2025-08-12 18:53:14,859 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 18:53:14,864 p=1421474 u=ubuntu n=ansible | TASK [display nginx status] ***************************************************************************************************************************************************************************************************
2025-08-12 18:53:14,886 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX] => {
    "nginx_running_status": {
        "changed": false,
        "failed": false,
        "name": "nginx",
        "state": "started",
        "status": {
            "ActiveEnterTimestamp": "Tue 2025-08-12 18:52:31 UTC",
            "ActiveEnterTimestampMonotonic": "379171950",
            "ActiveExitTimestampMonotonic": "0",
            "ActiveState": "active",
            "After": "network.target systemd-journald.socket basic.target system.slice sysinit.target",
            "AllowIsolate": "no",
            "AllowedCPUs": "",
            "AllowedMemoryNodes": "",
            "AmbientCapabilities": "",
            "AssertResult": "yes",
            "AssertTimestamp": "Tue 2025-08-12 18:52:31 UTC",
            "AssertTimestampMonotonic": "379106271",
            "Before": "shutdown.target multi-user.target",
            "BlockIOAccounting": "no",
            "BlockIOWeight": "[not set]",
            "CPUAccounting": "no",
            "CPUAffinity": "",
            "CPUAffinityFromNUMA": "no",
            "CPUQuotaPerSecUSec": "infinity",
            "CPUQuotaPeriodUSec": "infinity",
            "CPUSchedulingPolicy": "0",
            "CPUSchedulingPriority": "0",
            "CPUSchedulingResetOnFork": "no",
            "CPUShares": "[not set]",
            "CPUUsageNSec": "[not set]",
            "CPUWeight": "[not set]",
            "CacheDirectoryMode": "0755",
            "CanIsolate": "no",
            "CanReload": "yes",
            "CanStart": "yes",
            "CanStop": "yes",
            "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read",
            "CleanResult": "success",
            "CollectMode": "inactive",
            "ConditionResult": "yes",
            "ConditionTimestamp": "Tue 2025-08-12 18:52:31 UTC",
            "ConditionTimestampMonotonic": "379106271",
            "ConfigurationDirectoryMode": "0755",
            "Conflicts": "shutdown.target",
            "ControlGroup": "/system.slice/nginx.service",
            "ControlPID": "0",
            "DefaultDependencies": "yes",
            "DefaultMemoryLow": "0",
            "DefaultMemoryMin": "0",
            "Delegate": "no",
            "Description": "A high performance web server and a reverse proxy server",
            "DevicePolicy": "auto",
            "Documentation": "man:nginx(8)",
            "DynamicUser": "no",
            "EffectiveCPUs": "",
            "EffectiveMemoryNodes": "",
            "ExecMainCode": "0",
            "ExecMainExitTimestampMonotonic": "0",
            "ExecMainPID": "3451",
            "ExecMainStartTimestamp": "Tue 2025-08-12 18:52:31 UTC",
            "ExecMainStartTimestampMonotonic": "379171926",
            "ExecMainStatus": "0",
            "ExecReload": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecReloadEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; -s reload ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStart": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPre": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStartPreEx": "{ path=/usr/sbin/nginx ; argv[]=/usr/sbin/nginx -t -q -g daemon on; master_process on; ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStop": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; ignore_errors=yes ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "ExecStopEx": "{ path=/sbin/start-stop-daemon ; argv[]=/sbin/start-stop-daemon --quiet --stop --retry QUIT/5 --pidfile /run/nginx.pid ; flags=ignore-failure ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }",
            "FailureAction": "none",
            "FileDescriptorStoreMax": "0",
            "FinalKillSignal": "9",
            "FragmentPath": "/lib/systemd/system/nginx.service",
            "GID": "[not set]",
            "GuessMainPID": "yes",
            "IOAccounting": "no",
            "IOReadBytes": "18446744073709551615",
            "IOReadOperations": "18446744073709551615",
            "IOSchedulingClass": "0",
            "IOSchedulingPriority": "0",
            "IOWeight": "[not set]",
            "IOWriteBytes": "18446744073709551615",
            "IOWriteOperations": "18446744073709551615",
            "IPAccounting": "no",
            "IPEgressBytes": "[no data]",
            "IPEgressPackets": "[no data]",
            "IPIngressBytes": "[no data]",
            "IPIngressPackets": "[no data]",
            "Id": "nginx.service",
            "IgnoreOnIsolate": "no",
            "IgnoreSIGPIPE": "yes",
            "InactiveEnterTimestampMonotonic": "0",
            "InactiveExitTimestamp": "Tue 2025-08-12 18:52:31 UTC",
            "InactiveExitTimestampMonotonic": "379107988",
            "InvocationID": "f92e6bd5351a49a59a543ea4b5bed31d",
            "JobRunningTimeoutUSec": "infinity",
            "JobTimeoutAction": "none",
            "JobTimeoutUSec": "infinity",
            "KeyringMode": "private",
            "KillMode": "mixed",
            "KillSignal": "15",
            "LimitAS": "infinity",
            "LimitASSoft": "infinity",
            "LimitCORE": "infinity",
            "LimitCORESoft": "0",
            "LimitCPU": "infinity",
            "LimitCPUSoft": "infinity",
            "LimitDATA": "infinity",
            "LimitDATASoft": "infinity",
            "LimitFSIZE": "infinity",
            "LimitFSIZESoft": "infinity",
            "LimitLOCKS": "infinity",
            "LimitLOCKSSoft": "infinity",
            "LimitMEMLOCK": "65536",
            "LimitMEMLOCKSoft": "65536",
            "LimitMSGQUEUE": "819200",
            "LimitMSGQUEUESoft": "819200",
            "LimitNICE": "0",
            "LimitNICESoft": "0",
            "LimitNOFILE": "524288",
            "LimitNOFILESoft": "1024",
            "LimitNPROC": "15295",
            "LimitNPROCSoft": "15295",
            "LimitRSS": "infinity",
            "LimitRSSSoft": "infinity",
            "LimitRTPRIO": "0",
            "LimitRTPRIOSoft": "0",
            "LimitRTTIME": "infinity",
            "LimitRTTIMESoft": "infinity",
            "LimitSIGPENDING": "15295",
            "LimitSIGPENDINGSoft": "15295",
            "LimitSTACK": "infinity",
            "LimitSTACKSoft": "8388608",
            "LoadState": "loaded",
            "LockPersonality": "no",
            "LogLevelMax": "-1",
            "LogRateLimitBurst": "0",
            "LogRateLimitIntervalUSec": "0",
            "LogsDirectoryMode": "0755",
            "MainPID": "3451",
            "MemoryAccounting": "yes",
            "MemoryCurrent": "5427200",
            "MemoryDenyWriteExecute": "no",
            "MemoryHigh": "infinity",
            "MemoryLimit": "infinity",
            "MemoryLow": "0",
            "MemoryMax": "infinity",
            "MemoryMin": "0",
            "MemorySwapMax": "infinity",
            "MountAPIVFS": "no",
            "MountFlags": "",
            "NFileDescriptorStore": "0",
            "NRestarts": "0",
            "NUMAMask": "",
            "NUMAPolicy": "n/a",
            "Names": "nginx.service",
            "NeedDaemonReload": "no",
            "Nice": "0",
            "NoNewPrivileges": "no",
            "NonBlocking": "no",
            "NotifyAccess": "none",
            "OOMPolicy": "stop",
            "OOMScoreAdjust": "0",
            "OnFailureJobMode": "replace",
            "PIDFile": "/run/nginx.pid",
            "Perpetual": "no",
            "PrivateDevices": "no",
            "PrivateMounts": "no",
            "PrivateNetwork": "no",
            "PrivateTmp": "no",
            "PrivateUsers": "no",
            "ProtectControlGroups": "no",
            "ProtectHome": "no",
            "ProtectHostname": "no",
            "ProtectKernelLogs": "no",
            "ProtectKernelModules": "no",
            "ProtectKernelTunables": "no",
            "ProtectSystem": "no",
            "RefuseManualStart": "no",
            "RefuseManualStop": "no",
            "ReloadResult": "success",
            "RemainAfterExit": "no",
            "RemoveIPC": "no",
            "Requires": "system.slice sysinit.target",
            "Restart": "no",
            "RestartKillSignal": "15",
            "RestartUSec": "100ms",
            "RestrictNamespaces": "no",
            "RestrictRealtime": "no",
            "RestrictSUIDSGID": "no",
            "Result": "success",
            "RootDirectoryStartOnly": "no",
            "RuntimeDirectoryMode": "0755",
            "RuntimeDirectoryPreserve": "no",
            "RuntimeMaxUSec": "infinity",
            "SameProcessGroup": "no",
            "SecureBits": "0",
            "SendSIGHUP": "no",
            "SendSIGKILL": "yes",
            "Slice": "system.slice",
            "StandardError": "inherit",
            "StandardInput": "null",
            "StandardInputData": "",
            "StandardOutput": "journal",
            "StartLimitAction": "none",
            "StartLimitBurst": "5",
            "StartLimitIntervalUSec": "10s",
            "StartupBlockIOWeight": "[not set]",
            "StartupCPUShares": "[not set]",
            "StartupCPUWeight": "[not set]",
            "StartupIOWeight": "[not set]",
            "StateChangeTimestamp": "Tue 2025-08-12 18:52:31 UTC",
            "StateChangeTimestampMonotonic": "379171950",
            "StateDirectoryMode": "0755",
            "StatusErrno": "0",
            "StopWhenUnneeded": "no",
            "SubState": "running",
            "SuccessAction": "none",
            "SyslogFacility": "3",
            "SyslogLevel": "6",
            "SyslogLevelPrefix": "yes",
            "SyslogPriority": "30",
            "SystemCallErrorNumber": "0",
            "TTYReset": "no",
            "TTYVHangup": "no",
            "TTYVTDisallocate": "no",
            "TasksAccounting": "yes",
            "TasksCurrent": "2",
            "TasksMax": "4588",
            "TimeoutAbortUSec": "5s",
            "TimeoutCleanUSec": "infinity",
            "TimeoutStartUSec": "1min 30s",
            "TimeoutStopUSec": "5s",
            "TimerSlackNSec": "50000",
            "Transient": "no",
            "Type": "forking",
            "UID": "[not set]",
            "UMask": "0022",
            "UnitFilePreset": "enabled",
            "UnitFileState": "enabled",
            "UtmpMode": "init",
            "WantedBy": "multi-user.target",
            "WatchdogSignal": "6",
            "WatchdogTimestampMonotonic": "0",
            "WatchdogUSec": "0"
        }
    }
}
2025-08-12 18:53:14,897 p=1421474 u=ubuntu n=ansible | RUNNING HANDLER [Reload NGINX] ************************************************************************************************************************************************************************************************
2025-08-12 18:53:17,042 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX]
2025-08-12 18:53:17,053 p=1421474 u=ubuntu n=ansible | PLAY [Test HAProxy (http), HAProxy Web stats (STATS)+ Metrics (PROMEX) and HAProxy logs] **************************************************************************************************************************************
2025-08-12 18:53:17,058 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:53:20,263 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:53:20,275 p=1421474 u=ubuntu n=ansible | TASK [Gather HAProxy server public IP address] ********************************************************************************************************************************************************************************
2025-08-12 18:53:22,744 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:53:22,749 p=1421474 u=ubuntu n=ansible | TASK [Send HTTP request to HAProxy and collect response] **********************************************************************************************************************************************************************
2025-08-12 18:53:24,926 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0)
2025-08-12 18:53:31,018 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1)
2025-08-12 18:53:33,059 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2)
2025-08-12 18:53:33,067 p=1421474 u=ubuntu n=ansible | TASK [Display the HAProxy response] *******************************************************************************************************************************************************************************************
2025-08-12 18:53:33,085 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=0) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "18:53:24 10.1.1.30:60190 -- 10.1.1.22 (rev1-devb) 72\n",
    "item": 0
}
2025-08-12 18:53:33,090 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=1) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "18:53:30 10.1.1.30:51114 -- 10.1.1.9 (rev1-devc) 91\n",
    "item": 1
}
2025-08-12 18:53:33,094 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => (item=2) => {
    "ansible_loop_var": "item",
    "haproxy_response.results[item].content": "18:53:32 10.1.1.30:41482 -- 10.1.1.45 (rev1-deva) 92\n",
    "item": 2
}
2025-08-12 18:53:33,101 p=1421474 u=ubuntu n=ansible | TASK [Send HTTP requests to HAProxy stats page and collect responses] *********************************************************************************************************************************************************
2025-08-12 18:53:35,267 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:53:35,272 p=1421474 u=ubuntu n=ansible | TASK [Display the stats response content] *************************************************************************************************************************************************************************************
2025-08-12 18:53:35,287 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_stats_response.content": "# pxname,svname,qcur,qmax,scur,smax,slim,stot,bin,bout,dreq,dresp,ereq,econ,eresp,wretr,wredis,status,weight,act,bck,chkfail,chkdown,lastchg,downtime,qlimit,pid,iid,sid,throttle,lbtot,tracked,type,rate,rate_lim,rate_max,check_status,check_code,check_duration,hrsp_1xx,hrsp_2xx,hrsp_3xx,hrsp_4xx,hrsp_5xx,hrsp_other,hanafail,req_rate,req_rate_max,req_tot,cli_abrt,srv_abrt,comp_in,comp_out,comp_byp,comp_rsp,lastsess,last_chk,last_agt,qtime,ctime,rtime,ttime,agent_status,agent_code,agent_duration,check_desc,agent_desc,check_rise,check_fall,check_health,agent_rise,agent_fall,agent_health,addr,cookie,mode,algo,conn_rate,conn_rate_max,conn_tot,intercepted,dcon,dses,wrew,connect,reuse,cache_lookups,cache_hits,srv_icur,src_ilim,qtime_max,ctime_max,rtime_max,ttime_max,eint,idle_conn_cur,safe_conn_cur,used_conn_cur,need_conn_est,uweight,agg_server_status,agg_server_check_status,agg_check_status,srid,sess_other,h1sess,h2sess,h3sess,req_other,h1req,h2req,h3req,proto,-,ssl_sess,ssl_reused_sess,ssl_failed_handshake,quic_rxbuf_full,quic_dropped_pkt,quic_dropped_pkt_bufoverrun,quic_dropped_parsing_pkt,quic_socket_full,quic_sendto_err,quic_sendto_err_unknwn,quic_sent_pkt,quic_lost_pkt,quic_too_short_dgram,quic_retry_sent,quic_retry_validated,quic_retry_error,quic_half_open_conn,quic_hdshk_fail,quic_stless_rst_sent,quic_conn_migration_done,quic_transp_err_no_error,quic_transp_err_internal_error,quic_transp_err_connection_refused,quic_transp_err_flow_control_error,quic_transp_err_stream_limit_error,quic_transp_err_stream_state_error,quic_transp_err_final_size_error,quic_transp_err_frame_encoding_error,quic_transp_err_transport_parameter_error,quic_transp_err_connection_id_limit,quic_transp_err_protocol_violation_error,quic_transp_err_invalid_token,quic_transp_err_application_error,quic_transp_err_crypto_buffer_exceeded,quic_transp_err_key_update_error,quic_transp_err_aead_limit_reached,quic_transp_err_no_viable_path,quic_transp_err_crypto_error,quic_transp_err_unknown_error,quic_data_blocked,quic_stream_data_blocked,quic_streams_blocked_bidi,quic_streams_blocked_uni,h3_data,h3_headers,h3_cancel_push,h3_push_promise,h3_max_push_id,h3_goaway,h3_settings,h3_no_error,h3_general_protocol_error,h3_internal_error,h3_stream_creation_error,h3_closed_critical_stream,h3_frame_unexpected,h3_frame_error,h3_excessive_load,h3_id_error,h3_settings_error,h3_missing_settings,h3_request_rejected,h3_request_cancelled,h3_request_incomplete,h3_message_error,h3_connect_error,h3_version_fallback,pack_decompression_failed,qpack_encoder_stream_error,qpack_decoder_stream_error,h2_headers_rcvd,h2_data_rcvd,h2_settings_rcvd,h2_rst_stream_rcvd,h2_goaway_rcvd,h2_detected_conn_protocol_errors,h2_detected_strm_protocol_errors,h2_rst_stream_resp,h2_goaway_resp,h2_open_connections,h2_backend_open_streams,h2_total_connections,h2_backend_total_streams,h1_open_connections,h1_open_streams,h1_total_connections,h1_total_streams,h1_bytes_in,h1_bytes_out,h1_spliced_bytes_in,h1_spliced_bytes_out,\nweb_stats,FRONTEND,,,2,2,3000,3,392,62249,0,0,0,,,,,OPEN,,,,,,,,,1,2,0,,,,0,1,0,1,,,,0,1,0,0,1,0,,1,1,3,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,1,1,3,2,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,3,0,0,0,3,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,1,3,3,585,62261,0,0,\nhaproxy_frontend,FRONTEND,,,0,1,3000,4,592,985,0,0,0,,,,,OPEN,,,,,,,,,1,3,0,,,,0,0,0,1,,,,0,3,0,1,0,0,,0,1,4,,,0,0,0,0,,,,,,,,,,,,,,,,,,,,,http,,0,1,4,0,0,0,0,,,0,0,,,,,,,0,,,,,,,,,,0,4,0,0,0,4,0,0,,-,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,645,1411,0,0,\nhaproxy_backend,rev1_devA,0,0,0,1,1000,2,392,574,,0,,0,0,0,0,UP,1,1,0,0,0,116,0,,1,4,1,,2,,2,0,,1,L4OK,,0,0,1,0,1,0,0,,,,2,0,0,,,,,2,,,0,1,4,5,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,2,0,,,0,,0,1,4,6,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devB,0,0,0,1,1000,1,100,206,,0,,0,0,0,0,UP,1,1,0,0,0,116,0,,1,4,2,,1,,2,0,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,10,,,0,0,3,4,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,3,4,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,rev1_devC,0,0,0,1,1000,1,100,205,,0,,0,0,0,0,UP,1,1,0,0,0,116,0,,1,4,3,,1,,2,0,,1,L4OK,,0,0,1,0,0,0,0,,,,1,0,0,,,,,4,,,0,0,3,3,,,,Layer4 check passed,,2,3,4,,,,,,http,,,,,,,,0,1,0,,,0,,0,0,3,3,0,0,0,0,1,1,,,,0,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\nhaproxy_backend,BACKEND,0,0,0,1,300,4,592,985,0,0,,0,0,0,0,UP,3,3,0,,0,116,0,,1,4,0,,4,,1,0,,1,,,,0,3,0,1,0,0,,,,4,0,0,0,0,0,0,2,,,0,1,4,4,,,,,,,,,,,,,,http,,,,,,,,0,4,0,0,0,,,0,1,4,6,0,,,,,3,0,0,0,,,,,,,,,,,-,0,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,1430,588,0,0,\n"
}
2025-08-12 18:53:35,292 p=1421474 u=ubuntu n=ansible | TASK [Test the HAProxy metrics (promex) path] *********************************************************************************************************************************************************************************
2025-08-12 18:53:37,333 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:53:37,338 p=1421474 u=ubuntu n=ansible | TASK [Display the HAProxy metrics (promex) response content] ******************************************************************************************************************************************************************
2025-08-12 18:53:37,368 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_metrics_path_result.stdout_lines": [
        "# HELP haproxy_process_nbthread Number of started threads (global.nbthread)",
        "# TYPE haproxy_process_nbthread gauge",
        "haproxy_process_nbthread 1",
        "# HELP haproxy_process_nbproc Number of started worker processes (historical, always 1)",
        "# TYPE haproxy_process_nbproc gauge",
        "haproxy_process_nbproc 1",
        "# HELP haproxy_process_relative_process_id Relative worker process number (1)",
        "# TYPE haproxy_process_relative_process_id gauge",
        "haproxy_process_relative_process_id 1",
        "# HELP haproxy_process_uptime_seconds How long ago this worker process was started (seconds)",
        "# TYPE haproxy_process_uptime_seconds gauge",
        "haproxy_process_uptime_seconds 118",
        "# HELP haproxy_process_pool_failures_total Number of failed pool allocations since this worker was started",
        "# TYPE haproxy_process_pool_failures_total counter",
        "haproxy_process_pool_failures_total 0",
        "# HELP haproxy_process_max_fds Hard limit on the number of per-process file descriptors",
        "# TYPE haproxy_process_max_fds gauge",
        "haproxy_process_max_fds 6062",
        "# HELP haproxy_process_max_sockets Hard limit on the number of per-process sockets",
        "# TYPE haproxy_process_max_sockets gauge",
        "haproxy_process_max_sockets 6062",
        "# HELP haproxy_process_max_connections Hard limit on the number of per-process connections (configured or imposed by Ulimit-n)",
        "# TYPE haproxy_process_max_connections gauge",
        "haproxy_process_max_connections 3000",
        "# HELP haproxy_process_hard_max_connections Hard limit on the number of per-process connections (imposed by Memmax_MB or Ulimit-n)",
        "# TYPE haproxy_process_hard_max_connections gauge",
        "haproxy_process_hard_max_connections 3000",
        "# HELP haproxy_process_current_connections Current number of connections on this worker process",
        "# TYPE haproxy_process_current_connections gauge",
        "haproxy_process_current_connections 2",
        "# HELP haproxy_process_connections_total Total number of connections on this worker process since started",
        "# TYPE haproxy_process_connections_total counter",
        "haproxy_process_connections_total 186",
        "# HELP haproxy_process_requests_total Total number of requests on this worker process since started",
        "# TYPE haproxy_process_requests_total counter",
        "haproxy_process_requests_total 8",
        "# HELP haproxy_process_max_ssl_connections Hard limit on the number of per-process SSL endpoints (front+back), 0=unlimited",
        "# TYPE haproxy_process_max_ssl_connections gauge",
        "haproxy_process_max_ssl_connections 0",
        "# HELP haproxy_process_current_ssl_connections Current number of SSL endpoints on this worker process (front+back)",
        "# TYPE haproxy_process_current_ssl_connections gauge",
        "haproxy_process_current_ssl_connections 0",
        "# HELP haproxy_process_ssl_connections_total Total number of SSL endpoints on this worker process since started (front+back)",
        "# TYPE haproxy_process_ssl_connections_total counter",
        "haproxy_process_ssl_connections_total 0",
        "# HELP haproxy_process_max_pipes Hard limit on the number of pipes for splicing, 0=unlimited",
        "# TYPE haproxy_process_max_pipes gauge",
        "haproxy_process_max_pipes 0",
        "# HELP haproxy_process_pipes_used_total Current number of pipes in use in this worker process",
        "# TYPE haproxy_process_pipes_used_total counter",
        "haproxy_process_pipes_used_total 0",
        "# HELP haproxy_process_pipes_free_total Current number of allocated and available pipes in this worker process",
        "# TYPE haproxy_process_pipes_free_total counter",
        "haproxy_process_pipes_free_total 0",
        "# HELP haproxy_process_current_connection_rate Number of front connections created on this worker process over the last second",
        "# TYPE haproxy_process_current_connection_rate gauge",
        "haproxy_process_current_connection_rate 1",
        "# HELP haproxy_process_limit_connection_rate Hard limit for ConnRate (global.maxconnrate)",
        "# TYPE haproxy_process_limit_connection_rate gauge",
        "haproxy_process_limit_connection_rate 0",
        "# HELP haproxy_process_max_connection_rate Highest ConnRate reached on this worker process since started (in connections per second)",
        "# TYPE haproxy_process_max_connection_rate gauge",
        "haproxy_process_max_connection_rate 1",
        "# HELP haproxy_process_current_session_rate Number of sessions created on this worker process over the last second",
        "# TYPE haproxy_process_current_session_rate gauge",
        "haproxy_process_current_session_rate 1",
        "# HELP haproxy_process_limit_session_rate Hard limit for SessRate (global.maxsessrate)",
        "# TYPE haproxy_process_limit_session_rate gauge",
        "haproxy_process_limit_session_rate 0",
        "# HELP haproxy_process_max_session_rate Highest SessRate reached on this worker process since started (in sessions per second)",
        "# TYPE haproxy_process_max_session_rate gauge",
        "haproxy_process_max_session_rate 1",
        "# HELP haproxy_process_current_ssl_rate Number of SSL connections created on this worker process over the last second",
        "# TYPE haproxy_process_current_ssl_rate gauge",
        "haproxy_process_current_ssl_rate 0",
        "# HELP haproxy_process_limit_ssl_rate Hard limit for SslRate (global.maxsslrate)",
        "# TYPE haproxy_process_limit_ssl_rate gauge",
        "haproxy_process_limit_ssl_rate 0",
        "# HELP haproxy_process_max_ssl_rate Highest SslRate reached on this worker process since started (in connections per second)",
        "# TYPE haproxy_process_max_ssl_rate gauge",
        "haproxy_process_max_ssl_rate 0",
        "# HELP haproxy_process_current_frontend_ssl_key_rate Number of SSL keys created on frontends in this worker process over the last second",
        "# TYPE haproxy_process_current_frontend_ssl_key_rate gauge",
        "haproxy_process_current_frontend_ssl_key_rate 0",
        "# HELP haproxy_process_max_frontend_ssl_key_rate Highest SslFrontendKeyRate reached on this worker process since started (in SSL keys per second)",
        "# TYPE haproxy_process_max_frontend_ssl_key_rate gauge",
        "haproxy_process_max_frontend_ssl_key_rate 0",
        "# HELP haproxy_process_frontend_ssl_reuse Percent of frontend SSL connections which did not require a new key",
        "# TYPE haproxy_process_frontend_ssl_reuse gauge",
        "haproxy_process_frontend_ssl_reuse 0",
        "# HELP haproxy_process_current_backend_ssl_key_rate Number of SSL keys created on backends in this worker process over the last second",
        "# TYPE haproxy_process_current_backend_ssl_key_rate gauge",
        "haproxy_process_current_backend_ssl_key_rate 0",
        "# HELP haproxy_process_max_backend_ssl_key_rate Highest SslBackendKeyRate reached on this worker process since started (in SSL keys per second)",
        "# TYPE haproxy_process_max_backend_ssl_key_rate gauge",
        "haproxy_process_max_backend_ssl_key_rate 0",
        "# HELP haproxy_process_ssl_cache_lookups_total Total number of SSL session ID lookups in the SSL session cache on this worker since started",
        "# TYPE haproxy_process_ssl_cache_lookups_total counter",
        "haproxy_process_ssl_cache_lookups_total 0",
        "# HELP haproxy_process_ssl_cache_misses_total Total number of SSL session ID lookups that didn't find a session in the SSL session cache on this worker since started",
        "# TYPE haproxy_process_ssl_cache_misses_total counter",
        "haproxy_process_ssl_cache_misses_total 0",
        "# HELP haproxy_process_http_comp_bytes_in_total Number of bytes submitted to the HTTP compressor in this worker process over the last second",
        "# TYPE haproxy_process_http_comp_bytes_in_total counter",
        "haproxy_process_http_comp_bytes_in_total 0",
        "# HELP haproxy_process_http_comp_bytes_out_total Number of bytes emitted by the HTTP compressor in this worker process over the last second",
        "# TYPE haproxy_process_http_comp_bytes_out_total counter",
        "haproxy_process_http_comp_bytes_out_total 0",
        "# HELP haproxy_process_limit_http_comp Limit of CompressBpsOut beyond which HTTP compression is automatically disabled",
        "# TYPE haproxy_process_limit_http_comp gauge",
        "haproxy_process_limit_http_comp 0",
        "# HELP haproxy_process_current_zlib_memory Amount of memory currently used by HTTP compression on the current worker process (in bytes)",
        "# TYPE haproxy_process_current_zlib_memory gauge",
        "haproxy_process_current_zlib_memory NaN",
        "# HELP haproxy_process_max_zlib_memory Limit on the amount of memory used by HTTP compression above which it is automatically disabled (in bytes, see global.maxzlibmem)",
        "# TYPE haproxy_process_max_zlib_memory gauge",
        "haproxy_process_max_zlib_memory NaN",
        "# HELP haproxy_process_current_tasks Total number of tasks in the current worker process (active + sleeping)",
        "# TYPE haproxy_process_current_tasks gauge",
        "haproxy_process_current_tasks 20",
        "# HELP haproxy_process_current_run_queue Total number of active tasks+tasklets in the current worker process",
        "# TYPE haproxy_process_current_run_queue gauge",
        "haproxy_process_current_run_queue 0",
        "# HELP haproxy_process_idle_time_percent Percentage of last second spent waiting in the current worker thread",
        "# TYPE haproxy_process_idle_time_percent gauge",
        "haproxy_process_idle_time_percent 100",
        "# HELP haproxy_process_stopping 1 if the worker process is currently stopping, otherwise zero",
        "# TYPE haproxy_process_stopping gauge",
        "haproxy_process_stopping 0",
        "# HELP haproxy_process_jobs Current number of active jobs on the current worker process (frontend connections, master connections, listeners)",
        "# TYPE haproxy_process_jobs gauge",
        "haproxy_process_jobs 7",
        "# HELP haproxy_process_unstoppable_jobs Current number of unstoppable jobs on the current worker process (master connections)",
        "# TYPE haproxy_process_unstoppable_jobs gauge",
        "haproxy_process_unstoppable_jobs 1",
        "# HELP haproxy_process_listeners Current number of active listeners on the current worker process",
        "# TYPE haproxy_process_listeners gauge",
        "haproxy_process_listeners 4",
        "# HELP haproxy_process_active_peers Current number of verified active peers connections on the current worker process",
        "# TYPE haproxy_process_active_peers gauge",
        "haproxy_process_active_peers 0",
        "# HELP haproxy_process_connected_peers Current number of peers having passed the connection step on the current worker process",
        "# TYPE haproxy_process_connected_peers gauge",
        "haproxy_process_connected_peers 0",
        "# HELP haproxy_process_dropped_logs_total Total number of dropped logs for current worker process since started",
        "# TYPE haproxy_process_dropped_logs_total counter",
        "haproxy_process_dropped_logs_total 0",
        "# HELP haproxy_process_busy_polling_enabled 1 if busy-polling is currently in use on the worker process, otherwise zero (config.busy-polling)",
        "# TYPE haproxy_process_busy_polling_enabled gauge",
        "haproxy_process_busy_polling_enabled 0",
        "# HELP haproxy_process_failed_resolutions Total number of failed DNS resolutions in current worker process since started",
        "# TYPE haproxy_process_failed_resolutions counter",
        "haproxy_process_failed_resolutions 0",
        "# HELP haproxy_process_bytes_out_total Total number of bytes emitted by current worker process since started",
        "# TYPE haproxy_process_bytes_out_total counter",
        "haproxy_process_bytes_out_total 69126",
        "# HELP haproxy_process_spliced_bytes_out_total Total number of bytes emitted by current worker process through a kernel pipe since started",
        "# TYPE haproxy_process_spliced_bytes_out_total counter",
        "haproxy_process_spliced_bytes_out_total 0",
        "# HELP haproxy_process_bytes_out_rate Number of bytes emitted by current worker process over the last second",
        "# TYPE haproxy_process_bytes_out_rate gauge",
        "haproxy_process_bytes_out_rate 0",
        "# HELP haproxy_process_recv_logs_total Total number of log messages received by log-forwarding listeners on this worker process since started",
        "# TYPE haproxy_process_recv_logs_total counter",
        "haproxy_process_recv_logs_total 0",
        "# HELP haproxy_process_build_info Build info",
        "# TYPE haproxy_process_build_info gauge",
        "haproxy_process_build_info{version=\"2.9.15-1ppa1~focal\"} 1",
        "# HELP haproxy_process_max_memory_bytes Worker process's hard limit on memory usage in byes (-m on command line)",
        "# TYPE haproxy_process_max_memory_bytes gauge",
        "haproxy_process_max_memory_bytes 0",
        "# HELP haproxy_process_pool_allocated_bytes Amount of memory allocated in pools (in bytes)",
        "# TYPE haproxy_process_pool_allocated_bytes gauge",
        "haproxy_process_pool_allocated_bytes 74328",
        "# HELP haproxy_process_pool_used_bytes Amount of pool memory currently used (in bytes)",
        "# TYPE haproxy_process_pool_used_bytes gauge",
        "haproxy_process_pool_used_bytes 74328",
        "# HELP haproxy_process_start_time_seconds Start time in seconds",
        "# TYPE haproxy_process_start_time_seconds gauge",
        "haproxy_process_start_time_seconds 1755024698",
        "# HELP haproxy_frontend_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_frontend_current_sessions gauge",
        "haproxy_frontend_current_sessions{proxy=\"web_stats\"} 2",
        "haproxy_frontend_current_sessions{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_frontend_max_sessions gauge",
        "haproxy_frontend_max_sessions{proxy=\"web_stats\"} 2",
        "haproxy_frontend_max_sessions{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_frontend_limit_sessions gauge",
        "haproxy_frontend_limit_sessions{proxy=\"web_stats\"} 3000",
        "haproxy_frontend_limit_sessions{proxy=\"haproxy_frontend\"} 3000",
        "# HELP haproxy_frontend_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_frontend_sessions_total counter",
        "haproxy_frontend_sessions_total{proxy=\"web_stats\"} 4",
        "haproxy_frontend_sessions_total{proxy=\"haproxy_frontend\"} 4",
        "# HELP haproxy_frontend_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_frontend_bytes_in_total counter",
        "haproxy_frontend_bytes_in_total{proxy=\"web_stats\"} 544",
        "haproxy_frontend_bytes_in_total{proxy=\"haproxy_frontend\"} 592",
        "# HELP haproxy_frontend_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_frontend_bytes_out_total counter",
        "haproxy_frontend_bytes_out_total{proxy=\"web_stats\"} 67447",
        "haproxy_frontend_bytes_out_total{proxy=\"haproxy_frontend\"} 985",
        "# HELP haproxy_frontend_requests_denied_total Total number of denied requests since process started",
        "# TYPE haproxy_frontend_requests_denied_total counter",
        "haproxy_frontend_requests_denied_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_requests_denied_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_frontend_responses_denied_total counter",
        "haproxy_frontend_responses_denied_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_responses_denied_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_request_errors_total Total number of invalid requests since process started",
        "# TYPE haproxy_frontend_request_errors_total counter",
        "haproxy_frontend_request_errors_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_request_errors_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_status Current status of the service, per state label value.",
        "# TYPE haproxy_frontend_status gauge",
        "haproxy_frontend_status{proxy=\"web_stats\",state=\"DOWN\"} 0",
        "haproxy_frontend_status{proxy=\"web_stats\",state=\"UP\"} 1",
        "haproxy_frontend_status{proxy=\"haproxy_frontend\",state=\"DOWN\"} 0",
        "haproxy_frontend_status{proxy=\"haproxy_frontend\",state=\"UP\"} 1",
        "# HELP haproxy_frontend_limit_session_rate Limit on the number of sessions accepted in a second (frontend only, 'rate-limit sessions' setting)",
        "# TYPE haproxy_frontend_limit_session_rate gauge",
        "haproxy_frontend_limit_session_rate{proxy=\"web_stats\"} 0",
        "haproxy_frontend_limit_session_rate{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_frontend_max_session_rate gauge",
        "haproxy_frontend_max_session_rate{proxy=\"web_stats\"} 1",
        "haproxy_frontend_max_session_rate{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_frontend_http_responses_total counter",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"1xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"1xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"2xx\"} 2",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"2xx\"} 3",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"3xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"3xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"4xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"4xx\"} 1",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"5xx\"} 1",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"5xx\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"web_stats\",code=\"other\"} 0",
        "haproxy_frontend_http_responses_total{proxy=\"haproxy_frontend\",code=\"other\"} 0",
        "# HELP haproxy_frontend_http_requests_rate_max Highest value of http requests observed since the worker process started",
        "# TYPE haproxy_frontend_http_requests_rate_max gauge",
        "haproxy_frontend_http_requests_rate_max{proxy=\"web_stats\"} 1",
        "haproxy_frontend_http_requests_rate_max{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_http_requests_total Total number of HTTP requests processed by this object since the worker process started",
        "# TYPE haproxy_frontend_http_requests_total counter",
        "haproxy_frontend_http_requests_total{proxy=\"web_stats\"} 4",
        "haproxy_frontend_http_requests_total{proxy=\"haproxy_frontend\"} 4",
        "# HELP haproxy_frontend_http_comp_bytes_in_total Total number of bytes submitted to the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_bytes_in_total counter",
        "haproxy_frontend_http_comp_bytes_in_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_in_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_bytes_out_total Total number of bytes emitted by the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_bytes_out_total counter",
        "haproxy_frontend_http_comp_bytes_out_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_out_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_bytes_bypassed_total Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)",
        "# TYPE haproxy_frontend_http_comp_bytes_bypassed_total counter",
        "haproxy_frontend_http_comp_bytes_bypassed_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_bytes_bypassed_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_comp_responses_total Total number of HTTP responses that were compressed for this object since the worker process started",
        "# TYPE haproxy_frontend_http_comp_responses_total counter",
        "haproxy_frontend_http_comp_responses_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_comp_responses_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_connections_rate_max Highest value of connections per second observed since the worker process started",
        "# TYPE haproxy_frontend_connections_rate_max gauge",
        "haproxy_frontend_connections_rate_max{proxy=\"web_stats\"} 1",
        "haproxy_frontend_connections_rate_max{proxy=\"haproxy_frontend\"} 1",
        "# HELP haproxy_frontend_connections_total Total number of new connections accepted on this frontend since the worker process started",
        "# TYPE haproxy_frontend_connections_total counter",
        "haproxy_frontend_connections_total{proxy=\"web_stats\"} 4",
        "haproxy_frontend_connections_total{proxy=\"haproxy_frontend\"} 4",
        "# HELP haproxy_frontend_intercepted_requests_total Total number of HTTP requests intercepted on the frontend (redirects/stats/services) since the worker process started",
        "# TYPE haproxy_frontend_intercepted_requests_total counter",
        "haproxy_frontend_intercepted_requests_total{proxy=\"web_stats\"} 3",
        "haproxy_frontend_intercepted_requests_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_denied_connections_total Total number of incoming connections blocked on a listener/frontend by a tcp-request connection rule since the worker process started",
        "# TYPE haproxy_frontend_denied_connections_total counter",
        "haproxy_frontend_denied_connections_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_denied_connections_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_denied_sessions_total Total number of incoming sessions blocked on a listener/frontend by a tcp-request connection rule since the worker process started",
        "# TYPE haproxy_frontend_denied_sessions_total counter",
        "haproxy_frontend_denied_sessions_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_denied_sessions_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_frontend_failed_header_rewriting_total counter",
        "haproxy_frontend_failed_header_rewriting_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_failed_header_rewriting_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_cache_lookups_total Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_frontend_http_cache_lookups_total counter",
        "haproxy_frontend_http_cache_lookups_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_cache_lookups_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_http_cache_hits_total Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_frontend_http_cache_hits_total counter",
        "haproxy_frontend_http_cache_hits_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_http_cache_hits_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_frontend_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_frontend_internal_errors_total counter",
        "haproxy_frontend_internal_errors_total{proxy=\"web_stats\"} 0",
        "haproxy_frontend_internal_errors_total{proxy=\"haproxy_frontend\"} 0",
        "# HELP haproxy_backend_current_queue Number of current queued connections",
        "# TYPE haproxy_backend_current_queue gauge",
        "haproxy_backend_current_queue{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_queue Highest value of queued connections encountered since process started",
        "# TYPE haproxy_backend_max_queue gauge",
        "haproxy_backend_max_queue{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_backend_current_sessions gauge",
        "haproxy_backend_current_sessions{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_backend_max_sessions gauge",
        "haproxy_backend_max_sessions{proxy=\"haproxy_backend\"} 1",
        "# HELP haproxy_backend_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_backend_limit_sessions gauge",
        "haproxy_backend_limit_sessions{proxy=\"haproxy_backend\"} 300",
        "# HELP haproxy_backend_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_backend_sessions_total counter",
        "haproxy_backend_sessions_total{proxy=\"haproxy_backend\"} 4",
        "# HELP haproxy_backend_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_backend_bytes_in_total counter",
        "haproxy_backend_bytes_in_total{proxy=\"haproxy_backend\"} 592",
        "# HELP haproxy_backend_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_backend_bytes_out_total counter",
        "haproxy_backend_bytes_out_total{proxy=\"haproxy_backend\"} 985",
        "# HELP haproxy_backend_requests_denied_total Total number of denied requests since process started",
        "# TYPE haproxy_backend_requests_denied_total counter",
        "haproxy_backend_requests_denied_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_backend_responses_denied_total counter",
        "haproxy_backend_responses_denied_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_connection_errors_total Total number of failed connections to server since the worker process started",
        "# TYPE haproxy_backend_connection_errors_total counter",
        "haproxy_backend_connection_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_response_errors_total Total number of invalid responses since the worker process started",
        "# TYPE haproxy_backend_response_errors_total counter",
        "haproxy_backend_response_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_retry_warnings_total Total number of server connection retries since the worker process started",
        "# TYPE haproxy_backend_retry_warnings_total counter",
        "haproxy_backend_retry_warnings_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_redispatch_warnings_total Total number of server redispatches due to connection failures since the worker process started",
        "# TYPE haproxy_backend_redispatch_warnings_total counter",
        "haproxy_backend_redispatch_warnings_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_status Current status of the service, per state label value.",
        "# TYPE haproxy_backend_status gauge",
        "haproxy_backend_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_status{proxy=\"haproxy_backend\",state=\"UP\"} 1",
        "# HELP haproxy_backend_weight Server's effective weight, or sum of active servers' effective weights for a backend",
        "# TYPE haproxy_backend_weight gauge",
        "haproxy_backend_weight{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_active_servers Total number of active UP servers with a non-zero weight",
        "# TYPE haproxy_backend_active_servers gauge",
        "haproxy_backend_active_servers{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_backup_servers Total number of backup UP servers with a non-zero weight",
        "# TYPE haproxy_backend_backup_servers gauge",
        "haproxy_backend_backup_servers{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_check_up_down_total Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started",
        "# TYPE haproxy_backend_check_up_down_total counter",
        "haproxy_backend_check_up_down_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_check_last_change_seconds How long ago the last server state changed, in seconds",
        "# TYPE haproxy_backend_check_last_change_seconds gauge",
        "haproxy_backend_check_last_change_seconds{proxy=\"haproxy_backend\"} 119",
        "# HELP haproxy_backend_downtime_seconds_total Total time spent in DOWN state, for server or backend",
        "# TYPE haproxy_backend_downtime_seconds_total counter",
        "haproxy_backend_downtime_seconds_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_loadbalanced_total Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)",
        "# TYPE haproxy_backend_loadbalanced_total counter",
        "haproxy_backend_loadbalanced_total{proxy=\"haproxy_backend\"} 4",
        "# HELP haproxy_backend_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_backend_max_session_rate gauge",
        "haproxy_backend_max_session_rate{proxy=\"haproxy_backend\"} 1",
        "# HELP haproxy_backend_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_backend_http_responses_total counter",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"1xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"2xx\"} 3",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"3xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"4xx\"} 1",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"5xx\"} 0",
        "haproxy_backend_http_responses_total{proxy=\"haproxy_backend\",code=\"other\"} 0",
        "# HELP haproxy_backend_http_requests_total Total number of HTTP requests processed by this object since the worker process started",
        "# TYPE haproxy_backend_http_requests_total counter",
        "haproxy_backend_http_requests_total{proxy=\"haproxy_backend\"} 4",
        "# HELP haproxy_backend_client_aborts_total Total number of requests or connections aborted by the client since the worker process started",
        "# TYPE haproxy_backend_client_aborts_total counter",
        "haproxy_backend_client_aborts_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_server_aborts_total Total number of requests or connections aborted by the server since the worker process started",
        "# TYPE haproxy_backend_server_aborts_total counter",
        "haproxy_backend_server_aborts_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_in_total Total number of bytes submitted to the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_bytes_in_total counter",
        "haproxy_backend_http_comp_bytes_in_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_out_total Total number of bytes emitted by the HTTP compressor for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_bytes_out_total counter",
        "haproxy_backend_http_comp_bytes_out_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_bytes_bypassed_total Total number of bytes that bypassed HTTP compression for this object since the worker process started (CPU/memory/bandwidth limitation)",
        "# TYPE haproxy_backend_http_comp_bytes_bypassed_total counter",
        "haproxy_backend_http_comp_bytes_bypassed_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_comp_responses_total Total number of HTTP responses that were compressed for this object since the worker process started",
        "# TYPE haproxy_backend_http_comp_responses_total counter",
        "haproxy_backend_http_comp_responses_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_last_session_seconds How long ago some traffic was seen on this object on this worker process, in seconds",
        "# TYPE haproxy_backend_last_session_seconds gauge",
        "haproxy_backend_last_session_seconds{proxy=\"haproxy_backend\"} 5",
        "# HELP haproxy_backend_queue_time_average_seconds Avg. queue time for last 1024 successful connections.",
        "# TYPE haproxy_backend_queue_time_average_seconds gauge",
        "haproxy_backend_queue_time_average_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_connect_time_average_seconds Avg. connect time for last 1024 successful connections.",
        "# TYPE haproxy_backend_connect_time_average_seconds gauge",
        "haproxy_backend_connect_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_response_time_average_seconds Avg. response time for last 1024 successful connections.",
        "# TYPE haproxy_backend_response_time_average_seconds gauge",
        "haproxy_backend_response_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_total_time_average_seconds Avg. total time for last 1024 successful connections.",
        "# TYPE haproxy_backend_total_time_average_seconds gauge",
        "haproxy_backend_total_time_average_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_backend_failed_header_rewriting_total counter",
        "haproxy_backend_failed_header_rewriting_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_connection_attempts_total Total number of outgoing connection attempts on this backend/server since the worker process started",
        "# TYPE haproxy_backend_connection_attempts_total counter",
        "haproxy_backend_connection_attempts_total{proxy=\"haproxy_backend\"} 4",
        "# HELP haproxy_backend_connection_reuses_total Total number of reused connection on this backend/server since the worker process started",
        "# TYPE haproxy_backend_connection_reuses_total counter",
        "haproxy_backend_connection_reuses_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_cache_lookups_total Total number of HTTP requests looked up in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_backend_http_cache_lookups_total counter",
        "haproxy_backend_http_cache_lookups_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_http_cache_hits_total Total number of HTTP requests not found in the cache on this frontend/backend since the worker process started",
        "# TYPE haproxy_backend_http_cache_hits_total counter",
        "haproxy_backend_http_cache_hits_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_max_queue_time_seconds Maximum observed time spent in the queue",
        "# TYPE haproxy_backend_max_queue_time_seconds gauge",
        "haproxy_backend_max_queue_time_seconds{proxy=\"haproxy_backend\"} 0.000000",
        "# HELP haproxy_backend_max_connect_time_seconds Maximum observed time spent waiting for a connection to complete",
        "# TYPE haproxy_backend_max_connect_time_seconds gauge",
        "haproxy_backend_max_connect_time_seconds{proxy=\"haproxy_backend\"} 0.001000",
        "# HELP haproxy_backend_max_response_time_seconds Maximum observed time spent waiting for a server response",
        "# TYPE haproxy_backend_max_response_time_seconds gauge",
        "haproxy_backend_max_response_time_seconds{proxy=\"haproxy_backend\"} 0.004000",
        "# HELP haproxy_backend_max_total_time_seconds Maximum observed total request+response time (request+queue+connect+response+processing)",
        "# TYPE haproxy_backend_max_total_time_seconds gauge",
        "haproxy_backend_max_total_time_seconds{proxy=\"haproxy_backend\"} 0.006000",
        "# HELP haproxy_backend_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_backend_internal_errors_total counter",
        "haproxy_backend_internal_errors_total{proxy=\"haproxy_backend\"} 0",
        "# HELP haproxy_backend_uweight Server's user weight, or sum of active servers' user weights for a backend",
        "# TYPE haproxy_backend_uweight gauge",
        "haproxy_backend_uweight{proxy=\"haproxy_backend\"} 3",
        "# HELP haproxy_backend_agg_server_status Backend's aggregated gauge of servers' status",
        "# TYPE haproxy_backend_agg_server_status gauge",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"UP\"} 3",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"MAINT\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"DRAIN\"} 0",
        "haproxy_backend_agg_server_status{proxy=\"haproxy_backend\",state=\"NOLB\"} 0",
        "# HELP haproxy_backend_agg_server_check_status [DEPRECATED] Backend's aggregated gauge of servers' status",
        "# TYPE haproxy_backend_agg_server_check_status gauge",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"DOWN\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"UP\"} 3",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"MAINT\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"DRAIN\"} 0",
        "haproxy_backend_agg_server_check_status{proxy=\"haproxy_backend\",state=\"NOLB\"} 0",
        "# HELP haproxy_backend_agg_check_status Backend's aggregated gauge of servers' state check status",
        "# TYPE haproxy_backend_agg_check_status gauge",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"HANA\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"SOCKERR\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4OK\"} 3",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L4CON\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6OK\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L6RSP\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7TOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7RSP\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7OK\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7OKC\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"L7STS\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCERR\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCTOUT\"} 0",
        "haproxy_backend_agg_check_status{proxy=\"haproxy_backend\",state=\"PROCOK\"} 0",
        "# HELP haproxy_server_current_queue Number of current queued connections",
        "# TYPE haproxy_server_current_queue gauge",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_current_queue{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_max_queue Highest value of queued connections encountered since process started",
        "# TYPE haproxy_server_max_queue gauge",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_max_queue{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_current_sessions Number of current sessions on the frontend, backend or server",
        "# TYPE haproxy_server_current_sessions gauge",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_current_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_max_sessions Highest value of current sessions encountered since process started",
        "# TYPE haproxy_server_max_sessions gauge",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_max_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_limit_sessions Frontend/listener/server's maxconn, backend's fullconn",
        "# TYPE haproxy_server_limit_sessions gauge",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1000",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1000",
        "haproxy_server_limit_sessions{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1000",
        "# HELP haproxy_server_sessions_total Total number of sessions since process started",
        "# TYPE haproxy_server_sessions_total counter",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 2",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_sessions_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_bytes_in_total Total number of request bytes since process started",
        "# TYPE haproxy_server_bytes_in_total counter",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 392",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 100",
        "haproxy_server_bytes_in_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 100",
        "# HELP haproxy_server_bytes_out_total Total number of response bytes since process started",
        "# TYPE haproxy_server_bytes_out_total counter",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 574",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 206",
        "haproxy_server_bytes_out_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 205",
        "# HELP haproxy_server_responses_denied_total Total number of denied responses since process started",
        "# TYPE haproxy_server_responses_denied_total counter",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_responses_denied_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_connection_errors_total Total number of failed connections to server since the worker process started",
        "# TYPE haproxy_server_connection_errors_total counter",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_connection_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_response_errors_total Total number of invalid responses since the worker process started",
        "# TYPE haproxy_server_response_errors_total counter",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_response_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_retry_warnings_total Total number of server connection retries since the worker process started",
        "# TYPE haproxy_server_retry_warnings_total counter",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_retry_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_redispatch_warnings_total Total number of server redispatches due to connection failures since the worker process started",
        "# TYPE haproxy_server_redispatch_warnings_total counter",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_redispatch_warnings_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_status Current status of the service, per state label value.",
        "# TYPE haproxy_server_status gauge",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"NOLB\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"DOWN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"UP\"} 1",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"MAINT\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"DRAIN\"} 0",
        "haproxy_server_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"NOLB\"} 0",
        "# HELP haproxy_server_weight Server's effective weight, or sum of active servers' effective weights for a backend",
        "# TYPE haproxy_server_weight gauge",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_weight{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_check_failures_total Total number of failed individual health checks per server/backend, since the worker process started",
        "# TYPE haproxy_server_check_failures_total counter",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_failures_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_check_up_down_total Total number of failed checks causing UP to DOWN server transitions, per server/backend, since the worker process started",
        "# TYPE haproxy_server_check_up_down_total counter",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_up_down_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_check_last_change_seconds How long ago the last server state changed, in seconds",
        "# TYPE haproxy_server_check_last_change_seconds gauge",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 119",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 119",
        "haproxy_server_check_last_change_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 119",
        "# HELP haproxy_server_downtime_seconds_total Total time spent in DOWN state, for server or backend",
        "# TYPE haproxy_server_downtime_seconds_total counter",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_downtime_seconds_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_queue_limit Limit on the number of connections in queue, for servers only (maxqueue argument)",
        "# TYPE haproxy_server_queue_limit gauge",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_queue_limit{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "# HELP haproxy_server_current_throttle Throttling ratio applied to a server's maxconn and weight during the slowstart period (0 to 100%)",
        "# TYPE haproxy_server_current_throttle gauge",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_current_throttle{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "# HELP haproxy_server_loadbalanced_total Total number of requests routed by load balancing since the worker process started (ignores queue pop and stickiness)",
        "# TYPE haproxy_server_loadbalanced_total counter",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 2",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_loadbalanced_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_max_session_rate Highest value of sessions per second observed since the worker process started",
        "# TYPE haproxy_server_max_session_rate gauge",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_max_session_rate{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_check_status Status of last health check, per state label value.",
        "# TYPE haproxy_server_check_status gauge",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devA\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devB\",state=\"PROCOK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"HANA\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"SOCKERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4OK\"} 1",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L4CON\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L6RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7TOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7RSP\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7OK\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7OKC\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"L7STS\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCERR\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCTOUT\"} 0",
        "haproxy_server_check_status{proxy=\"haproxy_backend\",server=\"rev1_devC\",state=\"PROCOK\"} 0",
        "# HELP haproxy_server_check_code layer5-7 code, if available of the last health check.",
        "# TYPE haproxy_server_check_code gauge",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_check_code{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_check_duration_seconds Total duration of the latest server health check, in seconds.",
        "# TYPE haproxy_server_check_duration_seconds gauge",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_check_duration_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_http_responses_total Total number of HTTP responses with status 100-199 returned by this object since the worker process started",
        "# TYPE haproxy_server_http_responses_total counter",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"1xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"2xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"2xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"2xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"3xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"4xx\"} 1",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"4xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"4xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"5xx\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\",code=\"other\"} 0",
        "haproxy_server_http_responses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\",code=\"other\"} 0",
        "# HELP haproxy_server_client_aborts_total Total number of requests or connections aborted by the client since the worker process started",
        "# TYPE haproxy_server_client_aborts_total counter",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_client_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_server_aborts_total Total number of requests or connections aborted by the server since the worker process started",
        "# TYPE haproxy_server_server_aborts_total counter",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_server_aborts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_last_session_seconds How long ago some traffic was seen on this object on this worker process, in seconds",
        "# TYPE haproxy_server_last_session_seconds gauge",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 5",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 13",
        "haproxy_server_last_session_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 7",
        "# HELP haproxy_server_queue_time_average_seconds Avg. queue time for last 1024 successful connections.",
        "# TYPE haproxy_server_queue_time_average_seconds gauge",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_queue_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_connect_time_average_seconds Avg. connect time for last 1024 successful connections.",
        "# TYPE haproxy_server_connect_time_average_seconds gauge",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_connect_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_response_time_average_seconds Avg. response time for last 1024 successful connections.",
        "# TYPE haproxy_server_response_time_average_seconds gauge",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.001000",
        "haproxy_server_response_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.001000",
        "# HELP haproxy_server_total_time_average_seconds Avg. total time for last 1024 successful connections.",
        "# TYPE haproxy_server_total_time_average_seconds gauge",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.001000",
        "haproxy_server_total_time_average_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.001000",
        "# HELP haproxy_server_failed_header_rewriting_total Total number of failed HTTP header rewrites since the worker process started",
        "# TYPE haproxy_server_failed_header_rewriting_total counter",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_failed_header_rewriting_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_connection_attempts_total Total number of outgoing connection attempts on this backend/server since the worker process started",
        "# TYPE haproxy_server_connection_attempts_total counter",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 2",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_connection_attempts_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_connection_reuses_total Total number of reused connection on this backend/server since the worker process started",
        "# TYPE haproxy_server_connection_reuses_total counter",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_connection_reuses_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_idle_connections_current Current number of idle connections available for reuse on this server",
        "# TYPE haproxy_server_idle_connections_current gauge",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_idle_connections_limit Limit on the number of available idle connections on this server (server 'pool_max_conn' directive)",
        "# TYPE haproxy_server_idle_connections_limit gauge",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devA\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devB\"} NaN",
        "haproxy_server_idle_connections_limit{proxy=\"haproxy_backend\",server=\"rev1_devC\"} NaN",
        "# HELP haproxy_server_max_queue_time_seconds Maximum observed time spent in the queue",
        "# TYPE haproxy_server_max_queue_time_seconds gauge",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_max_queue_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_max_connect_time_seconds Maximum observed time spent waiting for a connection to complete",
        "# TYPE haproxy_server_max_connect_time_seconds gauge",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.001000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.000000",
        "haproxy_server_max_connect_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.000000",
        "# HELP haproxy_server_max_response_time_seconds Maximum observed time spent waiting for a server response",
        "# TYPE haproxy_server_max_response_time_seconds gauge",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.004000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.003000",
        "haproxy_server_max_response_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.003000",
        "# HELP haproxy_server_max_total_time_seconds Maximum observed total request+response time (request+queue+connect+response+processing)",
        "# TYPE haproxy_server_max_total_time_seconds gauge",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0.006000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0.004000",
        "haproxy_server_max_total_time_seconds{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0.003000",
        "# HELP haproxy_server_internal_errors_total Total number of internal errors since process started",
        "# TYPE haproxy_server_internal_errors_total counter",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_internal_errors_total{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_unsafe_idle_connections_current Current number of unsafe idle connections",
        "# TYPE haproxy_server_unsafe_idle_connections_current gauge",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_unsafe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_safe_idle_connections_current Current number of safe idle connections",
        "# TYPE haproxy_server_safe_idle_connections_current gauge",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_safe_idle_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_used_connections_current Current number of connections in use",
        "# TYPE haproxy_server_used_connections_current gauge",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 0",
        "haproxy_server_used_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 0",
        "# HELP haproxy_server_need_connections_current Estimated needed number of connections",
        "# TYPE haproxy_server_need_connections_current gauge",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_need_connections_current{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1",
        "# HELP haproxy_server_uweight Server's user weight, or sum of active servers' user weights for a backend",
        "# TYPE haproxy_server_uweight gauge",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devA\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devB\"} 1",
        "haproxy_server_uweight{proxy=\"haproxy_backend\",server=\"rev1_devC\"} 1"
    ]
}
2025-08-12 18:53:37,373 p=1421474 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] ******************************************************************************************************************************************************************************************
2025-08-12 18:53:39,415 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:53:39,421 p=1421474 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] ******************************************************************************************************************************************************************************************
2025-08-12 18:53:39,436 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : path to executable is /usr/sbin/haproxy",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [WARNING]  (4459) : Exiting Master process...",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [ALERT]    (4459) : Current worker (4479) exited with code 143 (Terminated)",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [WARNING]  (4459) : All workers exited. Exiting... (0)",
        "Aug 12 18:51:38 rev1-haproxy haproxy[12429]: [NOTICE]   (12429) : New worker (12431) forked",
        "Aug 12 18:51:38 rev1-haproxy haproxy[12429]: [NOTICE]   (12429) : Loading success.",
        "Aug 12 18:52:11 rev1-haproxy haproxy[12431]: 127.0.0.1:43444 [12/Aug/2025:18:52:11.739] haproxy_frontend haproxy_backend/rev1_devA 12/Aug/2025:18:52:11 +0000/0/1/0/1/0/1/4/0/12/Aug/2025:18:52:11 +0000/5/6/1755024731 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:46567 cpu_ns_avg:11641 lat_ns_tot:14937 lat_ns_avg:3734",
        "Aug 12 18:53:24 rev1-haproxy haproxy[12431]: 188.240.223.140:34280 [12/Aug/2025:18:53:24.600] haproxy_frontend haproxy_backend/rev1_devB 12/Aug/2025:18:53:24 +0000/0/0/0/0/0/0/3/1/12/Aug/2025:18:53:24 +0000/4/4/1755024804 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13524 cpu_ns_avg:3381 lat_ns_tot:5305 lat_ns_avg:1326",
        "Aug 12 18:53:30 rev1-haproxy haproxy[12431]: 188.240.223.140:6958 [12/Aug/2025:18:53:30.687] haproxy_frontend haproxy_backend/rev1_devC 12/Aug/2025:18:53:30 +0000/0/0/0/0/0/0/3/0/12/Aug/2025:18:53:30 +0000/3/3/1755024810 200 205 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12711 cpu_ns_avg:3177 lat_ns_tot:5230 lat_ns_avg:1307",
        "Aug 12 18:53:32 rev1-haproxy haproxy[12431]: 188.240.223.140:19688 [12/Aug/2025:18:53:32.734] haproxy_frontend haproxy_backend/rev1_devA 12/Aug/2025:18:53:32 +0000/0/0/0/0/0/0/3/0/12/Aug/2025:18:53:32 +0000/3/3/1755024812 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12721 cpu_ns_avg:3180 lat_ns_tot:5387 lat_ns_avg:1346"
    ]
}
2025-08-12 18:53:39,441 p=1421474 u=ubuntu n=ansible | TASK [Check HAProxy audit log lines] ******************************************************************************************************************************************************************************************
2025-08-12 18:53:41,451 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:53:41,457 p=1421474 u=ubuntu n=ansible | TASK [Display the HAProxy log lines] ******************************************************************************************************************************************************************************************
2025-08-12 18:53:41,474 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "haproxy_log_lines.stdout_lines": [
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : haproxy version is 2.9.15-1ppa1~focal",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [NOTICE]   (4459) : path to executable is /usr/sbin/haproxy",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [WARNING]  (4459) : Exiting Master process...",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [ALERT]    (4459) : Current worker (4479) exited with code 143 (Terminated)",
        "Aug 12 18:51:38 rev1-haproxy haproxy[4459]: [WARNING]  (4459) : All workers exited. Exiting... (0)",
        "Aug 12 18:51:38 rev1-haproxy haproxy[12429]: [NOTICE]   (12429) : New worker (12431) forked",
        "Aug 12 18:51:38 rev1-haproxy haproxy[12429]: [NOTICE]   (12429) : Loading success.",
        "Aug 12 18:52:11 rev1-haproxy haproxy[12431]: 127.0.0.1:43444 [12/Aug/2025:18:52:11.739] haproxy_frontend haproxy_backend/rev1_devA 12/Aug/2025:18:52:11 +0000/0/1/0/1/0/1/4/0/12/Aug/2025:18:52:11 +0000/5/6/1755024731 404 368 - - ---- 1/1/0/0/0 0/0 \"GET /metrics HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:46567 cpu_ns_avg:11641 lat_ns_tot:14937 lat_ns_avg:3734",
        "Aug 12 18:53:24 rev1-haproxy haproxy[12431]: 188.240.223.140:34280 [12/Aug/2025:18:53:24.600] haproxy_frontend haproxy_backend/rev1_devB 12/Aug/2025:18:53:24 +0000/0/0/0/0/0/0/3/1/12/Aug/2025:18:53:24 +0000/4/4/1755024804 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:13524 cpu_ns_avg:3381 lat_ns_tot:5305 lat_ns_avg:1326",
        "Aug 12 18:53:30 rev1-haproxy haproxy[12431]: 188.240.223.140:6958 [12/Aug/2025:18:53:30.687] haproxy_frontend haproxy_backend/rev1_devC 12/Aug/2025:18:53:30 +0000/0/0/0/0/0/0/3/0/12/Aug/2025:18:53:30 +0000/3/3/1755024810 200 205 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12711 cpu_ns_avg:3177 lat_ns_tot:5230 lat_ns_avg:1307",
        "Aug 12 18:53:32 rev1-haproxy haproxy[12431]: 188.240.223.140:19688 [12/Aug/2025:18:53:32.734] haproxy_frontend haproxy_backend/rev1_devA 12/Aug/2025:18:53:32 +0000/0/0/0/0/0/0/3/0/12/Aug/2025:18:53:32 +0000/3/3/1755024812 200 206 - - ---- 2/1/0/0/0 0/0 \"GET / HTTP/1.1\" 1 cpu_calls:4 cpu_ns_tot:12721 cpu_ns_avg:3180 lat_ns_tot:5387 lat_ns_avg:1346"
    ]
}
2025-08-12 18:53:41,490 p=1421474 u=ubuntu n=ansible | PLAY [Benchmarking the webserver using Apache Benchmark (ab) tests on the backend servers] ************************************************************************************************************************************
2025-08-12 18:53:41,496 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:53:48,239 p=1421474 u=ubuntu n=ansible | ok: [rev1_devC]
2025-08-12 18:53:48,542 p=1421474 u=ubuntu n=ansible | ok: [rev1_devB]
2025-08-12 18:53:49,119 p=1421474 u=ubuntu n=ansible | ok: [rev1_devA]
2025-08-12 18:53:49,144 p=1421474 u=ubuntu n=ansible | TASK [Send Apache Benchmark (ab) loads to webservers and collect response] ****************************************************************************************************************************************************
2025-08-12 18:53:54,819 p=1421474 u=ubuntu n=ansible | changed: [rev1_devA]
2025-08-12 18:53:54,975 p=1421474 u=ubuntu n=ansible | changed: [rev1_devB]
2025-08-12 18:53:55,009 p=1421474 u=ubuntu n=ansible | changed: [rev1_devC]
2025-08-12 18:53:55,015 p=1421474 u=ubuntu n=ansible | TASK [Display the ab test response] *******************************************************************************************************************************************************************************************
2025-08-12 18:53:55,044 p=1421474 u=ubuntu n=ansible | ok: [rev1_devA] => {
    "ab_ws_test_result.stdout_lines": [
        "This is ApacheBench, Version 2.3 <$Revision: 1843412 $>",
        "Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/",
        "Licensed to The Apache Software Foundation, http://www.apache.org/",
        "",
        "Benchmarking 10.1.1.45 (be patient)",
        "",
        "",
        "Server Software:        Werkzeug/3.0.6",
        "Server Hostname:        10.1.1.45",
        "Server Port:            5000",
        "",
        "Document Path:          /",
        "Document Length:        53 bytes",
        "",
        "Concurrency Level:      1000",
        "Time taken for tests:   1.138 seconds",
        "Complete requests:      1200",
        "Failed requests:        146",
        "   (Connect: 0, Receive: 0, Length: 146, Exceptions: 0)",
        "Keep-Alive requests:    0",
        "Total transferred:      271072 bytes",
        "HTML transferred:       63472 bytes",
        "Requests per second:    1054.29 [#/sec] (mean)",
        "Time per request:       948.507 [ms] (mean)",
        "Time per request:       0.949 [ms] (mean, across all concurrent requests)",
        "Transfer rate:          232.58 [Kbytes/sec] received",
        "",
        "Connection Times (ms)",
        "              min  mean[+/-sd] median   max",
        "Connect:        0    4   9.7      0      32",
        "Processing:    11  121  23.5    127     135",
        "Waiting:        1  110  23.4    117     125",
        "Total:         35  124  15.9    127     150",
        "",
        "Percentage of the requests served within a certain time (ms)",
        "  50%    127",
        "  66%    128",
        "  75%    129",
        "  80%    129",
        "  90%    130",
        "  95%    133",
        "  98%    139",
        "  99%    144",
        " 100%    150 (longest request)"
    ]
}
2025-08-12 18:53:55,045 p=1421474 u=ubuntu n=ansible | ok: [rev1_devB] => {
    "ab_ws_test_result.stdout_lines": [
        "This is ApacheBench, Version 2.3 <$Revision: 1843412 $>",
        "Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/",
        "Licensed to The Apache Software Foundation, http://www.apache.org/",
        "",
        "Benchmarking 10.1.1.22 (be patient)",
        "",
        "",
        "Server Software:        Werkzeug/3.0.6",
        "Server Hostname:        10.1.1.22",
        "Server Port:            5000",
        "",
        "Document Path:          /",
        "Document Length:        52 bytes",
        "",
        "Concurrency Level:      1000",
        "Time taken for tests:   0.882 seconds",
        "Complete requests:      1200",
        "Failed requests:        1088",
        "   (Connect: 0, Receive: 0, Length: 1088, Exceptions: 0)",
        "Keep-Alive requests:    0",
        "Total transferred:      271102 bytes",
        "HTML transferred:       63502 bytes",
        "Requests per second:    1360.93 [#/sec] (mean)",
        "Time per request:       734.793 [ms] (mean)",
        "Time per request:       0.735 [ms] (mean, across all concurrent requests)",
        "Transfer rate:          300.25 [Kbytes/sec] received",
        "",
        "Connection Times (ms)",
        "              min  mean[+/-sd] median   max",
        "Connect:        0    2   6.2      0      21",
        "Processing:    11   92  15.8     97     107",
        "Waiting:        1   82  15.9     86      96",
        "Total:         23   94  11.5     97     121",
        "",
        "Percentage of the requests served within a certain time (ms)",
        "  50%     97",
        "  66%     97",
        "  75%     97",
        "  80%     98",
        "  90%     99",
        "  95%    101",
        "  98%    110",
        "  99%    115",
        " 100%    121 (longest request)"
    ]
}
2025-08-12 18:53:55,054 p=1421474 u=ubuntu n=ansible | ok: [rev1_devC] => {
    "ab_ws_test_result.stdout_lines": [
        "This is ApacheBench, Version 2.3 <$Revision: 1843412 $>",
        "Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/",
        "Licensed to The Apache Software Foundation, http://www.apache.org/",
        "",
        "Benchmarking 10.1.1.9 (be patient)",
        "",
        "",
        "Server Software:        Werkzeug/3.0.6",
        "Server Hostname:        10.1.1.9",
        "Server Port:            5000",
        "",
        "Document Path:          /",
        "Document Length:        51 bytes",
        "",
        "Concurrency Level:      1000",
        "Time taken for tests:   0.891 seconds",
        "Complete requests:      1200",
        "Failed requests:        133",
        "   (Connect: 0, Receive: 0, Length: 133, Exceptions: 0)",
        "Keep-Alive requests:    0",
        "Total transferred:      268689 bytes",
        "HTML transferred:       61089 bytes",
        "Requests per second:    1346.41 [#/sec] (mean)",
        "Time per request:       742.717 [ms] (mean)",
        "Time per request:       0.743 [ms] (mean, across all concurrent requests)",
        "Transfer rate:          294.40 [Kbytes/sec] received",
        "",
        "Connection Times (ms)",
        "              min  mean[+/-sd] median   max",
        "Connect:        0    2   5.8      0      19",
        "Processing:    11   93  16.6     98     105",
        "Waiting:        1   83  16.6     88      95",
        "Total:         21   95  12.2     98     118",
        "",
        "Percentage of the requests served within a certain time (ms)",
        "  50%     98",
        "  66%     99",
        "  75%     99",
        "  80%     99",
        "  90%    100",
        "  95%    101",
        "  98%    106",
        "  99%    113",
        " 100%    118 (longest request)"
    ]
}
2025-08-12 18:53:55,097 p=1421474 u=ubuntu n=ansible | PLAY [Benchmarking the HAProxy Load Balancer using Apache Benchmark (ab) tests on the HAProxy server] *************************************************************************************************************************
2025-08-12 18:53:55,103 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:53:58,070 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:53:58,082 p=1421474 u=ubuntu n=ansible | TASK [Gather HAProxy server public IP address] ********************************************************************************************************************************************************************************
2025-08-12 18:54:00,398 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy]
2025-08-12 18:54:00,404 p=1421474 u=ubuntu n=ansible | TASK [Send Apache Benchmark (ab) loads to HAProxy and collect response] *******************************************************************************************************************************************************
2025-08-12 18:54:03,860 p=1421474 u=ubuntu n=ansible | changed: [rev1_HAproxy]
2025-08-12 18:54:03,867 p=1421474 u=ubuntu n=ansible | TASK [Display the ab test response] *******************************************************************************************************************************************************************************************
2025-08-12 18:54:03,884 p=1421474 u=ubuntu n=ansible | ok: [rev1_HAproxy] => {
    "ab_test_result.stdout_lines": [
        "This is ApacheBench, Version 2.3 <$Revision: 1843412 $>",
        "Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/",
        "Licensed to The Apache Software Foundation, http://www.apache.org/",
        "",
        "Benchmarking 188.240.223.140 (be patient)",
        "",
        "",
        "Server Software:        ",
        "Server Hostname:        188.240.223.140",
        "Server Port:            80",
        "",
        "Document Path:          /",
        "Document Length:        108 bytes",
        "",
        "Concurrency Level:      1000",
        "Time taken for tests:   1.368 seconds",
        "Complete requests:      1200",
        "Failed requests:        662",
        "   (Connect: 0, Receive: 0, Length: 662, Exceptions: 0)",
        "Non-2xx responses:      538",
        "Keep-Alive requests:    662",
        "Total transferred:      257005 bytes",
        "HTML transferred:       92901 bytes",
        "Requests per second:    877.28 [#/sec] (mean)",
        "Time per request:       1139.882 [ms] (mean)",
        "Time per request:       1.140 [ms] (mean, across all concurrent requests)",
        "Transfer rate:          183.49 [Kbytes/sec] received",
        "",
        "Connection Times (ms)",
        "              min  mean[+/-sd] median   max",
        "Connect:        0   62  29.8     67     139",
        "Processing:    23  334 309.1    223    1118",
        "Waiting:        1  334 309.2    223    1118",
        "Total:         24  396 311.9    288    1223",
        "",
        "Percentage of the requests served within a certain time (ms)",
        "  50%    288",
        "  66%    330",
        "  75%    544",
        "  80%    590",
        "  90%   1076",
        "  95%   1162",
        "  98%   1183",
        "  99%   1188",
        " 100%   1223 (longest request)"
    ]
}
2025-08-12 18:54:03,940 p=1421474 u=ubuntu n=ansible | PLAY [Test NGINX (snmp) proxy] ************************************************************************************************************************************************************************************************
2025-08-12 18:54:03,945 p=1421474 u=ubuntu n=ansible | TASK [Gathering Facts] ********************************************************************************************************************************************************************************************************
2025-08-12 18:54:06,763 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 18:54:06,774 p=1421474 u=ubuntu n=ansible | TASK [Gather NGINX public IP address] *****************************************************************************************************************************************************************************************
2025-08-12 18:54:08,986 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX]
2025-08-12 18:54:08,992 p=1421474 u=ubuntu n=ansible | TASK [Send SNMP request to NGINX server and collect responses] ****************************************************************************************************************************************************************
2025-08-12 18:54:11,050 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=0)
2025-08-12 18:54:13,127 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=1)
2025-08-12 18:54:15,125 p=1421474 u=ubuntu n=ansible | changed: [rev1_NGINX] => (item=2)
2025-08-12 18:54:15,132 p=1421474 u=ubuntu n=ansible | TASK [Display the NGINX response content] *************************************************************************************************************************************************************************************
2025-08-12 18:54:15,149 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=0) => {
    "ansible_loop_var": "item",
    "item": 0,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-deva"
}
2025-08-12 18:54:15,153 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=1) => {
    "ansible_loop_var": "item",
    "item": 1,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devb"
}
2025-08-12 18:54:15,156 p=1421474 u=ubuntu n=ansible | ok: [rev1_NGINX] => (item=2) => {
    "ansible_loop_var": "item",
    "item": 2,
    "nginx_response.results[item].stdout": "SNMPv2-MIB::sysName.0 = STRING: rev1-devc"
}
2025-08-12 18:54:15,172 p=1421474 u=ubuntu n=ansible | PLAY RECAP ********************************************************************************************************************************************************************************************************************
2025-08-12 18:54:15,172 p=1421474 u=ubuntu n=ansible | rev1_HAproxy               : ok=51   changed=24   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 18:54:15,172 p=1421474 u=ubuntu n=ansible | rev1_NGINX                 : ok=10   changed=4    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 18:54:15,172 p=1421474 u=ubuntu n=ansible | rev1_devA                  : ok=17   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 18:54:15,172 p=1421474 u=ubuntu n=ansible | rev1_devB                  : ok=17   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-08-12 18:54:15,172 p=1421474 u=ubuntu n=ansible | rev1_devC                  : ok=17   changed=11   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
